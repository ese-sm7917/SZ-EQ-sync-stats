{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package imports ##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from sklearn import mixture\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime as dt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma as gamma_func, gammaln, gammaincc, exp1\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import fiona\n",
    "\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon, Point\n",
    "import shapely.ops as ops\n",
    "\n",
    "\n",
    "#Check shapely speedups are enabled\n",
    "from shapely import speedups\n",
    "speedups.enabled\n",
    "\n",
    "#Set geopandas settings\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic zonation methods\n",
    "Set of routines to sort earthquakes into subregions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KML polygon file for the whole study area - execute cell if you have a KML file of rupture areas from Google Earth\n",
    "def load_polys_kml(fpath):\n",
    "    \"\"\"\n",
    "    Load EQ rupture zones from a KML file into gdf and find centroids\n",
    "    # fpath: filepath of the KML file\n",
    "    \"\"\"\n",
    "    polys = gpd.read_file(fpath, driver='KML', crs=\"EPSG:4326\")\n",
    "    # Add polygon centroids as well\n",
    "    polys = polys.to_crs('+proj=cea')\n",
    "    polys['centroid'] = polys['geometry'].centroid\n",
    "    polys['centroid_geog'] = polys['centroid'].to_crs(4326)\n",
    "    polys['centroid_lon'] = polys['centroid_geog'].x\n",
    "    polys['centroid_lat'] = polys['centroid_geog'].y\n",
    "    polys = polys.drop(columns=['centroid', 'Description'])\n",
    "    polys = polys.to_crs(4326)\n",
    "    # save to file\n",
    "    outfile = (fpath.split(sep='/')[0]+'/'+fpath.split(sep='/')[1].split(sep='_')[2].split(sep='.')[0]\n",
    "               +'_large_rupture_labels.txt')\n",
    "    print(outfile)\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(polys.to_string(columns=['centroid_lon','centroid_lat','Name'],header=False,index=False))\n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_polys_kml('figs/large_ruptures_SAM.kml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load polygons - execute cell if you have lists of coordinates defining target zones\n",
    "# Read in .dat file as df\n",
    "def load_polys(target_area):\n",
    "    fpath = 'sbams/' + target_area + '/'\n",
    "    regions_path = fpath + target_area + '_trench_points.txt'\n",
    "    regions = []\n",
    "    with open(regions_path, 'r') as reader:\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            regions.append(line.split()[5])\n",
    "    polygon_coords = []\n",
    "    for region in regions:\n",
    "        coords_df = pd.read_table((fpath+region+'_CropPolygon.dat'), sep=\"\\s+\", header=None, names=['lon', 'lat'])\n",
    "        polygon_coords.append(Polygon(zip(coords_df['lon'], coords_df['lat'])))\n",
    "\n",
    "\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    d = {'name': regions, 'geometry': polygon_coords}\n",
    "    polys = gpd.GeoDataFrame(d, crs=\"EPSG:4326\")\n",
    "    # Add polygon centroids as well\n",
    "    polys = polys.to_crs('+proj=cea')\n",
    "    polys['centroid'] = polys['geometry'].centroid\n",
    "    polys['centroid_geog'] = polys['centroid'].to_crs(4326)\n",
    "    polys = polys.drop(columns=['centroid'])\n",
    "    polys = polys.to_crs(4326)\n",
    "    return polys, regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polys, regions = load_polys('SAM')\n",
    "polys['centroid_lon'] = polys['centroid_geog'].x\n",
    "polys['centroid_lat'] = polys['centroid_geog'].y\n",
    "outfile = ('sbams/SAM/SAM_zone_centroids.txt')\n",
    "print(outfile)\n",
    "with open(outfile, 'w') as f:\n",
    "    f.write(polys.to_string(columns=['centroid_lon','centroid_lat','name'],header=False,index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to extract the earthquakes relevant to each polygon\n",
    "\n",
    "def assign_events_and_save(target_area):\n",
    "    \"\"\"\n",
    "    Assigns events to geopgraphic regions defined by KML polygons\n",
    "    # target_area is the study area (type str: Japan, SAM)\n",
    "    \"\"\"\n",
    "    # Initialise filepaths\n",
    "    cat_dir = target_area + '_EQ_data/'\n",
    "    declustM = cat_dir + 'Mizrahi_ETAS_decluster/ETAS_declustered_cat.csv'\n",
    "    rejM = cat_dir + 'Mizrahi_ETAS_decluster/ETAS_rejected_evs.csv'\n",
    "    declustZ = cat_dir + 'Zaliapin_decluster/declustered_catalog_data.csv'\n",
    "    rejZ = cat_dir + 'Zaliapin_decluster/rejected_ev.csv'\n",
    "    raw = cat_dir + 'raw_catalog_data.csv'\n",
    "\n",
    "    # Get the target regions\n",
    "    polys, regions = load_polys(target_area)\n",
    "    # Sort the earthquakes into subregions, one subregion at a time\n",
    "    for fpath in [declustM, rejM, declustZ, rejZ]:\n",
    "        # Load CSV catalog\n",
    "        cat = pd.read_csv(fpath, index_col=0)\n",
    "        # Write to a pandas geodataframe\n",
    "        gdf = gpd.GeoDataFrame(cat, geometry=gpd.points_from_xy(cat.lon, cat.lat))\n",
    "        for region_name in regions:\n",
    "            # Location filtering\n",
    "            outdir = cat_dir + 'zonal_cat/' + region_name\n",
    "            outfile = outdir + '/' + fpath.split(sep='/')[2]\n",
    "            print('Now processing ', outfile)\n",
    "            region = polys.loc[polys['name']==region_name]\n",
    "            region.reset_index(drop=True, inplace=True)\n",
    "            pip_mask = gdf.within(region.at[0, 'geometry'])\n",
    "            pip_data = gdf.loc[pip_mask]\n",
    "            # Write the earthquakes for the chosen region to a csv file\n",
    "            if not os.path.exists(outdir):\n",
    "                os.mkdir(outdir)\n",
    "            pip_data.to_csv(outfile, columns=['year', 'month', 'day', 'hour', 'minute', 'second', 'lat', 'lon', \n",
    "                                              'depth_km', 'mag', 'time'])\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_events_and_save('SAM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a catalog for plotting\n",
    "\n",
    "# Prepare catalog:\n",
    "def prep_cat_zaliapin(cat_init):\n",
    "    \"\"\" Loads and prepares a raw catalog for further processing\n",
    "    # input cat_init needs to be a file path to a CSV document containing labelled columns:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    \"\"\"\n",
    "    # Load catalog from file:\n",
    "    cat = pd.read_csv(cat_init, index_col=0)\n",
    "    cat = cat.sort_index()\n",
    "    \n",
    "    # Create datetimes\n",
    "    cat[\"time\"] = pd.to_datetime(cat[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    cat = cat.infer_objects()\n",
    "    cat.loc[:, 'depth_km'] *=0.001\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    cat['lat_rad'] = np.radians(cat['lat'])\n",
    "    cat['lon_rad'] = np.radians(cat['lon'])\n",
    "    return cat\n",
    "\n",
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    \"\"\"Haversine function\n",
    "    Takes in arguments in radians\n",
    "    \"\"\"\n",
    "    return np.square(np.sin(theta / 2))\n",
    "\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    #Haversine distance in km - calculate distance between 2 pts on a sphere\n",
    "    # lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2 must all be in radians\n",
    "    ####################################################################\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(\n",
    "        np.sqrt(\n",
    "            hav(lat_rad_1 - lat_rad_2)\n",
    "            + np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronisation detection\n",
    "\n",
    "Following methods outlined in Jara et al. (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronisation detection\n",
    "# Following Jara et al. (2017)\n",
    "def detect_sync(region, zone, method, search_type, shallow_0, shallow_1, deep_0, deep_1):\n",
    "    \"\"\"\n",
    "    Detect synchronisations between deep and shallow seismicity\n",
    "    region: large-scale target area, type str (e.g. SAM, Japan)\n",
    "    zone: subregion , type str\n",
    "    method: Preferred declustered catalog, type str (Mizrahi or Zaliapin)\n",
    "    search_type: Method to compute sync coefficients - if 'S', function will look backwards from shallow events.\n",
    "                 If 'D', function will look forwards from deep events\n",
    "    shallow_lim: the lower depth limit (km) for what is considered a shallow earthquake\n",
    "    deep_lim: the upper depth limit (km) for what is considered a deep earthquake\n",
    "    deep_extr: deepest limit to the seismicity to be considered\n",
    "    \"\"\"\n",
    "    # Build filepaths\n",
    "    root_dir = region+'_EQ_data/'\n",
    "    if method == 'Zaliapin':\n",
    "        fn_cat = root_dir+'zonal_cat/'+zone+'/declustered_catalog_data.csv'\n",
    "    elif method == 'Mizrahi':\n",
    "        fn_cat = root_dir+'zonal_cat/'+zone+'/ETAS_declustered_cat.csv'\n",
    "    \n",
    "    # Load catalog from file:\n",
    "    cat = pd.read_csv(fn_cat, index_col=0,parse_dates=['time'])\n",
    "    cat = cat.sort_index()\n",
    "    \n",
    "    cat = cat[['time', 'lat','lon', 'depth_km', 'mag']]\n",
    "    \n",
    "    # Define lengths of test windows from Jara et al.\n",
    "    test_periods_h = [1, 3, 6, 12]\n",
    "    test_periods_d = [1,2,3,4,5,6,7,8,9,10,15,20,25,30,60,90]\n",
    "    deltas_h = [dt.timedelta(hours=period) for period in test_periods_h]\n",
    "    deltas_d = [dt.timedelta(days=period) for period in test_periods_d]\n",
    "    deltas = [*deltas_h, *deltas_d]\n",
    "    col_labels_p = [] # store test period column labels\n",
    "    col_labels_m = [] # store magnitude labels\n",
    "    \n",
    "    # Initiliase columns to store synchronisation coeffs\n",
    "    for win_length in test_periods_h:\n",
    "        label_p = '{}h'.format(win_length)\n",
    "        label_m = '{}h_mag'.format(win_length)\n",
    "        cat[label_p] = 0.0\n",
    "        cat[label_m] = 0.0\n",
    "        col_labels_p.append(label_p)\n",
    "        col_labels_m.append(label_m)\n",
    "    for win_length in test_periods_d:\n",
    "        label_p = '{}d'.format(win_length)\n",
    "        label_m = '{}d_mag'.format(win_length)\n",
    "        cat[label_p] = 0.0\n",
    "        cat[label_m] = 0.0\n",
    "        col_labels_p.append(label_p)\n",
    "        col_labels_m.append(label_m)\n",
    "    \n",
    "    # Create deep and shallow catalogs:\n",
    "    deep_cat = cat.loc[(cat['depth_km'] >= deep_0) & (cat['depth_km'] < deep_1)]\n",
    "    shallow_cat = cat.loc[(cat['depth_km'] >= shallow_0) & (cat['depth_km'] < shallow_1)]\n",
    "    \n",
    "    # Perform search\n",
    "    if search_type == 'S':\n",
    "        print(\"SHALLOW MODE\")\n",
    "        for t_delta, col_label_p, col_label_m in zip(deltas, col_labels_p, col_labels_m):\n",
    "            print('Looking for synchronisations within {} of shallow event'.format(str(t_delta)))\n",
    "            for shallow_ev in shallow_cat.itertuples():\n",
    "                # get values of shallow event\n",
    "                sev_time = shallow_ev.time\n",
    "                #print(sev_time)\n",
    "                ref_time = sev_time - t_delta\n",
    "                # Specify the search time period\n",
    "                target_deep = deep_cat.loc[(deep_cat['time'] < sev_time) & (deep_cat['time'] >= ref_time)]\n",
    "                #print(target_deep)\n",
    "                if target_deep.shape[0] > 0:\n",
    "                    shallow_cat.loc[shallow_ev.Index, col_label_p] = 1.0 # set sync coefficient to 1\n",
    "                    shallow_cat.loc[shallow_ev.Index, col_label_m] = target_deep['mag'].max() # extract max magnitude of deep sync\n",
    "                    #print('Sync detected')\n",
    "                else:\n",
    "                    shallow_cat.loc[shallow_ev.Index, col_label_p] = 0.0\n",
    "                    shallow_cat.loc[shallow_ev.Index, col_label_m] = np.nan\n",
    "\n",
    "        # Save to file\n",
    "        outfile = '{}zonal_cat/{}/sync_coeff_S_data_{}-{}_{}-{}.csv'.format(root_dir,zone,str(int(shallow_0)),\n",
    "                                                                    str(int(shallow_1)),str(int(deep_0)),str(int(deep_1)))\n",
    "        shallow_cat.to_csv(outfile)\n",
    "        return shallow_cat  \n",
    "    \n",
    "    elif search_type == 'D':\n",
    "        print(\"DEEP MODE\")\n",
    "        for t_delta, col_label_p, col_label_m in zip(deltas, col_labels_p, col_labels_m):\n",
    "            print('Looking for synchronisations within {} of deep event'.format(str(t_delta)))\n",
    "            for deep_ev in deep_cat.itertuples():\n",
    "                # get values of deep event\n",
    "                dev_time = deep_ev.time\n",
    "                ref_time = dev_time + t_delta\n",
    "                # Specify the search time period\n",
    "                target_shallow = shallow_cat.loc[(shallow_cat['time'] > dev_time) & (shallow_cat['time'] <= ref_time)]\n",
    "                #print(target_deep)\n",
    "                if target_shallow.shape[0] > 0:\n",
    "                    deep_cat.loc[deep_ev.Index, col_label_p] = 1.0 # set sync coefficient to 1\n",
    "                    deep_cat.loc[deep_ev.Index, col_label_m] = target_shallow['mag'].max() \n",
    "                    # extract max magnitude of deep sync\n",
    "                else:\n",
    "                    deep_cat.loc[deep_ev.Index, col_label_p] = 0.0\n",
    "                    deep_cat.loc[deep_ev.Index, col_label_m] = np.nan\n",
    "\n",
    "        # Save to file\n",
    "        outfile = '{}zonal_cat/{}/sync_coeff_D_data_{}-{}_{}-{}.csv'.format(root_dir,zone,str(int(shallow_0)),\n",
    "                                                                    str(int(shallow_1)),str(int(deep_0)),str(int(deep_1)))\n",
    "        deep_cat.to_csv(outfile)\n",
    "        return deep_cat\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for both shallow and deep modes for SAM\n",
    "for mode in ['S', 'D']:\n",
    "    detect_sync('SAM', 'Bucaramanga','Zaliapin', mode, 0.0, 50.0, 80.0, 200.0)\n",
    "    detect_sync('SAM', 'Colombia','Zaliapin', mode, 0.0, 60.0, 80.0, 200.0)\n",
    "    detect_sync('SAM', 'Ecuador','Zaliapin', mode, 0.0, 60.0, 80.0, 250.0)\n",
    "    detect_sync('SAM', 'Peru','Zaliapin', mode, 0.0, 40.0, 80.0, 200)\n",
    "    detect_sync('SAM', 'Arequipa','Zaliapin', mode, 0.0, 50.0, 90.0, 250.0)\n",
    "    detect_sync('SAM', 'North_Chile','Zaliapin', mode, 0.0, 70.0, 70.0, 150.0)\n",
    "    detect_sync('SAM', 'North_Chile','Zaliapin', mode, 0.0, 70.0, 150.0, 250.0)\n",
    "    detect_sync('SAM', 'Atacama','Zaliapin', mode, 0.0, 70.0, 70.0, 120.0)\n",
    "    detect_sync('SAM', 'Atacama','Zaliapin', mode, 70.0, 120.0, 120.0, 250.0)\n",
    "    detect_sync('SAM', 'Central_Chile','Zaliapin', mode, 0.0, 60.0, 80.0, 100.0)\n",
    "    detect_sync('SAM', 'Central_Chile','Zaliapin', mode, 80.0, 100.0, 100.0, 250.0)\n",
    "    detect_sync('SAM', 'Central_Chile','Zaliapin', mode, 0.0, 60.0, 80.0, 250.0)\n",
    "    detect_sync('SAM', 'Maule','Zaliapin', mode, 0.0, 40.0, 80.0, 250.0)\n",
    "    detect_sync('SAM', 'Valdivia','Zaliapin', mode, 0.0, 40.0, 80.0, 250.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sync_coeffs_pts(region, zones, search_type, cat_start, cat_end):\n",
    "    \"\"\"\n",
    "    Plot sync coeffs for all subregions in a specific region, for a specific timewindow\n",
    "    region: the region of interest, type str (e.g. SAM)\n",
    "    zones: list of subregions (str) we want to plot sync coeffs for\n",
    "    search_type: specify type of search - i.e. shallow syncing with deep before it (S) \n",
    "                or deep syncing with shallow after (D)\n",
    "    cat_start: start of the plotting period (dt.datetime)\n",
    "    cat_end: end of the plotting period (dt.datetime)\n",
    "    \"\"\" \n",
    "    # Filenames\n",
    "    fn_large_ev = region+'_EQ_data/large_eq.csv'\n",
    "    if search_type == 'S':\n",
    "        fsearch = 'sync_coeff_S*.csv'\n",
    "    elif search_type == 'D':\n",
    "        fsearch = 'sync_coeff_D*.csv'\n",
    "    # Read in sync_coeffs\n",
    "    from pathlib import Path\n",
    "    for zone in zones:\n",
    "        basedir = region+'_EQ_data/zonal_cat/'+zone+'/'\n",
    "        basedir_p = Path(basedir)\n",
    "        print(basedir)\n",
    "        for fpath in basedir_p.glob(fsearch):\n",
    "            sync_coeffs = pd.read_csv(fpath, index_col=0, parse_dates=['time'])\n",
    "    \n",
    "            # plot the sync coefficients\n",
    "            sample_cat = sync_coeffs.loc[(sync_coeffs['time'] <= cat_end) &\n",
    "                                         (sync_coeffs['time'] >= cat_start)]\n",
    "            fig, axs = plt.subplots(nrows=5, ncols=3, figsize=(30,32))\n",
    "            dep_range = fpath.name.split(sep='data_')[1].split(sep='.c')[0]\n",
    "            if search_type == 'S':\n",
    "                fig.suptitle('Sync coefficients for {}, z: {} km (Shallow mode)'.format(zone,dep_range), y=0.89)\n",
    "            elif search_type == 'D':\n",
    "                fig.suptitle('Sync coefficients for {}, z: {} km (Deep mode)'.format(zone,dep_range), y=0.89)\n",
    "            test_periods_d = [2,3,4,5,6,7,8,9,10,15,20,25,30,60,90]\n",
    "            test_periods = ['{}d'.format(dur) for dur in test_periods_d]\n",
    "            print(test_periods)\n",
    "            \n",
    "            ## Plot important large evs ##\n",
    "            # Load catalog of large events (for plotting milestone lines)\n",
    "            large_ev_cat = prep_cat_zaliapin(fn_large_ev)\n",
    "            large_ev_cat = large_ev_cat.loc[(large_ev_cat['time'] <= cat_end) &\n",
    "                                         (large_ev_cat['time'] >= cat_start)]\n",
    "\n",
    "            # Get the target regions\n",
    "            polys, boxes = load_polys(region)\n",
    "\n",
    "            # Only include large events within 1000 km of centroid of zone\n",
    "            clonrad = np.radians((polys.loc[polys['name']==zone])['centroid_geog'].x.values)\n",
    "            clatrad = np.radians((polys.loc[polys['name']==zone])['centroid_geog'].y.values)\n",
    "            large_ev_cat['dist'] = haversine(clatrad,large_ev_cat['lat_rad'],clonrad,large_ev_cat['lon_rad'])\n",
    "            large_ev = large_ev_cat.loc[large_ev_cat['dist']<=1000.0]\n",
    "            large_ev = large_ev.set_index('time')\n",
    "            large_ev = large_ev.sort_index()\n",
    "            \n",
    "            poly_df = polys.loc[polys['name']==zone]\n",
    "            poly_df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "                    \n",
    "            # Define binary colourmap\n",
    "            from matplotlib.colors import ListedColormap\n",
    "            cmap = ListedColormap(['red', 'green'])\n",
    "            for ax, test_per in zip(axs.flat, test_periods):\n",
    "                coeffs = sample_cat[test_per]\n",
    "                dates = sample_cat['time']\n",
    "                im = ax.scatter(dates, coeffs, c=coeffs, cmap=cmap, \n",
    "                                label='{} (Total {} events)'.format(test_per, len(dates)))\n",
    "                for event in large_ev.itertuples():\n",
    "                    ax.axvline(event.Index,color='k', ls='--')\n",
    "                    hyp = gpd.GeoSeries([Point(event.lon, event.lat)])\n",
    "                    if hyp.within(poly_df.at[0,'geometry']).values:\n",
    "                        props = dict(boxstyle='round',facecolor='white',alpha=1.0)\n",
    "                        if event.depth_km <= 60.0:\n",
    "                            ax.text(event.Index,ax.get_ylim()[1]/2,event.name,ha='center',va='center', \n",
    "                                     fontsize='large', rotation=90, color='r', bbox=props)\n",
    "                        elif event.depth_km > 60.0:\n",
    "                            ax.text(event.Index,ax.get_ylim()[1]/2,event.name,ha='center',va='center', \n",
    "                                     fontsize='large', rotation=90, color='b', bbox=props)\n",
    "                    else:\n",
    "                        props = dict(boxstyle='round',facecolor='white',ls='--',alpha=1.0)\n",
    "                        ax.text(event.Index,ax.get_ylim()[1]/2,event.name,ha='center',va='center', \n",
    "                             fontsize='large', rotation=90, color='grey', bbox=props)\n",
    "                #ax.scatter(dates, coeffs, label=test_per)\n",
    "                #_=[ax.axvline(x[i],color='k') for i in idxs]\n",
    "                #_=[ax.text(x[i],ax.get_ylim()[1],f\"{x[i]:1.2f}\",ha='center',va='bottom') for i in idxs]\n",
    "                ax.set_ylabel('Synchronisation coefficient', fontsize=14)\n",
    "                ax.xaxis.set_tick_params(labelsize=14)\n",
    "                ax.yaxis.set_tick_params(labelsize=14)\n",
    "                ax.legend(loc='lower left')\n",
    "                # Add a colorbar for depth\n",
    "                #cbar = fig.colorbar(im, ax=ax)\n",
    "                #cbar.set_label('depth[km]', fontsize=12)\n",
    "                \n",
    "            plt.show()\n",
    "            \n",
    "            # Save the figure\n",
    "            root_dir = basedir.split(sep='/')[0] +'/'\n",
    "            outdir = root_dir+'figs-PDF/sync_coeffs/'\n",
    "            if not os.path.exists(outdir):\n",
    "                os.mkdir(outdir)\n",
    "            box = basedir.split(sep='/')[2]\n",
    "            if search_type == 'S':\n",
    "                fn_output = outdir+box+'_'+dep_range+'_S_sync_coeffs.pdf'\n",
    "            elif search_type == 'D':\n",
    "                fn_output = outdir+box+'_'+dep_range+'_D_sync_coeffs.pdf'\n",
    "            fig.savefig(fn_output, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = ['Bucaramanga', 'Colombia', 'Ecuador', 'Peru', 'Arequipa', 'North_Chile',\n",
    "           'Atacama', 'Central_Chile', 'Maule', 'Valdivia']\n",
    "plot_sync_coeffs_pts('SAM', zones, 'S', dt.datetime(2000,1,1), dt.datetime(2021,7,1))\n",
    "plot_sync_coeffs_pts('SAM', zones, 'D', dt.datetime(2000,1,1), dt.datetime(2021,7,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sync_coeffs_bar(region, zones, search_type, cat_start, cat_end):\n",
    "    \"\"\"\n",
    "    Plot sync coeffs for all subregions in a specific region, for a specific timewindow, in 6 month bins, and counting\n",
    "    the number of positive and negative synchronisations in each bin\n",
    "    region: the region of interest, type str (e.g. SAM)\n",
    "    zones: list of subregions (str) we want to plot sync coeffs for\n",
    "    search_type: specify type of search - i.e. shallow syncing with deep before it (S) \n",
    "                or deep syncing with shallow after (D)\n",
    "    cat_start: start of the plotting period (dt.datetime)\n",
    "    cat_end: end of the plotting period (dt.datetime)\n",
    "    \"\"\" \n",
    "    # Filenames\n",
    "    fn_large_ev = region+'_EQ_data/large_eq.csv'\n",
    "    if search_type == 'S':\n",
    "        fsearch = 'sync_coeff_S*.csv'\n",
    "    elif search_type == 'D':\n",
    "        fsearch = 'sync_coeff_D*.csv'\n",
    "    # Read in sync_coeffs\n",
    "    from pathlib import Path\n",
    "    for zone in zones:\n",
    "        basedir = region+'_EQ_data/zonal_cat/'+zone+'/'\n",
    "        basedir_p = Path(basedir)\n",
    "        print(basedir)\n",
    "        for fpath in basedir_p.glob(fsearch):\n",
    "            sync_coeffs = pd.read_csv(fpath, index_col=0, parse_dates=['time'])\n",
    "    \n",
    "            # plot the sync coefficients\n",
    "            sample_cat = sync_coeffs.loc[(sync_coeffs['time'] <= cat_end) &\n",
    "                                         (sync_coeffs['time'] >= cat_start)]\n",
    "            fig, axs = plt.subplots(nrows=5, ncols=3, figsize=(30,32))\n",
    "            dep_range = fpath.name.split(sep='data_')[1].split(sep='.c')[0]\n",
    "            if search_type == 'S':\n",
    "                fig.suptitle('Sync coefficients for {}, z: {} km (Shallow mode)'.format(zone,dep_range), y=0.89)\n",
    "            elif search_type == 'D':\n",
    "                fig.suptitle('Sync coefficients for {}, z: {} km (Deep mode)'.format(zone,dep_range), y=0.89)\n",
    "            test_periods_d = [2,3,4,5,6,7,8,9,10,15,20,25,30,60,90]\n",
    "            test_periods = ['{}d'.format(dur) for dur in test_periods_d]\n",
    "            print(test_periods)\n",
    "            \n",
    "            ## Plot important large evs ##\n",
    "            # Load catalog of large events (for plotting milestone lines)\n",
    "            large_ev_cat = prep_cat_zaliapin(fn_large_ev)\n",
    "            large_ev_cat = large_ev_cat.loc[(large_ev_cat['time'] <= cat_end) &\n",
    "                                         (large_ev_cat['time'] >= cat_start)]\n",
    "\n",
    "            # Get the target regions\n",
    "            polys, boxes = load_polys(region)\n",
    "\n",
    "            # Only include large events within 1000 km of centroid of zone\n",
    "            clonrad = np.radians((polys.loc[polys['name']==zone])['centroid_geog'].x.values)\n",
    "            clatrad = np.radians((polys.loc[polys['name']==zone])['centroid_geog'].y.values)\n",
    "            large_ev_cat['dist'] = haversine(clatrad,large_ev_cat['lat_rad'],clonrad,large_ev_cat['lon_rad'])\n",
    "            large_ev = large_ev_cat.loc[large_ev_cat['dist']<=1000.0]\n",
    "            large_ev = large_ev.set_index('time')\n",
    "            large_ev = large_ev.sort_index()\n",
    "            \n",
    "            poly_df = polys.loc[polys['name']==zone]\n",
    "            poly_df.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            # Plot bar charts\n",
    "            def zero_col(col):\n",
    "                if col[test_per] == 1.0:\n",
    "                    return np.nan\n",
    "                elif col[test_per] == 0.0:\n",
    "                    return 0.0\n",
    "\n",
    "            def one_col(col):\n",
    "                if col[test_per] == 1.0:\n",
    "                    return 1.0\n",
    "                elif col[test_per] == 0.0:\n",
    "                    return np.nan\n",
    "                \n",
    "            for ax, test_per in zip(axs.flat, test_periods):\n",
    "                # Plot the bars:\n",
    "                cat_plot = sample_cat[['time', test_per]]\n",
    "                cat_plot['zeros'] = cat_plot.apply(lambda cat_plot : zero_col(cat_plot), axis=1)\n",
    "                cat_plot['ones'] = cat_plot.apply(lambda cat_plot : one_col(cat_plot), axis=1)\n",
    "\n",
    "                cat_plot = cat_plot.groupby(pd.Grouper(key='time', freq='6M')).count()\n",
    "                cat_plot.index.freq = None\n",
    "\n",
    "                ax.bar(cat_plot.index, cat_plot['zeros'].values, label='SC=0 ({})'.format(test_per), width=182)\n",
    "                ax.bar(cat_plot.index, cat_plot['ones'].values, label='SC=1 ({})'.format(test_per), \n",
    "                       bottom=cat_plot['zeros'].values, width=182)\n",
    "                # Add markers for large events\n",
    "                for event in large_ev.itertuples():\n",
    "                    ax.axvline(event.Index,color='k', ls='--')\n",
    "                    hyp = gpd.GeoSeries([Point(event.lon, event.lat)])\n",
    "                    if hyp.within(poly_df.at[0,'geometry']).values:\n",
    "                        props = dict(boxstyle='round',facecolor='white',alpha=1.0)\n",
    "                        if event.depth_km <= 60.0:\n",
    "                            ax.text(event.Index,ax.get_ylim()[1]/2,event.name,ha='center',va='center', \n",
    "                                     fontsize='large', rotation=90, color='r', bbox=props)\n",
    "                        elif event.depth_km > 60.0:\n",
    "                            ax.text(event.Index,ax.get_ylim()[1]/2,event.name,ha='center',va='center', \n",
    "                                     fontsize='large', rotation=90, color='b', bbox=props)\n",
    "                    else:\n",
    "                        props = dict(boxstyle='round',facecolor='white',ls='--',alpha=1.0)\n",
    "                        ax.text(event.Index,ax.get_ylim()[1]/2,event.name,ha='center',va='center', \n",
    "                             fontsize='large', rotation=90, color='grey', bbox=props)\n",
    "                #ax.scatter(dates, coeffs, label=test_per)\n",
    "                #_=[ax.axvline(x[i],color='k') for i in idxs]\n",
    "                #_=[ax.text(x[i],ax.get_ylim()[1],f\"{x[i]:1.2f}\",ha='center',va='bottom') for i in idxs]\n",
    "                ax.set_ylabel('Count', fontsize=14)\n",
    "                ax.xaxis.set_tick_params(labelsize=14)\n",
    "                ax.yaxis.set_tick_params(labelsize=14)\n",
    "                ax.legend(loc='upper left')\n",
    "                # Add a colorbar for depth\n",
    "                #cbar = fig.colorbar(im, ax=ax)\n",
    "                #cbar.set_label('depth[km]', fontsize=12)\n",
    "                \n",
    "            plt.show()\n",
    "            \n",
    "            # Save the figure\n",
    "            root_dir = basedir.split(sep='/')[0] +'/'\n",
    "            outdir = root_dir+'figs-PDF/sync_coeffs_bars/'\n",
    "            if not os.path.exists(outdir):\n",
    "                os.mkdir(outdir)\n",
    "            box = basedir.split(sep='/')[2]\n",
    "            if search_type == 'S':\n",
    "                fn_output = outdir+box+'_'+dep_range+'_S_bars_sync_coeffs.pdf'\n",
    "            elif search_type == 'D':\n",
    "                fn_output = outdir+box+'_'+dep_range+'_D_bars_sync_coeffs.pdf'\n",
    "            fig.savefig(fn_output, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = ['Bucaramanga', 'Colombia', 'Ecuador', 'Peru', 'Arequipa', 'North_Chile',\n",
    "           'Atacama', 'Central_Chile', 'Maule', 'Valdivia']\n",
    "plot_sync_coeffs_bar('SAM', zones, 'S', dt.datetime(2000,1,1), dt.datetime(2021,7,1))\n",
    "plot_sync_coeffs_bar('SAM', zones, 'D', dt.datetime(2000,1,1), dt.datetime(2021,7,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection of accelerations (incomplete)\n",
    "Following Marsan et al. (2017): <br>\n",
    " - Separate catalog into nodes with 0.05 degree spacing\n",
    " - For each node, consider EQ within 50 km of the node, and discard nodes with fewer than 50 EQ\n",
    " - For each of the remaining nodes, compute best quadratic fit $ \\hat{N}(t) = at^2 + bt + c $ to the cumulative time series of background earthquakes $ \\hat{N}(t) $, produced by each of the 2 declustering methods\n",
    " - Impose constraint that initial rate $2at_s + b$ and final rate $2at_e + b$ must be positive\n",
    " - Characterise the time series using the constant $\\phi$, where: $$ \\phi = \\frac{\\hat{N(t_e)}}{\\hat{N(t_s)}} = \\frac{2at_e + b}{2at_s + b} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gridding function\n",
    "# total area for the grid\n",
    "xmin, ymin, xmax, ymax= gdf.total_bounds\n",
    "# how many cells across and down\n",
    "n_cells=30\n",
    "cell_size = (xmax-xmin)/n_cells\n",
    "# projection of the grid\n",
    "crs = \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +a=6371007.181 +b=6371007.181 +units=m +no_defs\"\n",
    "# create the cells in a loop\n",
    "grid_cells = []\n",
    "for x0 in np.arange(xmin, xmax+cell_size, cell_size ):\n",
    "    for y0 in np.arange(ymin, ymax+cell_size, cell_size):\n",
    "        # bounds\n",
    "        x1 = x0-cell_size\n",
    "        y1 = y0+cell_size\n",
    "        grid_cells.append( shapely.geometry.box(x0, y0, x1, y1)  )\n",
    "cell = geopandas.GeoDataFrame(grid_cells, columns=['geometry'], \n",
    "                                 crs=crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detection of accelerations following Marsan et al. (2017)\n",
    "\n",
    "# Calculate acceleration coefficients\n",
    "def calc_accel_coeffs(fpath_cat1, fpath_cat2, regions_fpath):\n",
    "    \"\"\"\n",
    "    Calculate background rate acceleration/deceleration coefficients\n",
    "    fpath_cat1: filepath (str) to CSV file containing declustered catalog using Method 1 (Z-BZ 2013)\n",
    "    fpath_cat2: filepath (str) to CSV file containing declustered catalog using Method 2 (ETAS)\n",
    "    regions_fpath: Path of KML file defining geographic regions of interest in the study area as polygons\n",
    "    \"\"\"\n",
    "    # Read in the polygons and initialise cols to store the coeffs\n",
    "    regions = gpd.read_file(regions_fpath, driver='KML')\n",
    "    regions['phi_1'] = 0.0\n",
    "    regions['phi_2'] = 0.0\n",
    "    \n",
    "    # Read in the declustered catalogs\n",
    "    cat1 = pd.read_csv(fpath_cat1, index_col=0)\n",
    "    cat2 = pd.read_csv(fpath_cat2, index_col=0)\n",
    "    \n",
    "    # Calculate the coefficients for each region\n",
    "    for region in regions.itertuples():\n",
    "        region_name = region.Name\n",
    "        cat1_in_region = assign_events(cat1, regions, region_name)\n",
    "        cat2_in_region = assign_events(cat2, regions, region_name)\n",
    "\n",
    "        # Declustered cat from method 1\n",
    "        cum_cat1 = cat1_in_region.set_index('time')\n",
    "        cum_cat1['count'] = 1.0\n",
    "        cum_cat1 = cum_cat.cumsum()\n",
    "\n",
    "        # Declustered cat from method 2\n",
    "        cum_cat2 = cat2_in_region.set_index('time')\n",
    "        cum_cat2['count'] = 1.0\n",
    "        cum_cat2 = cum_cat2.cumsum()\n",
    "\n",
    "        # Fit quadratics\n",
    "        from numpy.polynomial import Polynomial as P\n",
    "        time_diffs1 = (1./(24.*60.*60.))*(cum_cat1.index - cum_cat1.index.min()).total_seconds()\n",
    "        counts1 = cum_cat1['count'].values\n",
    "        p1 = P.fit(time_diffs1,counts1, 2)\n",
    "        time_diffs2 = (1./(24.*60.*60.))*(cum_cat2.index - cum_cat2.index.min()).total_seconds()\n",
    "        counts2 = cum_cat2['count'].values\n",
    "        p2 = P.fit(time_diffs2,counts2, 2)\n",
    "\n",
    "        # calculate the phi coefficient\n",
    "        phi_1 = (2*((cum_cat1.index.max() - cum_cat1.index.min()).total_seconds())*p1.coef[1] +  p1.coef[0])/(p1.coef[0])\n",
    "        phi_2 = (2*((cum_cat2.index.max() - cum_cat2.index.min()).total_seconds())*p2.coef[1] +  p2.coef[1])/(p2.coef[1])\n",
    "        regions.loc[region.Index, 'phi_1' ] = phi_1\n",
    "        regions.loc[region.Index, 'phi_2' ] = phi_2\n",
    "        \n",
    "        # Filter according to Marsan criteria\n",
    "        regions = regions.loc[((regions['phi_1']>1.0) & (regions['phi_2']>1.0)) | \n",
    "                              ((regions['phi_1']<1.0) & (regions['phi_2']<1.0)) | \n",
    "                              (np.abs(regions['phi_1']-regions['phi_2'])<0.3)]\n",
    "        regions['phi_ave'] = (regions['phi_1'] + regions['phi_2'])/2.0\n",
    "        \n",
    "        return regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting routines for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOTTING ####\n",
    "\n",
    "def plot_cumulative(cat_declust_fn, cat_orig_fn, large_ev_fn, cat_start, mc, method, fig_fpath):\n",
    "    # cat_declust: path to CSV file containing a declustered catalog\n",
    "    # cat_orig: path to CSV file with the original catalog\n",
    "    # cat_start: time (datetime) past which rate data is considered reliable\n",
    "    # mc: the catalog completeness magnitude\n",
    "    # method: name of the declustering method, type str (Zaliapin or Mizrahi)\n",
    "    # fig_fpath: path to output PDF file for plot\n",
    "    \n",
    "    # Load the catalogs\n",
    "    cat_declust = pd.read_csv(cat_declust_fn, index_col=0, parse_dates=[\"time\"], dtype={\"url\": str, \"alert\": str})\n",
    "    cat_orig = prep_cat_zaliapin(cat_orig_fn)\n",
    "    \n",
    "    # Load catalog of large events (for plotting milestone lines)\n",
    "    ### Create df of important events:\n",
    "    large_ev = prep_cat_zaliapin(large_ev_fn)\n",
    "    large_ev = large_ev.set_index('time')\n",
    "    large_ev = large_ev.sort_index()\n",
    "    \n",
    "    # Counting\n",
    "    cum_cat = cat_orig.loc[(cat_orig['time'] >= cat_start) & (cat_orig['mag'] >= 4.8)]\n",
    "    cum_cat = cum_cat.set_index('time')\n",
    "    cum_cat['count'] = 1.0\n",
    "    cum_cat = cum_cat.cumsum()\n",
    "    print('Total events: {}'.format(len(cum_cat.index)))\n",
    "\n",
    "    # And repeat on declustered catalog\n",
    "    cum_cat_declustered = cat_declust.copy(deep=True)\n",
    "    cum_cat_declustered = cum_cat_declustered.set_index('time')\n",
    "    cum_cat_declustered['count'] = 1.0\n",
    "    cum_cat_declustered = cum_cat_declustered.cumsum()\n",
    "    print('Events in declustered catalog: {}'.format(len(cum_cat_declustered)))\n",
    "\n",
    "    # Fit quadratics\n",
    "    from numpy.polynomial import Polynomial as P\n",
    "    time_diffs_orig = (1./(24.*60.*60.))*(cum_cat.index - cat_start).total_seconds()\n",
    "    counts_orig = cum_cat['count'].values\n",
    "    p_orig = P.fit(time_diffs_orig,counts_orig,2)\n",
    "    time_diffs_declust = (1./(24.*60.*60.))*(cum_cat_declustered.index - cat_start).total_seconds()\n",
    "    counts_declust = cum_cat_declustered['count'].values\n",
    "    p_declust = P.fit(time_diffs_declust,counts_declust,2)\n",
    "\n",
    "\n",
    "    # Plot cumulative no of events with time and fit a quadratic\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    ax1.plot(cum_cat.index.values, cum_cat['count'], 'b', label='Original catalog')\n",
    "    ax1.plot(cum_cat_declustered.index.values, cum_cat_declustered['count'], 'r', \n",
    "             label='Declustered catalog - {}'.format(method))\n",
    "    #ax1.plot(cum_cat.index.values, p_orig(time_diffs_orig), 'steelblue', label='quadratic fit - original')\n",
    "    #ax1.plot(cum_cat_declustered.index.values, p_declust(time_diffs_declust), 'salmon', label='quadratic fit - declustered')\n",
    "\n",
    "\n",
    "    # Plot important large evs\n",
    "    bottom, top = ax1.get_ylim()\n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=1.0)\n",
    "    indices = large_ev.index.values\n",
    "    for date, name, m in zip(large_ev.index.values, large_ev['name'], large_ev['mag']):\n",
    "        #ind_ev = indices.where(indices == date)\n",
    "        ax1.plot([date, date], [bottom, top], 'k--')\n",
    "        #ax.arrow(date, 0, 0, cum_cat['count'].values[ind_ev], head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "        ax1.text(date, bottom+400, (name+' $M_w$'+str(m)), size=\"large\", rotation=90,\n",
    "                         horizontalalignment='center', verticalalignment='center',\n",
    "                         rotation_mode='anchor', bbox=props)\n",
    "\n",
    "    # Tidy up plot\n",
    "    ax1.xaxis.set_tick_params(labelsize=14)\n",
    "    ax1.yaxis.set_tick_params(labelsize=14)\n",
    "    ax1.set_ylabel('Cumulative number of events', fontsize=14)\n",
    "    #ax1.set_title('All earthquakes', fontsize=16)\n",
    "    ax1.legend(loc='best', fontsize=14)\n",
    "    plt.show()\n",
    "    fig.savefig(fig_fpath, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_compare(data_path, cat_start, mc):\n",
    "    \"\"\"\n",
    "    Function to compare declustering methods against the original catalog\n",
    "    # data_path: filepath to folder holding the 2 declustered catalogs and the original catalog\n",
    "    # cat_start: time (datetime) past which rate data is considered reliable\n",
    "    # mc: the catalog completeness magnitude\n",
    "    \"\"\"\n",
    "    # Build filepaths\n",
    "    cat_declust_fn1 = data_path + 'Zaliapin_decluster/declustered_catalog_data.csv'\n",
    "    cat_declust_fn2 = data_path + 'Mizrahi_ETAS_decluster/ETAS_declustered_cat.csv'\n",
    "    cat_orig_fn = data_path + 'raw_catalog_data.csv'\n",
    "    \n",
    "    fn_large_ev = data_path + 'large_eq.csv'\n",
    "    fn_output = data_path + 'figs-PDF/' + 'cum_ev_compare_methods.pdf'\n",
    "    \n",
    "    # Load the catalogs\n",
    "    cat_declust1 = pd.read_csv(cat_declust_fn1, index_col=0, parse_dates=[\"time\"], dtype={\"url\": str, \"alert\": str})\n",
    "    cat_declust2 = pd.read_csv(cat_declust_fn2, index_col=0, parse_dates=[\"time\"], dtype={\"url\": str, \"alert\": str})\n",
    "    cat_orig = prep_cat_zaliapin(cat_orig_fn)\n",
    "    \n",
    "    # Load catalog of large events (for plotting milestone lines)\n",
    "    ### Create df of important events:\n",
    "    large_ev = prep_cat_zaliapin(fn_large_ev)\n",
    "    large_ev = large_ev.set_index('time')\n",
    "    large_ev = large_ev.sort_index()\n",
    "    \n",
    "    # Counting\n",
    "    m_cut = mc-(0.1/2)\n",
    "    cum_cat = cat_orig.loc[(cat_orig['time'] >= cat_start) & (cat_orig['mag'] >= m_cut)]\n",
    "    cum_cat = cum_cat.set_index('time')\n",
    "    cum_cat['count'] = 1.0\n",
    "    cum_cat = cum_cat.cumsum()\n",
    "    print('Total events: {}'.format(len(cum_cat.index)))\n",
    "\n",
    "    # And repeat on declustered catalog 1\n",
    "    cum_cat_declustered1 = cat_declust1.copy(deep=True)\n",
    "    cum_cat_declustered1 = cum_cat_declustered1.set_index('time')\n",
    "    cum_cat_declustered1['count'] = 1.0\n",
    "    cum_cat_declustered1 = cum_cat_declustered1.cumsum()\n",
    "    print('Events in Zaliapin declustered catalog: {}'.format(len(cum_cat_declustered1)))\n",
    "    \n",
    "    # And repeat on declustered catalog 2\n",
    "    cum_cat_declustered2 = cat_declust2.copy(deep=True)\n",
    "    cum_cat_declustered2 = cum_cat_declustered2.set_index('time')\n",
    "    cum_cat_declustered2['count'] = 1.0\n",
    "    cum_cat_declustered2 = cum_cat_declustered2.cumsum()\n",
    "    print('Events in Mizrahi declustered catalog: {}'.format(len(cum_cat_declustered2)))\n",
    "\n",
    "    # Plot cumulative no of events with time\n",
    "    fig, ax1 = plt.subplots(figsize=(20, 10))\n",
    "    ax1.plot(cum_cat.index.values, cum_cat['count'], 'k', label='Original catalog')\n",
    "    ax1.plot(cum_cat_declustered1.index.values, cum_cat_declustered1['count'], 'g', \n",
    "             label='Declustered catalog - Zaliapin')\n",
    "    ax1.plot(cum_cat_declustered2.index.values, cum_cat_declustered2['count'], 'orange', \n",
    "             label='Declustered catalog - Mizrahi-ETAS')\n",
    "\n",
    "    # Plot important large evs\n",
    "    bottom, top = ax1.get_ylim()\n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=1.0)\n",
    "    indices = large_ev.index.values\n",
    "    for date, name, m, dep in zip(large_ev.index.values, large_ev['name'], large_ev['mag'], large_ev['depth_km']):\n",
    "        colour = 'k'\n",
    "        if dep > 60.0 and dep < 200.0:\n",
    "            colour = 'royalblue'\n",
    "        elif dep < 60.0:\n",
    "            colour = 'r'\n",
    "        else:\n",
    "            colour = 'midnightblue'\n",
    "        ax1.plot([date, date], [bottom, top], ls='--', c=colour)\n",
    "        #ax.arrow(date, 0, 0, cum_cat['count'].values[ind_ev], head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "        ax1.text(date, top, (name+' $M_w$'+'{:.1f}'.format(m)), size=\"medium\", rotation=45,\n",
    "                         horizontalalignment='center', verticalalignment='center', c=colour,\n",
    "                         rotation_mode='anchor', bbox=props)\n",
    "\n",
    "    # Tidy up plot\n",
    "    ax1.xaxis.set_tick_params(labelsize=14)\n",
    "    ax1.yaxis.set_tick_params(labelsize=14)\n",
    "    ax1.set_ylabel('Cumulative number of events', fontsize=14)\n",
    "    #ax1.set_title('All earthquakes', fontsize=16)\n",
    "    ax1.legend(loc=4, fontsize=16)\n",
    "    plt.show()\n",
    "    fig.savefig(fn_output, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_compare('SAM_EQ_data/', dt.datetime(1980,1,1), 4.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare deep and shallow\n",
    "def plot_cumulative_compare(data_path, deep_lim, shallow_lim, method):\n",
    "    \"\"\"\n",
    "    Compare the deep and shallow catalogs\n",
    "    # data_path: filepath to folder holding the declustered catalog\n",
    "    # deep_lim: define deep catalog containing events of depth greater than this depth\n",
    "    # shallow_lim: define shallow catalog containing events of depth smaller than this depth\n",
    "    # method: name of the declustering method, type str (Zaliapin or Mizrahi)\n",
    "    \"\"\"\n",
    "    # Build filepaths\n",
    "    if method == 'Zaliapin':\n",
    "        declust_cat_fn = data_path + 'Zaliapin_decluster/declustered_catalog_data.csv'\n",
    "    elif method == 'Mizrahi':\n",
    "        declust_cat_fn = data_path + 'Mizrahi_ETAS_decluster/ETAS_declustered_cat.csv'\n",
    "    else:\n",
    "        raise ValueError('method must be type str, \"Mizrahi\" or \"Zaliapin\".')\n",
    "    \n",
    "    fn_large_ev = data_path + 'large_eq.csv'\n",
    "    fn_output = data_path + 'figs-PDF/' + 'compare_deep_shallow' + method + '_cat.pdf'\n",
    "    \n",
    "     # Load the catalogs\n",
    "    cat_declustered = pd.read_csv(declust_cat_fn, index_col=0, parse_dates=[\"time\"], dtype={\"url\": str, \"alert\": str})\n",
    "\n",
    "    # Deep declustered catalog\n",
    "    cum_deep_declustered = cat_declustered.loc[cat_declustered['depth_km'] > deep_lim]\n",
    "    cum_deep_declustered = cum_deep_declustered.set_index('time')\n",
    "    cum_deep_declustered['count'] = 1.0\n",
    "    cum_deep_declustered = cum_deep_declustered.cumsum()\n",
    "\n",
    "    # Shallow declustered catalog\n",
    "    cum_shallow_declustered = cat_declustered.loc[cat_declustered['depth_km'] < shallow_lim]\n",
    "    cum_shallow_declustered = cum_shallow_declustered.set_index('time')\n",
    "    cum_shallow_declustered['count'] = 1.0\n",
    "    cum_shallow_declustered = cum_shallow_declustered.cumsum()\n",
    "\n",
    "    # Plot cumulative no of events with time\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    ax1.plot(cum_deep_declustered.index.values, cum_deep_declustered['count']/len(cum_deep_declustered.index),\n",
    "             'b', label='Deep EQs ($z\\geq${}km)'.format(deep_lim))\n",
    "    ax1.plot(cum_shallow_declustered.index.values, cum_shallow_declustered['count']/len(cum_shallow_declustered.index),\n",
    "             'r', label='Shallow EQs($z\\leq${}km)'.format(shallow_lim))\n",
    "\n",
    "    # Tidy up plot\n",
    "    ax1.set_title('Catalog using {} declustering'.format(method))\n",
    "    ax1.xaxis.set_tick_params(labelsize=14)\n",
    "    ax1.yaxis.set_tick_params(labelsize=14)\n",
    "    ax1.set_ylabel('Normalised cumulative \\n # of events', fontsize=14)\n",
    "    #ax1.set_title('All earthquakes', fontsize=16)\n",
    "    ax1.legend(loc='best', fontsize=14)\n",
    "    plt.show()\n",
    "    fig.savefig(fn_output, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_compare('Jara_search_area/', 80.0, 40.0, 'Zaliapin')\n",
    "plot_cumulative_compare('Jara_search_area/', 80.0, 40.0, 'Mizrahi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rolling averages ##\n",
    "# Plot the removed EQs vs the ones remaining in the declustered catalog\n",
    "def plot_and_compare_30D_rates_rolling(region, box, dep_lims, method):\n",
    "    \"\"\"\n",
    "    Plots bar chart of 30d sliding window rate for the declustered and rejected catalogs\n",
    "    # region: The study region (type str) - e.g. Japan, SAM...\n",
    "    # box: the subregion of interest -e.g. Arequipa, Atacama etc.\n",
    "                directory region_EQ_data must contain subfolder zonal_cat/box/ -  for each 'box', CSV files for \n",
    "                declustered catalog, rejected catalog must be present\n",
    "    # dep_lims: list of depth limits - supply 3 numbers to plot 2 depth bands; \n",
    "                supply 4 numbers to plot 3 depth bands (last number must be deeper limit to consider seismicity)\n",
    "    # method: name of the declustering method, type str (Zaliapin or Mizrahi)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build filepaths\n",
    "    root_dir = region + '_EQ_data/'\n",
    "    if method == 'Zaliapin':\n",
    "        declust_cat_fn = root_dir + 'zonal_cat/' + box + '/declustered_catalog_data.csv'\n",
    "        rejected_cat_fn = root_dir + 'zonal_cat/' + box + '/rejected_ev.csv'\n",
    "    elif method == 'Mizrahi':\n",
    "        declust_cat_fn = root_dir + 'zonal_cat/' + box + '/ETAS_declustered_cat.csv'\n",
    "        rejected_cat_fn = root_dir + 'zonal_cat/' + box + '/ETAS_rejected_evs.csv'\n",
    "    else:\n",
    "        raise ValueError('method must be type str, \"Mizrahi\" or \"Zaliapin\".')\n",
    "    \n",
    "    fn_large_ev = root_dir + 'large_eq.csv'\n",
    "\n",
    "    \n",
    "    # Load the catalogs\n",
    "    declust_cat = pd.read_csv(declust_cat_fn, index_col=0, parse_dates=[\"time\"], dtype={\"url\": str, \"alert\": str})\n",
    "    rejected_cat = pd.read_csv(rejected_cat_fn, index_col=0, parse_dates=[\"time\"], dtype={\"url\": str, \"alert\": str})\n",
    "\n",
    "    resampled_declustered = declust_cat.set_index('time').groupby(pd.Grouper(freq='30D')).count()\n",
    "    \n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    # Plot important large evs\n",
    "    # Load catalog of large events (for plotting milestone lines)\n",
    "    large_ev_cat = prep_cat_zaliapin(fn_large_ev)\n",
    "    \n",
    "    # Get the target regions\n",
    "    polys, zones = load_polys(region)\n",
    "\n",
    "    # Only include large events within 1000 km of centroid of zone\n",
    "    clonrad = np.radians((polys.loc[polys['name']==box])['centroid_geog'].x.values)\n",
    "    clatrad = np.radians((polys.loc[polys['name']==box])['centroid_geog'].y.values)\n",
    "    large_ev_cat['dist'] = haversine(clatrad,large_ev_cat['lat_rad'],clonrad,large_ev_cat['lon_rad'])\n",
    "    large_ev = large_ev_cat.loc[large_ev_cat['dist']<=1000.0]\n",
    "    large_ev = large_ev.set_index('time')\n",
    "    large_ev = large_ev.sort_index()\n",
    "    \n",
    "\n",
    "    if len(dep_lims) == 3:\n",
    "        # Deep declustered catalog\n",
    "        cum_deep_declustered = declust_cat.loc[(declust_cat['depth_km'] > dep_lims[1]) & \n",
    "                                               (declust_cat['depth_km'] <= dep_lims[2])]\n",
    "        cum_deep_declustered_roll = cum_deep_declustered.set_index('time')\n",
    "        cum_deep_declustered_roll['count'] = 1.0\n",
    "        cum_deep_declustered_roll = cum_deep_declustered_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # Shallow declustered catalog\n",
    "        cum_shallow_declustered = declust_cat.loc[declust_cat['depth_km'] < dep_lims[0]]\n",
    "        cum_shallow_declustered_roll = cum_shallow_declustered.set_index('time')\n",
    "        cum_shallow_declustered_roll['count'] = 1.0\n",
    "        cum_shallow_declustered_roll = cum_shallow_declustered_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # Deep catalog of clusters\n",
    "        cum_deep_clusters = rejected_cat.loc[(rejected_cat['depth_km'] > dep_lims[1]) &\n",
    "                                             (rejected_cat['depth_km'] <= dep_lims[2])]\n",
    "        cum_deep_clusters_roll = cum_deep_clusters.set_index('time')\n",
    "        cum_deep_clusters_roll['count'] = 1.0\n",
    "        cum_deep_clusters_roll = cum_deep_clusters_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # Shallow catalog of clusters\n",
    "        cum_shallow_clusters = rejected_cat.loc[rejected_cat['depth_km'] < dep_lims[0]]\n",
    "        cum_shallow_clusters_roll = cum_shallow_clusters.set_index('time')\n",
    "        cum_shallow_clusters_roll['count'] = 1.0\n",
    "        cum_shallow_clusters_roll = cum_shallow_clusters_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # convert the date format to matplotlib date format \n",
    "        #plt_date = mdates.date2num(resampled_declustered.index.values)\n",
    "        #bins = mdates.datestr2num([\"{}/01/01\".format(i) for i in np.arange(1970, 2015)])\n",
    "        # plot it\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, sharex=True, figsize=(30,20))\n",
    "        fig.suptitle('{} 30-day earthquake rate \\n (using {} declustering)'.format(box, method),\n",
    "                     x=0.5, y=0.93, fontsize=18)\n",
    "\n",
    "\n",
    "        # Plot shallow declustered\n",
    "        ax1.bar(cum_shallow_declustered_roll.index.values, cum_shallow_declustered_roll.to_numpy(), color='r', \n",
    "                label='$z<${:.1f} km - declustered'.format(dep_lims[0]))\n",
    "\n",
    "        # Plot shallow clusters\n",
    "        ax2.bar(cum_shallow_clusters_roll.index.values, cum_shallow_clusters_roll.to_numpy(), color='r', \n",
    "                label='$z<${:.1f} km - rejected'.format(dep_lims[0]))\n",
    "        \n",
    "        # Plot deep declustered\n",
    "        ax3.bar(cum_deep_declustered_roll.index.values, cum_deep_declustered_roll.to_numpy(), color='b', \n",
    "                label='{:.1f}$<z<${:.1f} km - declustered'.format(dep_lims[1], dep_lims[2]))\n",
    "\n",
    "        # Plot deep clusters\n",
    "        ax4.bar(cum_deep_clusters_roll.index.values, cum_deep_clusters_roll.to_numpy(), \n",
    "                color='b', label='{:.1f}$<z<${:.1f} km - rejected'.format(dep_lims[1], dep_lims[2]))\n",
    "\n",
    "        # x ticks and limit\n",
    "        ax4.xaxis.set_tick_params(labelsize=12, labelrotation=0)\n",
    "        \n",
    "        indices = large_ev.index.values\n",
    "        \n",
    "        #Finalise plots\n",
    "        for ax in [ax1,ax2,ax3,ax4]:\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator(5))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            ax.yaxis.set_tick_params(labelsize=12)\n",
    "            ax.set_ylabel('EQs/30 days', fontsize=14)\n",
    "            ax.legend(loc='upper left', fontsize=14)\n",
    "            #Plot the large events\n",
    "            for date, name, m in zip(large_ev.index.values, large_ev['name'], large_ev['mag']):\n",
    "                ax.axvline(date,color='k', ls='--')\n",
    "        \n",
    "            \n",
    "    elif len(dep_lims) == 4:\n",
    "        # D1 declustered catalog\n",
    "        cum_D1_declustered = declust_cat.loc[(declust_cat['depth_km'] <= dep_lims[2]) & \n",
    "                                             (declust_cat['depth_km'] >= dep_lims[1])]\n",
    "        cum_D1_declustered_roll = cum_D1_declustered.set_index('time')\n",
    "        cum_D1_declustered_roll['count'] = 1.0\n",
    "        cum_D1_declustered_roll = cum_D1_declustered_roll['count'].rolling('30D').sum()\n",
    "        \n",
    "        # D2 declustered catalog\n",
    "        cum_D2_declustered = declust_cat.loc[(declust_cat['depth_km'] > dep_lims[2]) & \n",
    "                                             (declust_cat['depth_km'] <= dep_lims[3])]\n",
    "        cum_D2_declustered_roll = cum_D2_declustered.set_index('time')\n",
    "        cum_D2_declustered_roll['count'] = 1.0\n",
    "        cum_D2_declustered_roll = cum_D2_declustered_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # Shallow declustered catalog\n",
    "        cum_shallow_declustered = declust_cat.loc[declust_cat['depth_km'] < dep_lims[0]]\n",
    "        cum_shallow_declustered_roll = cum_shallow_declustered.set_index('time')\n",
    "        cum_shallow_declustered_roll['count'] = 1.0\n",
    "        cum_shallow_declustered_roll = cum_shallow_declustered_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # D1 catalog of clusters\n",
    "        cum_D1_clusters = rejected_cat.loc[(rejected_cat['depth_km'] <= dep_lims[2]) &\n",
    "                                           (rejected_cat['depth_km'] >= dep_lims[1])]\n",
    "        cum_D1_clusters_roll = cum_D1_clusters.set_index('time')\n",
    "        cum_D1_clusters_roll['count'] = 1.0\n",
    "        cum_D1_clusters_roll = cum_D1_clusters_roll['count'].rolling('30D').sum()\n",
    "        \n",
    "        # D2 catalog of clusters\n",
    "        cum_D2_clusters = rejected_cat.loc[(rejected_cat['depth_km'] > dep_lims[2]) & \n",
    "                                           (rejected_cat['depth_km'] <= dep_lims[3])]\n",
    "        cum_D2_clusters_roll = cum_D2_clusters.set_index('time')\n",
    "        cum_D2_clusters_roll['count'] = 1.0\n",
    "        cum_D2_clusters_roll = cum_D2_clusters_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # Shallow catalog of clusters\n",
    "        cum_shallow_clusters = rejected_cat.loc[rejected_cat['depth_km'] < dep_lims[0]]\n",
    "        cum_shallow_clusters_roll = cum_shallow_clusters.set_index('time')\n",
    "        cum_shallow_clusters_roll['count'] = 1.0\n",
    "        cum_shallow_clusters_roll = cum_shallow_clusters_roll['count'].rolling('30D').sum()\n",
    "\n",
    "        # convert the date format to matplotlib date format \n",
    "        #plt_date = mdates.date2num(resampled_declustered.index.values)\n",
    "        # plot it\n",
    "        fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6, sharex=True, figsize=(30,30))\n",
    "        fig.suptitle('{} 30-day earthquake rate \\n (using {} declustering)'.format(box, method),\n",
    "                     x=0.5, y=0.93, fontsize=18)\n",
    "\n",
    "        # Plot shallow declustered\n",
    "        ax1.bar(cum_shallow_declustered_roll.index.values, cum_shallow_declustered_roll.to_numpy(), color='r', \n",
    "                label='$z<${:.1f} km - declustered'.format(dep_lims[0]))\n",
    "\n",
    "        # Plot shallow clusters\n",
    "        ax2.bar(cum_shallow_clusters_roll.index.values, cum_shallow_clusters_roll.to_numpy(),\n",
    "                color='r', label='$z<${:.1f} km - rejected'.format(dep_lims[0]))\n",
    "        \n",
    "        # Plot D1 declustered\n",
    "        ax3.bar(cum_D1_declustered_roll.index.values, cum_D1_declustered_roll.to_numpy(), \n",
    "                color='royalblue', label='{:.1f}$<z<${:.1f} km - declustered'.format(dep_lims[1], dep_lims[2]))\n",
    "\n",
    "        # Plot D1 clusters\n",
    "        ax4.bar(cum_D1_clusters_roll.index.values, cum_D1_clusters_roll.to_numpy(),\n",
    "                color='royalblue', label='{:.1f}$<z<${:.1f} km - rejected'.format(dep_lims[1], dep_lims[2]))\n",
    "        \n",
    "        # Plot D2 declustered\n",
    "        ax5.bar(cum_D2_declustered_roll.index.values, cum_D2_declustered_roll.to_numpy(), \n",
    "                color='midnightblue', label='{:.1f}$<z<${:.1f} km - declustered'.format(dep_lims[2], dep_lims[3]))\n",
    "\n",
    "        # Plot D2 clusters\n",
    "        ax6.bar(cum_D2_clusters_roll.index.values, cum_D2_clusters_roll.to_numpy(),\n",
    "                color='midnightblue', label='{:.1f}$<z<${:.1f} km - rejected'.format(dep_lims[2], dep_lims[3]))\n",
    "\n",
    "\n",
    "        # x ticks and limit\n",
    "        ax6.xaxis.set_tick_params(labelsize=12, labelrotation=0)\n",
    "\n",
    "        # Plot important large evs\n",
    "        # Load catalog of large events (for plotting milestone lines)\n",
    "        large_ev = prep_cat_zaliapin(fn_large_ev)\n",
    "        large_ev = large_ev.set_index('time')\n",
    "        large_ev = large_ev.sort_index()\n",
    "\n",
    "        indices = large_ev.index.values\n",
    "        \n",
    "        #Finalise plots\n",
    "        for ax in [ax1,ax2,ax3,ax4,ax5,ax6]:\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator(5))\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            ax.yaxis.set_tick_params(labelsize=12)\n",
    "            ax.set_ylabel('EQs/30 days', fontsize=14)\n",
    "            ax.legend(loc='upper left', fontsize=14)\n",
    "            #Plot the large events\n",
    "            for date, name, m in zip(large_ev.index.values, large_ev['name'], large_ev['mag']):\n",
    "                ax.axvline(date,color='k', ls='--')\n",
    "        #for date, name, m in zip(large_ev.index.values, large_ev['name'], large_ev['mag']):\n",
    "            #ax1.text(date,ax1.get_ylim()[1],name,ha='center',va='center', fontsize='large', rotation=0, bbox=props)\n",
    "            \n",
    "    # Tidy up plots\n",
    "    for event in large_ev.itertuples():\n",
    "        region = polys.loc[polys['name']==box]\n",
    "        region.reset_index(drop=True, inplace=True)\n",
    "        hyp = gpd.GeoSeries([Point(event.lon, event.lat)])\n",
    "        if hyp.within(region.at[0,'geometry']).values:\n",
    "            props = dict(boxstyle='round',facecolor='white',alpha=1.0)\n",
    "            if event.depth_km <= 60.0:\n",
    "                ax1.text(event.Index,ax1.get_ylim()[1],event.name,ha='center',va='center', \n",
    "                         fontsize='large', rotation=0, color='r', bbox=props)\n",
    "            elif event.depth_km > 60.0:\n",
    "                ax1.text(event.Index,ax1.get_ylim()[1],event.name,ha='center',va='center', \n",
    "                         fontsize='large', rotation=0, color='b', bbox=props)\n",
    "        else:\n",
    "            props = dict(boxstyle='round',facecolor='white',ls='--',alpha=1.0)\n",
    "            ax1.text(event.Index,ax1.get_ylim()[1],event.name,ha='center',va='center', \n",
    "                 fontsize='large', rotation=0, color='grey', bbox=props)\n",
    "\n",
    "    plt.xlabel('Start of 30 day period', fontsize=14)\n",
    "    plt.show()\n",
    "    # Save to file\n",
    "    outdir = root_dir+'figs-PDF/rates/'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fn_output = outdir+box+'_'+method +'_declust_'+'30d_rate_comparisonsbar.pdf'\n",
    "    fig.savefig(fn_output, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_method in ['Zaliapin', 'Mizrahi']:\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Colombia', [60.0, 80.0, 250.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Ecuador', [60.0, 80.0, 250.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Peru', [60.0, 80.0, 200.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Arequipa', [50.0, 90.0, 250.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'North_Chile', [70.0, 70.0, 150.0, 250.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Atacama', [70.0, 70.0, 120.0, 250.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Central_Chile', [60.0, 80.0, 100.0, 250.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Maule', [40.0, 80.0, 250.0], d_method)\n",
    "    plot_and_compare_30D_rates_rolling('SAM', 'Jara_target', [40.0, 80.0, 900.0], d_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the linewidth into dataunits for use with logscale\n",
    "\n",
    "def linewidth_from_data_units(linewidth, axis, reference='y'):\n",
    "    \"\"\"\n",
    "    Convert a linewidth in data units to linewidth in points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    linewidth: float\n",
    "        Linewidth in data units of the respective reference-axis\n",
    "    axis: matplotlib axis\n",
    "        The axis which is used to extract the relevant transformation\n",
    "        data (data limits and size must not change afterwards)\n",
    "    reference: string\n",
    "        The axis that is taken as a reference for the data width.\n",
    "        Possible values: 'x' and 'y'. Defaults to 'y'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    linewidth: float\n",
    "        Linewidth in points\n",
    "    \"\"\"\n",
    "    fig = axis.get_figure()\n",
    "    if reference == 'x':\n",
    "        length = fig.bbox_inches.width * axis.get_position().width\n",
    "        value_range = np.diff(axis.get_xlim())\n",
    "    elif reference == 'y':\n",
    "        length = fig.bbox_inches.height * axis.get_position().height\n",
    "        value_range = np.diff(axis.get_ylim())\n",
    "    # Convert length to points\n",
    "    length *= 72\n",
    "    # Scale linewidth to value range\n",
    "    return linewidth * (length / value_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jara 'periodogram'\n",
    "def plot_periodogram(region, params_dict):\n",
    "    \"\"\"\n",
    "    Plots similar figure to Jara et al. (2017), Fig.3a-b\n",
    "    # region: the region of focus, type str (e.g. Japan, SAM)\n",
    "    # params_dict: dict of function parameters, necessary keywords:\n",
    "        # box: subregion, type str (e.g. Arequipa)\n",
    "        # dep_lim: type list, depth filtering limits, upper and lower, e.g. \n",
    "        # dep_range: controls whether to plot the periodogram for deep or shallow events\n",
    "        # method: name of the declustering method, type str (Zaliapin or Mizrahi)\n",
    "        # c_palette: cmap to use for plotting the periodogram\n",
    "    \"\"\"\n",
    "    # Build filepaths\n",
    "    box = params_dict[\"box\"]\n",
    "    root_dir = region + '_EQ_data/'\n",
    "    method = params_dict[\"method\"]\n",
    "    if method == 'Zaliapin':\n",
    "        fn_declust_cat = root_dir + 'zonal_cat/' + box +  '/declustered_catalog_data.csv'\n",
    "    elif method == 'Mizrahi':\n",
    "        fn_declust_cat = root_dir + 'zonal_cat/' + box + '/ETAS_declustered_cat.csv'\n",
    "    else:\n",
    "        raise ValueError('method must be type str, \"Mizrahi\" or \"Zaliapin\".')\n",
    "    \n",
    "    fn_large_ev = root_dir + 'large_eq.csv'\n",
    "    \n",
    "    # Load the catalogs\n",
    "    declust_cat = pd.read_csv(fn_declust_cat, index_col=0, parse_dates=[\"time\"], dtype={\"url\": str, \"alert\": str})\n",
    "\n",
    "    # Declustered catalog with correct depth filter\n",
    "    assert (len(params_dict[\"dep_lim\"]) == 2), (\"Provide upper and lower depth limits.\")\n",
    "    assert (params_dict[\"dep_lim\"][0] < params_dict[\"dep_lim\"][1]), (\"Need upper depth limit < lower depth limit\")\n",
    "\n",
    "    upper_lim = params_dict[\"dep_lim\"][0]\n",
    "    lower_lim = params_dict[\"dep_lim\"][1]\n",
    "    cum_declustered = declust_cat.loc[(declust_cat['depth_km'] >= upper_lim) & \n",
    "                                      (declust_cat['depth_km'] < lower_lim)]\n",
    "    cum_declustered_roll = cum_declustered.set_index('time')\n",
    "    cum_declustered_roll['count'] = 1.0\n",
    "\n",
    "    \n",
    "    import matplotlib.dates as mdates\n",
    "    \n",
    "    win_lengths = np.logspace(0, 2.85, 20, base=10.0, endpoint=True)\n",
    "    windows = ['{}D'.format(i) for i in win_lengths.astype('int')]\n",
    "    l_widths = np.flip(np.logspace(0, 0.5, len(win_lengths), base=2.0, endpoint=True))\n",
    "\n",
    "\n",
    "    from matplotlib.collections import LineCollection\n",
    "    fig, ax = plt.subplots(figsize=(13,7))\n",
    "    fig.suptitle('{}:{} events at {:.1f}$<z<${:.1f} km using {} declustering'.format(region, box, upper_lim, \n",
    "                lower_lim, method),x=0.5, y=1, fontsize=14)\n",
    "\n",
    "\n",
    "    for win, win_length, l_w in zip(windows, win_lengths, l_widths):\n",
    "        rolling_df = cum_declustered_roll['count'].rolling(win, center=True).sum()\n",
    "        norm_vals = rolling_df.to_numpy()/rolling_df.to_numpy().max()\n",
    "        dates = rolling_df.index.values\n",
    "        y = np.full(len(dates), win_length)\n",
    "        s = pd.Series(y, index=dates)\n",
    "        #convert dates to numbers first\n",
    "        inxval = mdates.date2num(s.index.to_pydatetime())\n",
    "        points = np.array([inxval, s.values]).T.reshape(-1,1,2)\n",
    "        segments = np.concatenate([points[:-1],points[1:]], axis=1)\n",
    "        norm = plt.Normalize(norm_vals.min(), norm_vals.max())\n",
    "        lc = LineCollection(segments, cmap=params_dict[\"c_palette\"], linewidth=18.5, norm=norm)\n",
    "        #print(linewidth_from_data_units(l_w, ax, reference='y'))\n",
    "        # set color to date values\n",
    "        lc.set_array(norm_vals)\n",
    "        # note that you could also set the colors according to y values\n",
    "        # lc.set_array(s.values)\n",
    "        # add collection to axes\n",
    "        line = ax.add_collection(lc)\n",
    "\n",
    "    fig.colorbar(line, ax=ax, label='# of events normalised against max #events in window')\n",
    "\n",
    "    ## Plot important large evs ##\n",
    "    # Load catalog of large events (for plotting milestone lines)\n",
    "    large_ev_cat = prep_cat_zaliapin(fn_large_ev)\n",
    "    \n",
    "    # Get the target regions\n",
    "    polys, zones = load_polys(region)\n",
    "\n",
    "    # Only include large events within 1000 km of centroid of zone\n",
    "    clonrad = np.radians((polys.loc[polys['name']==box])['centroid_geog'].x.values)\n",
    "    clatrad = np.radians((polys.loc[polys['name']==box])['centroid_geog'].y.values)\n",
    "    large_ev_cat['dist'] = haversine(clatrad,large_ev_cat['lat_rad'],clonrad,large_ev_cat['lon_rad'])\n",
    "    large_ev = large_ev_cat.loc[large_ev_cat['dist']<=1000.0]\n",
    "    large_ev = large_ev.set_index('time')\n",
    "    large_ev = large_ev.sort_index()\n",
    "\n",
    "    # these are matplotlib.patch.Patch properties\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=1.0)\n",
    "    indices = large_ev.index.values\n",
    "    #for date, name in zip(large_ev.index.values, large_ev['name']):\n",
    "        #ax.axvline(date,color='k', ls='--')\n",
    "        #ax.text(date,ax.get_ylim()[1]-0.2,name,rotation=45, ha='center', va='center', \n",
    "                #rotation_mode='anchor', bbox=props)\n",
    "        \n",
    "    # Tidy up plots\n",
    "    for event in large_ev.itertuples():\n",
    "        region = polys.loc[polys['name']==box]\n",
    "        region.reset_index(drop=True, inplace=True)\n",
    "        hyp = gpd.GeoSeries([Point(event.lon, event.lat)])\n",
    "        if hyp.within(region.at[0,'geometry']).values:\n",
    "            props = dict(boxstyle='round',facecolor='white',alpha=1.0)\n",
    "            if event.depth_km <= 60.0:\n",
    "                ax.axvline(event.Index,color='r', ls='-')\n",
    "                ax.text(event.Index,ax.get_ylim()[1]-0.2,event.name,ha='center',va='center',\n",
    "                        rotation=45, color='r', rotation_mode='anchor', bbox=props)\n",
    "            elif event.depth_km > 60.0:\n",
    "                ax.axvline(event.Index,color='b', ls='-')\n",
    "                ax.text(event.Index,ax.get_ylim()[1]-0.2,event.name,ha='center',va='center', \n",
    "                         rotation=45, color='b', rotation_mode='anchor', bbox=props)\n",
    "        else:\n",
    "            props = dict(boxstyle='round',facecolor='white',ls='--',alpha=1.0)\n",
    "            if event.depth_km <= 60.0:\n",
    "                ax.axvline(event.Index,color='r', ls='--')\n",
    "                ax.text(event.Index,ax.get_ylim()[1]-0.2,event.name,ha='center',va='center',\n",
    "                        rotation=45, color='r', alpha=0.7, rotation_mode='anchor', bbox=props)\n",
    "            elif event.depth_km > 60.0:\n",
    "                ax.axvline(event.Index,color='b', ls='--')\n",
    "                ax.text(event.Index,ax.get_ylim()[1]-0.2,event.name,ha='center',va='center', \n",
    "                         rotation=45, color='b', alpha=0.7, rotation_mode='anchor', bbox=props)\n",
    "    \n",
    "    # Tidy up the plot\n",
    "    ax.set_yscale('log')\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator(5))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "    ax.yaxis.set_tick_params(labelsize=12)\n",
    "    ax.set_ylabel('Length of reference time window [days]', fontsize=12)\n",
    "    ax.set_xlabel('Start of sliding time window', fontsize=12)\n",
    "    #ax.legend(loc='upper left', fontsize=14)\n",
    "    \n",
    "    # x ticks and limit\n",
    "    ax.xaxis.set_tick_params(labelsize=12, labelrotation=0)   \n",
    "\n",
    "    ax.autoscale_view()\n",
    "    plt.show()\n",
    "    \n",
    "    lower_lim = str(int(lower_lim))\n",
    "    upper_lim = str(int(upper_lim))\n",
    "    outdir = root_dir + 'figs-PDF/periodograms/'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fn_output = outdir+box+'_'+method+'_'+upper_lim+'-'+lower_lim+'_declust_cat_periodogram.pdf'\n",
    "    fig.savefig(fn_output, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Produce figs for SAM depth bands\n",
    "cases = pd.read_csv('SAM_EQ_data/SAM_periodogram_params.csv', index_col='id')\n",
    "\n",
    "for method in ['Zaliapin', 'Mizrahi']:\n",
    "    for case in cases.itertuples():\n",
    "        params = {\"box\": case.box_name, \"dep_lim\": [case.u_lim, case.l_lim], \"method\": method, \"c_palette\": case.color,}\n",
    "        plot_periodogram('SAM', params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOTTING ####\n",
    "\n",
    "alt_cat = cat_preprocessed.set_index('time')\n",
    "# Plot events magnitude and time\n",
    "fig, ax2 = plt.subplots(figsize=(15, 10))\n",
    "for date, magnitude in zip(alt_cat.index.values, alt_cat['mag']):\n",
    "    ax2.plot([date, date],[0, magnitude],'k')\n",
    "ax2.plot(alt_cat.index.values, alt_cat['mag'], 'ko')\n",
    "ax2.set_ylim(bottom=4.0)\n",
    "ax2.xaxis.set_tick_params(labelsize=14)\n",
    "ax2.yaxis.set_tick_params(labelsize=14)\n",
    "ax2.set_xlabel('Years', fontsize=14)\n",
    "ax2.set_ylabel('Magnitude', fontsize=14)\n",
    "\n",
    "# Plot events with latitude and time\n",
    "fig, ax3 = plt.subplots(figsize=(15, 10))\n",
    "im = ax3.scatter(alt_cat.index.values, alt_cat['lat'], c=alt_cat['depth_km'], s=2*((alt_cat['mag'])**2.5), cmap='viridis')\n",
    "ax3.xaxis.set_tick_params(labelsize=14)\n",
    "ax3.yaxis.set_tick_params(labelsize=14)\n",
    "ax3.set_xlabel('Years', fontsize=14)\n",
    "ax3.set_ylabel('Latitude', fontsize=14)\n",
    "\n",
    "# produce a legend with sizes from the scatter\n",
    "kw = dict(prop=\"sizes\", num=8, color='k', fmt=\"{x:.1f}\",\n",
    "          func=lambda s: (s/2)**(1/2.5))\n",
    "legend2 = ax3.legend(*im.legend_elements(**kw),\n",
    "                    loc=\"best\", title=\"Magnitude\", bbox_to_anchor=(1.3, 1.0), fontsize=12)\n",
    "legend2.get_title().set_fontsize('12')\n",
    "\n",
    "# Add a colorbar for depth\n",
    "cbar = fig.colorbar(im, ax=ax3, label='depth[km]')\n",
    "cbar.set_label('depth[km]', fontsize=12)\n",
    "\n",
    "#plt.show()\n",
    "#fig.savefig('rate_graph.pdf', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOTTING for Iquique 2014 ##\n",
    "\n",
    "alt_cat = cat_preprocessed.loc[(cat_preprocessed['time'] > datetime(2013,1,1))]\n",
    "alt_cat = alt_cat.loc[(alt_cat['lat'] > -24.0) & (alt_cat['lat'] < -18.0) & (alt_cat['lon'] > -74.0) & (alt_cat['lon'] < -68.0)]\n",
    "alt_cat = alt_cat.set_index('time')\n",
    "# Plot events magnitude and time\n",
    "fig, ax2 = plt.subplots(figsize=(15, 10))\n",
    "for date, magnitude in zip(alt_cat.index.values, alt_cat['mag']):\n",
    "    ax2.plot([date, date],[0, magnitude],'k')\n",
    "ax2.plot(alt_cat.index.values, alt_cat['mag'], 'ko')\n",
    "ax2.set_ylim(bottom=4.0)\n",
    "ax2.xaxis.set_tick_params(labelsize=14)\n",
    "ax2.yaxis.set_tick_params(labelsize=14)\n",
    "ax2.set_xlabel('Years', fontsize=14)\n",
    "ax2.set_ylabel('Magnitude', fontsize=14)\n",
    "\n",
    "# Plot events with latitude and time\n",
    "fig, ax3 = plt.subplots(figsize=(15, 10))\n",
    "im = ax3.scatter(alt_cat['lon'], alt_cat['lat'], s=2*((alt_cat['mag'])**3), c=alt_cat.index.values, cmap='viridis')\n",
    "ax3.xaxis.set_tick_params(labelsize=14)\n",
    "ax3.yaxis.set_tick_params(labelsize=14)\n",
    "ax3.set_xlabel('Longitude', fontsize=14)\n",
    "ax3.set_ylabel('Latitude', fontsize=14)\n",
    "\n",
    "# produce a legend with sizes from the scatter\n",
    "#kw = dict(prop=\"sizes\", num=8, color='k', fmt=\"{x:.1f}\",\n",
    "          #func=lambda s: (s/2)**(1/2.5))\n",
    "#legend2 = ax3.legend(*im.legend_elements(**kw),\n",
    "                    #loc=\"best\", title=\"Magnitude\", bbox_to_anchor=(1.3, 1.0), fontsize=12)\n",
    "#legend2.get_title().set_fontsize('12')\n",
    "\n",
    "# Add a colorbar for depth\n",
    "cbar = fig.colorbar(im, ax=ax3, label='date')\n",
    "#cbar.set_label('depth[km]', fontsize=12)\n",
    "\n",
    "#plt.show()\n",
    "#fig.savefig('rate_graph.pdf', dpi=200, bbox_inches='tight')\n",
    "\n",
    "#alt_cat.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G-R analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAG-FREQ PLOT\n",
    "\n",
    "def G_R_plotting(cat, region, zone, dmag, mc):\n",
    "    \"\"\" Plot the Gutenberg-Richter plot for a catalog\n",
    "        # cat must be a dataframe containing a column for event magnitudes (col label ='mag')\n",
    "        # dmag is the magnitude step\n",
    "        # zone: name of the subregion of interest\n",
    "    \"\"\"\n",
    "    # Define magnitude bins (includes left edge of first bin and right edge of last bin)\n",
    "    mag_bins = np.arange(cat.mag.min(),cat.mag.max()+dmag,dmag)\n",
    "    \n",
    "    # Count up number of earthquakes in each bin (in ascending order)\n",
    "    mag_counts = cat['mag'].value_counts(bins=mag_bins, ascending=True)\n",
    "    mag_counts.sort_index(inplace=True)\n",
    "    #print(mag_counts.values)\n",
    "    #print('counts in mag bins', mag_counts.values)\n",
    "\n",
    "    N_ascend = mag_counts[::-1].values.cumsum()\n",
    "\n",
    "    # Flip N to descending magnitude to match mag_bins\n",
    "    N = np.array(N_ascend[::-1])\n",
    "    #print(N)\n",
    "\n",
    "    # Make sure all values of N are finite (since can't plot zero on log scale)\n",
    "    ind_finite = N > 0\n",
    "    N = N[ind_finite]\n",
    "    M = mag_bins[:-1][ind_finite]\n",
    "\n",
    "    # Estimate G-R params\n",
    "    #log_N = np.log10(N)\n",
    "    #polycoeffs = np.polyfit(M, log_N, 1)\n",
    "    #p1 = np.poly1d(polycoeffs)\n",
    "\n",
    "    # Plot G-R plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    #ax.plot(M, p1(M), label='$\\log(N) = $%.3f $%.3f$M' % (polycoeffs[1], polycoeffs[0]))\n",
    "    ax.plot(M, np.log10(N), 'bx')\n",
    "    ax.xaxis.set_tick_params(labelsize=14)\n",
    "    ax.yaxis.set_tick_params(labelsize=14)\n",
    "    ax.set_xlabel('Magnitude', fontsize=14)\n",
    "    ax.set_ylabel('$log(N)$', fontsize=14)\n",
    "    #ax.legend(loc='best', fontsize=12)\n",
    "    ax.set_title('{} ($M_c$={})'.format(zone, mc))\n",
    "    plt.show()\n",
    "    # Save file\n",
    "    root_dir = region+'_EQ_data/'\n",
    "    outdir = root_dir + 'figs-PDF/G-R_plots/'\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    fn_output = outdir+zone+'_G-R_plot.pdf'\n",
    "    fig.savefig(fn_output, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First find the best-fitting completeness magnitude and the best-fitting b-value\n",
    "## Using code by Mizrahi et al. (2021) as sourced from https://github.com/lmizrahi/etas/blob/main/mc_b_est.py\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# joint beta and completeness magnitude estimation\n",
    "# using p-value of Kolmogorov-Smirnov distance to fitted Gutenberg-Richter law\n",
    "#\n",
    "# as described by Mizrahi et al., 2021\n",
    "# Leila Mizrahi, Shyam Nandan, Stefan Wiemer;\n",
    "# The Effect of Declustering on the Size Distribution of Mainshocks.\n",
    "# Seismological Research Letters 2021; doi: https://doi.org/10.1785/0220200231\n",
    "# inspired by method of Clauset et al., 2009\n",
    "##############################################################################\n",
    "\n",
    "# mc is the binned completeness magnitude,\n",
    "# so the 'true' completeness magnitude is mc - delta_m / 2\n",
    "\n",
    "\n",
    "def round_half_up(n, decimals=0):\n",
    "    # this is because numpy does weird rounding.\n",
    "    multiplier = 10 ** decimals\n",
    "    return np.floor(n * multiplier + 0.5) / multiplier\n",
    "\n",
    "\n",
    "def estimate_beta_tinti(magnitudes, mc, weights=None, axis=None, delta_m=0):\n",
    "    # Tinti, S., & Mulargia, F. (1987). Confidence intervals of b values for grouped magnitudes.\n",
    "    # Bulletin of the Seismological Society of America, 77(6), 2125-2134.\n",
    "\n",
    "    if delta_m > 0:\n",
    "        p = (1 + (delta_m / (np.average(magnitudes - mc, weights=weights, axis=axis))))\n",
    "        beta = 1 / delta_m * np.log(p)\n",
    "    else:\n",
    "        beta = 1 / np.average((magnitudes - (mc - delta_m / 2)), weights=weights, axis=axis)\n",
    "    return beta\n",
    "\n",
    "\n",
    "def simulate_magnitudes(n, beta, mc):\n",
    "    mags = np.random.uniform(size=n)\n",
    "    mags = (-1 * np.log(1 - mags) / beta) + mc\n",
    "    return mags\n",
    "\n",
    "\n",
    "def fitted_cdf_discrete(sample, mc, delta_m, x_max=None, beta=None):\n",
    "    if beta is None:\n",
    "        beta = estimate_beta_tinti(sample, mc=mc, delta_m=delta_m)\n",
    "\n",
    "    if x_max is None:\n",
    "        sample_bin_n = (sample.max() - mc) / delta_m\n",
    "    else:\n",
    "        sample_bin_n = (x_max - mc) / delta_m\n",
    "    bins = np.arange(sample_bin_n + 1)\n",
    "    cdf = 1 - np.exp(-beta * delta_m * (bins + 1))\n",
    "    x, y = mc + bins * delta_m, cdf\n",
    "\n",
    "    x, y_count = np.unique(x, return_counts=True)\n",
    "    return x, y[np.cumsum(y_count) - 1]\n",
    "\n",
    "\n",
    "def empirical_cdf(sample, weights=None):\n",
    "    try:\n",
    "        sample = sample.values\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        weights = weights.values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sample_idxs_sorted = np.argsort(sample)\n",
    "    sample_sorted = sample[sample_idxs_sorted]\n",
    "    if weights is not None:\n",
    "        weights_sorted = weights[sample_idxs_sorted]\n",
    "        x, y = sample_sorted, np.cumsum(weights_sorted) / weights_sorted.sum()\n",
    "    else:\n",
    "        x, y = sample_sorted, np.arange(1, len(sample) + 1) / len(sample)\n",
    "\n",
    "    # only return one value per bin\n",
    "    x, y_count = np.unique(x, return_counts=True)\n",
    "    return x, y[np.cumsum(y_count) - 1]\n",
    "\n",
    "\n",
    "def ks_test_gr(sample, mc, delta_m, ks_ds=None, n_samples=10000, beta=None):\n",
    "    sample = sample[sample >= mc - delta_m / 2]\n",
    "    if len(sample) == 0:\n",
    "        print(\"no sample\")\n",
    "        return 1, 0, []\n",
    "    if len(np.unique(sample)) == 1:\n",
    "        print(\"sample contains only one value\")\n",
    "        return 1, 0, []\n",
    "    if beta is None:\n",
    "        beta = estimate_beta_tinti(sample, mc=mc, delta_m=delta_m)\n",
    "\n",
    "    if ks_ds is None:\n",
    "        ks_ds = []\n",
    "\n",
    "        n_sample = len(sample)\n",
    "        simulated_all = round_half_up(\n",
    "            simulate_magnitudes(mc=mc - delta_m / 2, beta=beta, n=n_samples * n_sample) / delta_m\n",
    "        ) * delta_m\n",
    "\n",
    "        x_max = np.max(simulated_all)\n",
    "        x_fit, y_fit = fitted_cdf_discrete(sample, mc=mc, delta_m=delta_m, x_max=x_max, beta=beta)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            simulated = simulated_all[n_sample * i:n_sample * (i + 1)].copy()\n",
    "            x_emp, y_emp = empirical_cdf(simulated)\n",
    "            y_fit_int = np.interp(x_emp, x_fit, y_fit)\n",
    "\n",
    "            ks_d = np.max(np.abs(y_emp - y_fit_int))\n",
    "            ks_ds.append(ks_d)\n",
    "    else:\n",
    "        x_fit, y_fit = fitted_cdf_discrete(sample, mc=mc, delta_m=delta_m, beta=beta)\n",
    "\n",
    "    x_emp, y_emp = empirical_cdf(sample)\n",
    "    y_emp_int = np.interp(x_fit, x_emp, y_emp)\n",
    "\n",
    "    orig_ks_d = np.max(np.abs(y_fit - y_emp_int))\n",
    "\n",
    "    return orig_ks_d, sum(ks_ds >= orig_ks_d) / len(ks_ds), ks_ds\n",
    "\n",
    "\n",
    "def estimate_mc(sample, mcs_test, delta_m, p_pass, stop_when_passed=True, verbose=False, beta=None,\n",
    "                n_samples=10000):\n",
    "    \"\"\"\n",
    "    sample: np array of magnitudes to test\n",
    "    mcs_test: completeness magnitudes to test\n",
    "    delta_m: magnitude bins (sample has to be rounded to bins beforehand)\n",
    "    p_pass: p-value with which the test is passed\n",
    "    stop_when_passed: stop calculations when first mc passes the test\n",
    "    verbose: verbose\n",
    "    beta: if beta is 'known', only estimate mc\n",
    "    n_samples: number of magnitude samples to be generated in p-value calculation of KS distance\n",
    "    \"\"\"\n",
    "\n",
    "    ks_ds = []\n",
    "    ps = []\n",
    "    i = 0\n",
    "    for mc in mcs_test:\n",
    "        if verbose:\n",
    "            print('\\ntesting mc', mc)\n",
    "        ks_d, p, _ = ks_test_gr(sample, mc=mc, delta_m=delta_m, n_samples=n_samples, beta=beta)\n",
    "\n",
    "        ks_ds.append(ks_d)\n",
    "        ps.append(p)\n",
    "\n",
    "        i += 1\n",
    "        if verbose:\n",
    "            print('..p-value: ', p)\n",
    "\n",
    "        if p >= p_pass and stop_when_passed:\n",
    "            break\n",
    "    ps = np.array(ps)\n",
    "    if np.any(ps >= p_pass):\n",
    "        best_mc = mcs_test[np.argmax(ps >= p_pass)]\n",
    "        if beta is None:\n",
    "            beta = estimate_beta_tinti(sample[sample >= best_mc - delta_m / 2], mc=best_mc, delta_m=delta_m)\n",
    "        if verbose:\n",
    "            print(\"\\n\\nFirst mc to pass the test:\", best_mc, \"\\nwith a b-value of:\", beta/np.log(10))\n",
    "        return mcs_test, ks_ds, ps, best_mc, beta/np.log(10)\n",
    "    else:\n",
    "        best_mc = None\n",
    "        beta = None\n",
    "        if verbose:\n",
    "            print(\"None of the mcs passed the test.\")\n",
    "        mcs_test = None\n",
    "        return mcs_test, ks_ds, ps, best_mc, beta\n",
    "\n",
    "    # beta is the Tinti beta - so b value is beta/ln10\n",
    "    # return the b-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GR analysis for SAM\n",
    "for zone in ['Arequipa','Bucaramanga','Colombia','Jara_target','North_Chile','Valdivia',\n",
    " 'Atacama','Central_Chile','Ecuador','Maule','Peru']: \n",
    "    cat = pd.read_csv('SAM_EQ_data/zonal_cat/{}/raw_cat_in_region.csv'.format(zone), index_col=0)\n",
    "    magnitude_sample = cat['mag'].values\n",
    "    mcs = round_half_up(np.arange(2.0, 5.5, 0.1), 1)\n",
    "    print('NOW PROCESSING: {}'.format(zone))\n",
    "    mcs_tested, ks_distances, p_values, mc_winner, b_value_winner = estimate_mc(magnitude_sample,mcs,delta_m=0.1,\n",
    "                                        p_pass=0.05,stop_when_passed=False,verbose=True,n_samples=1000)\n",
    "    print('Mc for {} is {}'.format(zone, mc_winner))\n",
    "    G_R_plotting(cat, 'SAM', zone, 0.1, mc_winner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    " - Jara, J., Socquet, A., Marsan, D., Bouchon, M., 2017. Long-Term Interactions Between Intermediate Depth and Shallow Seismicity in North Chile Subduction Zone. *Geophysical Research Letters 44*, 9283–9292. https://doi.org/10.1002/2017GL075029 <br>\n",
    " - Marsan, D., Bouchon, M., Gardonio, B., Perfettini, H., Socquet, A., Enescu, B., 2017. Change in seismicity along the Japan trench, 1990–2011, and its relationship with seismic coupling. *Journal of Geophysical Research: Solid Earth 122*, 4645–4659. https://doi.org/10.1002/2016JB013715 <br>\n",
    " - Marsan, D., Reverso, T., Helmstetter, A., Enescu, B., 2013. Slow slip and aseismic deformation episodes associated with the subducting Pacific plate offshore Japan, revealed by changes in seismicity. *Journal of Geophysical Research: Solid Earth 118*, 4900–4909. https://doi.org/10.1002/jgrb.50323 <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py-test",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
