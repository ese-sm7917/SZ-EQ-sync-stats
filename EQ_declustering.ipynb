{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARCGEN': 'r',\n",
       " 'DXF': 'rw',\n",
       " 'CSV': 'raw',\n",
       " 'OpenFileGDB': 'r',\n",
       " 'ESRIJSON': 'r',\n",
       " 'ESRI Shapefile': 'raw',\n",
       " 'FlatGeobuf': 'rw',\n",
       " 'GeoJSON': 'raw',\n",
       " 'GeoJSONSeq': 'rw',\n",
       " 'GPKG': 'raw',\n",
       " 'GML': 'rw',\n",
       " 'OGR_GMT': 'rw',\n",
       " 'GPX': 'rw',\n",
       " 'GPSTrackMaker': 'rw',\n",
       " 'Idrisi': 'r',\n",
       " 'MapInfo File': 'raw',\n",
       " 'DGN': 'raw',\n",
       " 'PCIDSK': 'rw',\n",
       " 'OGR_PDS': 'r',\n",
       " 'S57': 'r',\n",
       " 'SQLite': 'raw',\n",
       " 'TopoJSON': 'r',\n",
       " 'KML': 'rw'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Package imports ##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from sklearn import mixture\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime as dt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma as gamma_func, gammaln, gammaincc, exp1\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import fiona\n",
    "\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "import shapely.ops as ops\n",
    "\n",
    "\n",
    "#Check shapely speedups are enabled\n",
    "from shapely import speedups\n",
    "speedups.enabled\n",
    "\n",
    "#Set geopandas settings\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Ingest data from the International Seismological Centre (ISC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ISC Catalog stepwise search ###\n",
    "# ObsPy plugin breaks when searching for too many events, so we perform search in steps of 1 year (when using FDSN)\n",
    "\n",
    "def search_ISC_cat_step(search_params_dict):\n",
    "    \"\"\"\n",
    "    Perform ISC catalog search stepwise (in 1 year long steps)\n",
    "    search_params_dict - dict containing the following keywords:\n",
    "    start_time (UTCDateTime or dt.datetime), end_time (UTCDateTime or dt.datetime), min_latitude, max_latitude, \n",
    "    min_longitude, max_longitude, min_mag, max_mag, min_depth, max_depth, \n",
    "    t_step (for segmeting search, type relativedelta)\n",
    "    \"\"\"\n",
    "    calc_start = dt.datetime.now() # time the function\n",
    "    start_time = search_params_dict[\"start_time\"]\n",
    "    end_time = search_params_dict[\"end_time\"]\n",
    "    min_latitude = search_params_dict[\"min_latitude\"]\n",
    "    max_latitude = search_params_dict[\"max_latitude\"]\n",
    "    min_longitude = search_params_dict[\"min_longitude\"]\n",
    "    max_longitude =  search_params_dict[\"max_longitude\"]\n",
    "    min_mag = search_params_dict[\"min_mag\"]\n",
    "    max_mag = search_params_dict[\"max_mag\"]\n",
    "    min_depth = search_params_dict[\"min_depth\"]\n",
    "    max_depth = search_params_dict[\"max_depth\"]\n",
    "    t_step = search_params_dict[\"t_step\"]\n",
    "    \n",
    "    # Start search\n",
    "    t1 = start_time\n",
    "    t2 = t1 + t_step\n",
    "    #print('Processing:', t1, t2)\n",
    "    client = Client(\"ISC\")\n",
    "\n",
    "    # Initialise catalog\n",
    "    cat_init = client.get_events(starttime=t1,endtime=t2,\n",
    "                                  minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                                  minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                                  minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                                  mindepth=min_depth, maxdepth=max_depth, orderby=\"time-asc\",\n",
    "                                  includeallorigins=True, includeallmagnitudes=True)\n",
    "\n",
    "    # Set up loop for stepwise search\n",
    "    t1=t2\n",
    "    t2+=t_step\n",
    "    print('Beginning loop', t1, t2)\n",
    "    cat = cat_init\n",
    "    while t2 < end_time:\n",
    "        try:\n",
    "            #print('Loop Processing', t1, t2)\n",
    "            catalogue = client.get_events(starttime=t1,endtime=t2,\n",
    "                                  minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                                  minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                                  minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                                  mindepth=min_depth, maxdepth=max_depth, orderby=\"time-asc\",\n",
    "                                  includeallorigins=True, includeallmagnitudes=True)\n",
    "            cat=cat.__add__(catalogue)\n",
    "            t1=t2\n",
    "            t2+=t_step\n",
    "        except:\n",
    "            import sys\n",
    "            print(\"Oops!\", sys.exc_info()[0], \"occurred for \", t1, \" - \", t2)\n",
    "            print('FDSN Web Search failure - finalising catalog...')\n",
    "            final_cat = cat\n",
    "            break\n",
    "\n",
    "    # Add final time step and add to main catalog    \n",
    "    assert t1 < end_time    \n",
    "    try:\n",
    "        cat1 = client.get_events(starttime=t1,endtime=end_time,\n",
    "                                  minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                                  minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                                  minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                                  mindepth=min_depth, maxdepth=max_depth, orderby=\"time-asc\",\n",
    "                                  includeallorigins=True, includeallmagnitudes=True)\n",
    "        final_cat = cat.__add__(cat1)\n",
    "        print('Final cat', final_cat)\n",
    "    except:\n",
    "        import sys\n",
    "        print(\"Reminder:\", sys.exc_info()[0], \"occurred.\")\n",
    "        print('FDSN Web Search failure - catalog now finalised.')\n",
    "    \n",
    "    print('    took', (dt.datetime.now() - calc_start), 'to download the EQ catalog\\n')\n",
    "    return final_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ISC Catalog stepwise search ###\n",
    "\n",
    "def search_ISC_cat(search_params_dict):\n",
    "    \"\"\"\n",
    "    Perform ISC catalog search stepwise (in 1 year long steps)\n",
    "    search_params_dict - dict containing the following keywords:\n",
    "    start_time (UTCDateTime or dt.datetime), end_time (UTCDateTime or dt.datetime), min_latitude, max_latitude, \n",
    "    min_longitude, max_longitude, min_mag, max_mag, min_depth, max_depth, \n",
    "    t_step (for segmeting search, type relativedelta)\n",
    "    \"\"\"\n",
    "    calc_start = dt.datetime.now() # time the function\n",
    "    \n",
    "    start_time = search_params_dict[\"start_time\"]\n",
    "    end_time = search_params_dict[\"end_time\"]\n",
    "    min_latitude = search_params_dict[\"min_latitude\"]\n",
    "    max_latitude = search_params_dict[\"max_latitude\"]\n",
    "    min_longitude = search_params_dict[\"min_longitude\"]\n",
    "    max_longitude =  search_params_dict[\"max_longitude\"]\n",
    "    min_mag = search_params_dict[\"min_mag\"]\n",
    "    max_mag = search_params_dict[\"max_mag\"]\n",
    "    min_depth = search_params_dict[\"min_depth\"]\n",
    "    max_depth = search_params_dict[\"max_depth\"]\n",
    "    \n",
    "    # Start search\n",
    "    client = Client(\"ISC\")\n",
    "\n",
    "    # Initialise catalog\n",
    "    cat = client.get_events(starttime=start_time,endtime=end_time,\n",
    "                                  minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                                  minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                                  minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                                  mindepth=min_depth, maxdepth=max_depth, orderby=\"time-asc\",\n",
    "                           includeallorigins=True, includeallmagnitudes=True)\n",
    "    \n",
    "    print('    took', (dt.datetime.now() - calc_start), 'to download the EQ catalog\\n')\n",
    "    print(cat)\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE CATALOG DATAFRAME ##\n",
    "def create_cat_df(cat, outfile_path):\n",
    "    \"\"\"\n",
    "    Create pd dataframe of events from ObsPy catalog object\n",
    "    final_cat: ObsPy catalog object\n",
    "    outfile_path: str - filepath for output CSV file with catalog data, e.g. Japan_EQ_data\n",
    "    \"\"\"\n",
    "    # Set destination path\n",
    "    outfile = outfile_path + '/raw_catalog_data.csv'\n",
    "    \n",
    "    # Create empty lists\n",
    "    year = []\n",
    "    month = []\n",
    "    day = []\n",
    "    hour = []\n",
    "    minute = []\n",
    "    second = []\n",
    "    lat = []\n",
    "    lon = []\n",
    "    dep = []\n",
    "    mag = []\n",
    "    mauth = []\n",
    "    time = []\n",
    "\n",
    "    # Loop over each event in the catalogue\n",
    "    empty = []\n",
    "    for event in cat:\n",
    "        year.append(event.origins[0].time.year)\n",
    "        month.append(event.origins[0].time.month)\n",
    "        day.append(event.origins[0].time.day)\n",
    "        hour.append(event.origins[0].time.hour)\n",
    "        minute.append(event.origins[0].time.minute)\n",
    "        second.append(event.origins[0].time.second)\n",
    "        lat.append(event.origins[0].latitude)\n",
    "        lon.append(event.origins[0].longitude)\n",
    "        dep.append(event.origins[0].depth)\n",
    "        ## Magnitudes\n",
    "        # The following selection is performed for each event:\n",
    "        # If an event has a GCMT Mw, we pick that. If an event has Mw, but not GCMT, we use the average of all MW/Mw.\n",
    "        # If an event has no Mw, we use the average of the ISC and NEIC magnitudes, or in the absence of both, we use\n",
    "        # either ISC or NEIC. If an event has no ISC or NEIC magnitude, we use mags from local agencies, ignoring mags\n",
    "        # by MOS, the IDC and BJI\n",
    "        ISC_mags = [] #temporarily holds all event magnitudes for selection\n",
    "        NEIC_mags = []\n",
    "        mags_mw = []\n",
    "        reg_mags = []\n",
    "        m_gcmt = 0.0\n",
    "        M = 0.0\n",
    "        for mags in event.magnitudes: \n",
    "            try:\n",
    "                if mags.creation_info.author == 'GCMT':\n",
    "                    m_gcmt = mags.mag\n",
    "                    break\n",
    "                elif mags.creation_info.author != 'GCMT' and (('W' in mags.magnitude_type) and ('w' in mags.magnitude_type)):\n",
    "                    ev_mags_mw.append(mags.mag)\n",
    "                elif mags.creation_info.author == 'ISC':\n",
    "                    ISC_mags.append(mags.mag)\n",
    "                elif mags.creation_info.author == 'NEIC':\n",
    "                    NEIC_mags.append(mags.mag)\n",
    "                elif (mags.creation_info.author != 'IDC') and (mags.creation_info.author != 'MOS') and (mags.creation_info.author != 'BJI'):\n",
    "                    reg_mags.append(mags.mag)\n",
    "            except ValueError:\n",
    "                M = np.nan\n",
    "\n",
    "        if m_gcmt != 0.0:\n",
    "            mag.append(m_gcmt)\n",
    "            #mauth.append('GCMT')\n",
    "        elif (mags_mw != empty and m_gcmt == 0.0):\n",
    "            #print(ev_mags_mw)\n",
    "            mag.append(np.mean(np.array(mags_mw)))\n",
    "            #mauth.append('Mw/Any')\n",
    "        elif (NEIC_mags != empty) and (ISC_mags != empty) and (mags_mw == empty) and (m_gcmt == 0.0):\n",
    "            #print('NEIC+ISC Chosen: ', np.mean(np.array([max(ISC_mags), max(NEIC_mags)])))\n",
    "            mag.append(np.mean(np.array([max(ISC_mags), max(NEIC_mags)])))\n",
    "            #mauth.append('NEIC/ISC')\n",
    "        elif (NEIC_mags == empty) and (ISC_mags != empty):\n",
    "            #print('ISC Chosen: ', np.mean(np.array(ISC_mags)))\n",
    "            mag.append(np.mean(np.array(ISC_mags)))\n",
    "            #mauth.append('ISC')\n",
    "        elif (NEIC_mags != empty) and (ISC_mags == empty):\n",
    "            #print('NEIC Chosen: ', np.mean(np.array(NEIC_mags)))\n",
    "            mag.append(np.mean(np.array(ISC_mags)))\n",
    "            #mauth.append('NEIC')\n",
    "        elif (NEIC_mags == empty) and (ISC_mags == empty) and (reg_mags != empty):\n",
    "            #print('Regional mags: ', reg_mags)\n",
    "            mag.append(np.mean(np.array(reg_mags)))\n",
    "            #mauth.append('Reg')\n",
    "        else:\n",
    "            mag.append(np.nan)\n",
    "            #mauth.append(np.nan)\n",
    "            \n",
    "\n",
    "    # Create the dataframe\n",
    "    #print(len(year), len(month), len(day), len(hour), len(minute), len(second), len(lat), len(lon), len(dep), len(mag)) \n",
    "    data = pd.DataFrame(np.array([year, month, day, hour, minute, second, lat, lon, dep, mag]).T, \n",
    "                 columns=[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\",\n",
    "                          \"lat\", \"lon\", \"depth_km\", \"mag\"])\n",
    "    \n",
    "    # Apply datetimes\n",
    "    data = data.dropna() # remove events without a magnitude\n",
    "    data.drop_duplicates(subset=[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\",\n",
    "                          \"lat\", \"lon\", \"depth_km\"], keep='first', inplace=True, ignore_index=True)\n",
    "    data[\"time\"] = pd.to_datetime(data[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "    data = data.set_index('time')\n",
    "    data = data.sort_index()\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    print('Catalog contains {} events'.format(len(data.index)))\n",
    "    # Save raw data to csv\n",
    "    #data.to_csv(outfile)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### South American margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# South America - using a min mag of 4.5\n",
    "SAM_search_params1 = {\"start_time\": dt.datetime(1970,1,1), \"end_time\": dt.datetime(2021, 7, 1), \n",
    "                         \"min_latitude\": -47, \"max_latitude\": 8, \"min_longitude\": -83, \"max_longitude\": -60, \n",
    "                         \"min_mag\": 4.5, \"max_mag\": None, \"min_depth\": None, \"max_depth\": None, \n",
    "                         \"t_step\": relativedelta(years=1, months=0, days=0)}\n",
    "\n",
    "SAM_search_result = search_ISC_cat(SAM_search_params1)\n",
    "print(SAM_search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cat_df(SAM_search_result, 'SAM_EQ_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japan earthquake catalogs\n",
    "We need a stepwise search here (`search_ISC_cat_step`) as there too many events in the catalog for a single search, due to the lower magnitude cutoff used. Note that `search_ISC_cat_step` is a lot slower than `search_ISC_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japan South catalog\n",
    "Japan_S_search_params1 = {\"start_time\": dt.datetime(1970,1,1), \"end_time\": dt.datetime(2021, 7, 1), \n",
    "                         \"min_latitude\": 30, \"max_latitude\": 36, \"min_longitude\": 128, \"max_longitude\": 145, \n",
    "                         \"min_mag\": 3.0, \"max_mag\": None, \"min_depth\": None, \"max_depth\": None, \n",
    "                         \"t_step\": relativedelta(years=1, months=0, days=0)}\n",
    "\n",
    "Japan_S_search_result = search_ISC_cat_step(Japan_S_search_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japan central\n",
    "Japan_C_search_params1 = {\"start_time\": dt.datetime(1970,1,1), \"end_time\": dt.datetime(2021, 7, 1), \n",
    "                         \"min_latitude\": 36, \"max_latitude\": 38, \"min_longitude\": 135, \"max_longitude\": 147, \n",
    "                         \"min_mag\": 3.0, \"max_mag\": None, \"min_depth\": None, \"max_depth\": None,\n",
    "                        \"t_step\": relativedelta(years=1, months=0, days=0)}\n",
    "\n",
    "Japan_C_search_result = search_ISC_cat_step(Japan_C_search_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japan North\n",
    "Japan_N_search_params1 = {\"start_time\": dt.datetime(1970,1,1), \"end_time\": dt.datetime(2021, 7, 1), \n",
    "                         \"min_latitude\": 38, \"max_latitude\": 45, \"min_longitude\": 136, \"max_longitude\": 152, \n",
    "                         \"min_mag\": 3.0, \"max_mag\": None, \"min_depth\": None, \"max_depth\": None,\n",
    "                        \"t_step\": relativedelta(years=1, months=0, days=0)}\n",
    "\n",
    "Japan_N_search_result = search_ISC_cat_step(Japan_N_search_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Japan_search_result = Japan_S_search_result.__add__(Japan_C_search_result)\n",
    "Japan_search_result = Japan_search_result.__add__(Japan_N_search_result)\n",
    "print(Japan_search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cat_df(Japan_search_result, 'Japan_EQ_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aleutians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epidemic Type Aftershock Sequence (ETAS) Declustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAS Implementation by Marsan et al. (2017)\n",
    "*Please note that this section is still not fully operational - implementation details in Marsan et al. (2017)/Jara et al. (2017) are lacking*\n",
    "\n",
    "Omori Law parameters are fixed as follows:\n",
    "$$ \\alpha = 2, p = 1, c = 10^{-3} days, \\gamma = 2 $$ <br>\n",
    "\n",
    "Algorithm can be described as follows: <br>\n",
    "1. Compute triggering rate from catalog as $\\nu(x_i,y_i,t_i)/K $ <br>\n",
    "2. Compute background rate ($ \\mu(x_i, y_i, t_i) $), under the assumption of $\\omega_i = 0.5$\n",
    "3. Compute initial total rate as $ \\lambda(x_i,y_i,t_i) = \\mu(x_i, y_i, t_i) + \\nu(x_i, y_i, t_i) $\n",
    "4. Compute $\\omega_i = \\frac{\\mu(x_i, y_i, t_i)}{\\lambda(x_i,y_i,t_i)}$\n",
    "5. Use $\\omega_i $ to compute ML estimate of K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations\n",
    "$$ \\lambda(x,y,t) = \\mu(x,y,t) + \\nu(x,y,t) $$ <br>\n",
    "where $\\lambda(x,y,t)$ is the total seismicity rate, $\\mu(x,y,t)$ is the background seismicity rate, and $\\nu(x,y,t)$ is the triggering rate <br>\n",
    "\n",
    "The triggering rate: <br>\n",
    "$$ \\nu(x,y,t) = \\displaystyle\\sum_{i/t_i < t}^{} \\frac{Ke^{\\alpha m_i}}{(t+c-t_i)} \\times \\frac{\\gamma - 1}{2\\pi} \\times \\frac{L_i^{\\gamma-1}}{\\left((x-x_i)^2 + (y-y_i)^2 + L_i^2 \\right )^{\\frac{\\gamma + 1}{2}}} $$ <br>\n",
    "\n",
    "The background rate: <br>\n",
    "$$ \\mu(x,y,t) = \\displaystyle\\sum_{i}^{} \\omega_i e^{-\\sqrt{(x-x_i)^2 + (y-y_i)^2}/\\ell} e^{-|t-t_i|/\\tau} \\times \\frac{1}{2 \\pi \\ell^2 a_i} $$ <br>\n",
    "\n",
    "$$ a_i = 2\\tau - \\tau \\left( e^{-\\frac{t_s - t_i}{\\tau}} - e^{\\frac{t_s - t_i}{\\tau}} \\right) $$ <br>\n",
    "where $t_s, t_e$ are the start and end times of the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the catalog for processing\n",
    "def prep_cat_ETAS(cat_init, cat_start, cat_end): \n",
    "    \"\"\" Loads and prepares the catalog for further processing\n",
    "    # input cat_init needs to be a file path to a CSV document containing labelled columns:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    \"\"\"\n",
    "    # Load catalog from file:\n",
    "    cat = pd.read_csv(cat_init, index_col=0)\n",
    "    \n",
    "    # Apply datetimes\n",
    "    cat[\"time\"] = pd.to_datetime(cat[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    cat = cat.infer_objects()\n",
    "    cat.dtypes\n",
    "    cat.loc[:, 'depth_km'] *=0.001\n",
    "\n",
    "    # Define a geodataframe using the EQ catalog from above\n",
    "    cat_gdf = gpd.GeoDataFrame(cat, geometry=gpd.points_from_xy(cat.lon, cat.lat))\n",
    "    cat_gdf = cat_gdf.set_crs(\"EPSG:4326\")\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    cat_gdf['lat_rad'] = np.radians(cat_gdf['lat'])\n",
    "    cat_gdf['lon_rad'] = np.radians(cat_gdf['lon'])\n",
    "    # Compute the time difference between event occurrence times and the start and end times of the catalog, in days\n",
    "    cat_gdf['t_diff_e'] = (1./(24.*60.*60.))*((cat_gdf['time']- cat_end).dt.total_seconds())\n",
    "    cat_gdf['t_diff_s'] = (1./(24.*60.*60.))*((cat_gdf['time'] - cat_start).dt.total_seconds())\n",
    "    return cat_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    \"\"\"Haversine function\n",
    "    Takes in arguments in radians\n",
    "    \"\"\"\n",
    "    return np.square(np.sin(theta / 2))\n",
    "\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    #Haversine distance in km - calculate distance between 2 pts on a sphere\n",
    "    # lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2 must all be in radians\n",
    "    ####################################################################\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(\n",
    "        np.sqrt(\n",
    "            hav(lat_rad_1 - lat_rad_2)\n",
    "            + np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic length/rupture radius in km\n",
    "def L_i(m, L_0):\n",
    "    return L_0*np.power(10, 0.5*(m-2))\n",
    "\n",
    "# a coefficients\n",
    "# Need to fix abs values in exponents\n",
    "def a_coeff(t_diff_e,t_diff_s,tau): # tau is the temporal smoothing param; t_diff_e=t_catend-tevent; t_diff_s=t_catstart-tevent\n",
    "    a = 2*tau - tau*(np.exp(-(np.abs(t_diff_s)/(tau))) - np.exp(-(np.abs(t_diff_e)/(tau)))) # times in days\n",
    "    return a\n",
    "\n",
    "# Calculate triggering rate\n",
    "def nu_calc(c, alpha, p, gamma, K, m, time_diffs, r_sq, L_0):\n",
    "    # Numerical calculations for nu\n",
    "    T1 = K*np.exp(alpha*m)/np.power((time_diffs), p)\n",
    "    #T1 = np.exp(alpha*m)\n",
    "    T2 = (gamma-1)/2*np.pi\n",
    "    T3 = np.power(L_i(m, L_0), (gamma-1))\n",
    "    T4 = np.power((r_sq + np.power(L_i(m, L_0),2)), (gamma+1)/2)\n",
    "    return T1*T2*(T3/T4)\n",
    "    \n",
    "# Calculate the background rate:\n",
    "def mu_calc(r_sq, t_diff, omega, tau, l, a_coeffs): # tau, l are the temporal and spatial smoothing params\n",
    "    T1 = omega*np.exp(-np.sqrt(r_sq)/l)\n",
    "    T2 = np.exp(-np.abs(t_diff)/tau) # times need to be in days\n",
    "    T3 = 1/(2*np.pi*(l**2)*a_coeffs)\n",
    "    lam = T1*T2*T3\n",
    "    return lam\n",
    "\n",
    "# Triggering rate from catalog\n",
    "def nuK(catalog, c, alpha, p, gamma, K, L_0):\n",
    "    \"\"\" Calculates nu(xi,yi,ti) - triggering rate at time and place of each event in catalog\n",
    "    # L_0 is the reference length\n",
    "    # If K=1, then function actually returns nu(xi,yi,ti)/K\n",
    "    \"\"\"\n",
    "    func_start = dt.datetime.now() # time the function\n",
    "    cat = catalog.copy(deep=True)\n",
    "    cat['nuK'] = 0.0\n",
    "    #print('Looping through catalog...')\n",
    "    for triggered in cat.itertuples():\n",
    "        # get values of source event\n",
    "        ttime = triggered.time\n",
    "        #print('Triggered event time:', ttime)\n",
    "        tlatrad = triggered.lat_rad\n",
    "        tlonrad = triggered.lon_rad\n",
    "        potential_triggers = cat.loc[cat[\"time\"] < ttime]\n",
    "        potential_triggers['c'] = c\n",
    "        potential_triggers['t_diffs'] = (1./(24.*60.*60.))*(ttime - potential_triggers['time']).dt.total_seconds()\n",
    "        potential_triggers['t_denom'] = potential_triggers['t_diffs'] + potential_triggers['c']\n",
    "        #print(potential_triggers['t_denom'])\n",
    "        #print(potential_triggers)\n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        potential_triggers['r_squared'] = np.square(haversine(tlatrad,potential_triggers['lat_rad'],tlonrad,potential_triggers['lon_rad']))\n",
    "        #print(len(potential_triggers['mag']), len(potential_triggers['r_squared']), len(potential_triggers['t_denom']))\n",
    "        # Calculate triggering rate nu for each event i.e. nu(xi,yi,ti)\n",
    "        nuK_array = nu_calc(c, alpha, p, gamma, K, potential_triggers['mag'], potential_triggers['t_denom'], potential_triggers['r_squared'], L_0)\n",
    "        cat.loc[triggered.Index, 'nuK'] = nuK_array.sum()\n",
    "    print('    took', (dt.datetime.now() - func_start), 'to compute nu(xi,yi,ti)/K \\n')\n",
    "    return cat\n",
    "\n",
    "# Background rate from catalog\n",
    "def mu(catalog, tau, l):\n",
    "    cat = catalog.copy(deep=True)\n",
    "    cat['mu_i'] = 0.0\n",
    "    #cat['omega_i'] = 0.5\n",
    "    for event in cat.itertuples():\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        evtime = event.time\n",
    "        #print(evtime)\n",
    "        #print('Triggered event time:', ttime)\n",
    "        evlatrad = event.lat_rad\n",
    "        evlonrad = event.lon_rad\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((evtime - temp_cat['time']).dt.total_seconds())\n",
    "        \n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(evlatrad,temp_cat['lat_rad'],evlonrad,temp_cat['lon_rad']))\n",
    "        temp_cat['a_coeffs'] = a_coeff(temp_cat['t_diff_e'], temp_cat['t_diff_s'], tau)\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_i'],tau,l,temp_cat['a_coeffs'])\n",
    "        #print(temp_cat)\n",
    "        cat.loc[event.Index, 'mu_i'] = temp_cat['mu_indiv'].sum()\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameter F_i for individual events\n",
    "def F_i(alpha, t_diff, c, m):\n",
    "    return np.exp(alpha*m)*(np.log(t_diff) - np.log(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declustering function ##\n",
    "def decluster(path, cat_start, cat_end, tau, l, c, alpha, p, gamma, L_0, atol):\n",
    "    \"\"\"\n",
    "    # Function to estimate normalisation parameter K and best estimates for omega\n",
    "    # path must be a filepath (str) to a CSV file containing an output ISC search result with cols:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    # tau, l are temporal and spatial smoothing params, to be given in days and km\n",
    "    # c, alpha, gamma are Omori-Utsu Law and power spectral density constants\n",
    "    # L_0 is the reference rupture length\n",
    "    # atol is the tolerance level for convergence in the MLE estimate of K\n",
    "    \"\"\"\n",
    "    calc_start = dt.datetime.now() # time the function\n",
    "    assert cat_start < cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    \n",
    "    print('\\nCatalog start time: ', cat_start, ' Catalog end time:', cat_end, \n",
    "          '\\nNumber of events in catalog: ', len(cat.index),\n",
    "          '\\nSmoothing time:', tau, 'days '\n",
    "          'Smoothing length: ', l, 'km ', '\\nOmori Law constants: \\nc:',  c, 'days', ' alpha: ', alpha, ' p: ', \n",
    "          p, ' gamma:', gamma, '\\nReference rupture length:', L_0, 'km')\n",
    "    \n",
    "    # Load catalog from file and prepare for processing\n",
    "    print('Preparing catalog for processing...')\n",
    "    cat_preprocessed = prep_cat_ETAS(path, cat_start, cat_end)\n",
    "    \n",
    "    print('Now processing catalog...')\n",
    "    # Now calculate nu(xi,yi,ti)/K for all events:\n",
    "    print('Calculating initial triggering rate...')\n",
    "    nuK_cat = nuK(cat_preprocessed, c, alpha, p, gamma, 1.0, L_0)\n",
    "    \n",
    "    nuK_cat['omega_i'] = 0.5\n",
    "    \n",
    "    # Calculate background rates at time and place of each event, assuming omega is 0.5\n",
    "    print('Estimating a priori background rates...')\n",
    "    initial_mu_cat = mu(nuK_cat, tau, l)\n",
    "    \n",
    "    # initial_mu_cat should now contain both a nu and a mu for each event\n",
    "    # Now compute updated omega:\n",
    "    initial_mu_cat['lambda_i'] = initial_mu_cat['mu_i'] + initial_mu_cat['nuK']\n",
    "    initial_mu_cat['omega_i'] = initial_mu_cat['mu_i'] / initial_mu_cat['nuK']\n",
    "    \n",
    "    # Initialise some columns for the MLE estimate\n",
    "    initial_mu_cat['c'] = c # in days\n",
    "    initial_mu_cat['t_diffs'] = (1./(24.*60.*60.))*(cat_end - initial_mu_cat['time']).dt.total_seconds() # in days\n",
    "    initial_mu_cat['t_quantity'] = initial_mu_cat['t_diffs'] + initial_mu_cat['c']\n",
    "    initial_mu_cat['K_num'] = 1 - (initial_mu_cat['omega_i'])\n",
    "    initial_mu_cat['F_i'] = F_i(alpha, initial_mu_cat['t_quantity'], c, initial_mu_cat['mag'])\n",
    "    \n",
    "    # Initialise K - an initial guess\n",
    "    K = initial_mu_cat['K_num'].sum() / initial_mu_cat['F_i'].sum()\n",
    "    fevals = 0 # record number of function evaluations so we can later compare methods\n",
    "    K_prev = K + 2*atol # initialise the previous K simply so that while loop argument is initially true\n",
    "    updated_cat = initial_mu_cat.copy(deep=True)\n",
    "    print('Starting iteration for MLE estimate of K')\n",
    "    while abs(K - K_prev) > atol:\n",
    "        K_prev = K\n",
    "        \n",
    "        # Compute updated nu based on new value of K\n",
    "        updated_cat = nuK(updated_cat, c, alpha, p, gamma, K, L_0)\n",
    "        \n",
    "        # Now compute updated omega:\n",
    "        updated_cat['lambda_i'] = updated_cat['mu_i'] + updated_cat['nuK']\n",
    "        updated_cat['omega_i'] = updated_cat['mu_i'] / updated_cat['nuK']\n",
    "        updated_cat = mu(updated_cat, tau, l)\n",
    "        updated_cat['K_num'] = 1 - (updated_cat['omega_i'])\n",
    "        \n",
    "        # Compute K using updated omega:\n",
    "        K =  updated_cat['K_num'].sum() / updated_cat['F_i'].sum()\n",
    "        fevals += 1\n",
    "        #print('Current iteration solution: ',K)\n",
    "    print('The final value of K is', K)\n",
    "    print('\\n', fevals, 'function evaluations were required for K convergence')\n",
    "    \n",
    "    # Using final K value calculate a final triggering rate nu(xi,yi,ti):\n",
    "    final_cat = nuK(updated_cat, c, alpha, p, gamma, K, L_0)\n",
    "    \n",
    "    # now the final catalog should contain the correct omegas, which can be used to estimate a background seismicity rate curve\n",
    "    # Check this visually using a histogram\n",
    "    plt.hist(final_cat[\"omega_i\"],log=True)\n",
    "    print('    took', (dt.datetime.now() - calc_start), 'for declustering \\n')\n",
    "    return final_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING ###\n",
    "cat_start = dt.datetime(1970,1,1)\n",
    "cat_end = dt.datetime(2021, 6, 21)\n",
    "cat = catalog_gdf\n",
    "tau=100; l=100; c=0.001; alpha=2; p=1; gamma=2; L_0=1.78; atol=0.01\n",
    "print('\\nCatalog start time: ', cat_start, ' Catalog end time:', cat_end, \n",
    "          '\\nNumber of events in catalog: ', len(cat.index),\n",
    "          '\\nSmoothing time:', tau, 'days '\n",
    "          'Smoothing length: ', l, 'km ', '\\nOmori Law constants: \\nc:',  c, 'days', ' alpha: ', alpha, ' p: ', \n",
    "          p, ' gamma:', gamma, '\\nReference rupture length:', L_0, 'km')\n",
    "\n",
    "# Preprocess\n",
    "cat_preprocessed = prep_catalog(cat, cat_start, cat_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final estimate of background seismicity rate ##\n",
    "def mu_final(x,y,cat_start, cat_end, cat, tau, l):\n",
    "    #########################################################\n",
    "    # Function computes timeseries of the background seismicity rate\n",
    "    # x,y refer to a spatial reference point - should be the centroid of the study area\n",
    "    # Function will build an array of datetime objects with a timestep of 1 day using the cat_start, cat_end times\n",
    "    # catalog should contain omega, a_coeff values for each event - the output of func decluster\n",
    "    #########################################################\n",
    "    assert t_cat_start < t_cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    \n",
    "    # Time steps for calculating background rate\n",
    "    times = np.arange(start_time, end_time, dt.timedelta(days=1)).astype(datetime)\n",
    "    mu_t_series = []\n",
    "    \n",
    "    # Compute background rate at each time step\n",
    "    for t_step in times:\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((t_step - temp_cat['time']).dt.total_seconds())\n",
    "        \n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(x, temp_cat['lat_rad'], y, temp_cat['lon_rad']))\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_i'],tau,l,temp_cat['a_coeffs'])\n",
    "        mu_t_series.append(temp_cat['mu_indiv'].sum())\n",
    "    return times, mu_t_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declustering implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISC web search params\n",
    "start_time = dt.datetime(1970,1,1)\n",
    "end_time = dt.datetime(2021, 6, 21)\n",
    "min_latitude = -25\n",
    "max_latitude = -11\n",
    "min_longitude = -80\n",
    "max_longitude = -66\n",
    "min_mag = 4.5\n",
    "max_mag = None\n",
    "min_depth = None\n",
    "max_depth = None\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECLUSTERING ###\n",
    "\n",
    "# Separate catalog into deep and shallow following Jara et al. (2017)\n",
    "catalog_shallow = catalog_gdf.loc[catalog_gdf['depth_km'] < 40.0]\n",
    "catalog_deep = catalog_gdf.loc[catalog_gdf['depth_km'] > 80.0]\n",
    "declustered_shallow_cat = decluster(catalog_shallow, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "declustered_deep_cat = decluster(catalog_deep, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "\n",
    "# And calculate rates:\n",
    "times, deep_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_deep_cat, tau=100, l=100)\n",
    "times, shallow_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_shallow_cat, tau=100, l=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAS implementation by Mizrahi et al. (2021)\n",
    "\n",
    "This ETAS implementation by Mizrahi assumes that the background seismicity rate does not vary in time/space. While this may well be valid when attempting to decluster small catalogs (e.g. $10^\\circ \\times 10^\\circ$ area), the assumption almost certainly breaks down when trying to decluster a much larger catalog (e.g. on the scale of an entire margin, like the whole of the South American margin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First find the best-fitting completeness magnitude and the best-fitting b-value\n",
    "## Using code by Mizrahi et al. (2021) as sourced from https://github.com/lmizrahi/etas/blob/main/mc_b_est.py\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# joint beta and completeness magnitude estimation\n",
    "# using p-value of Kolmogorov-Smirnov distance to fitted Gutenberg-Richter law\n",
    "#\n",
    "# as described by Mizrahi et al., 2021\n",
    "# Leila Mizrahi, Shyam Nandan, Stefan Wiemer;\n",
    "# The Effect of Declustering on the Size Distribution of Mainshocks.\n",
    "# Seismological Research Letters 2021; doi: https://doi.org/10.1785/0220200231\n",
    "# inspired by method of Clauset et al., 2009\n",
    "##############################################################################\n",
    "\n",
    "# mc is the binned completeness magnitude,\n",
    "# so the 'true' completeness magnitude is mc - delta_m / 2\n",
    "\n",
    "\n",
    "def round_half_up(n, decimals=0):\n",
    "    # this is because numpy does weird rounding.\n",
    "    multiplier = 10 ** decimals\n",
    "    return np.floor(n * multiplier + 0.5) / multiplier\n",
    "\n",
    "\n",
    "def estimate_beta_tinti(magnitudes, mc, weights=None, axis=None, delta_m=0):\n",
    "    # Tinti, S., & Mulargia, F. (1987). Confidence intervals of b values for grouped magnitudes.\n",
    "    # Bulletin of the Seismological Society of America, 77(6), 2125-2134.\n",
    "\n",
    "    if delta_m > 0:\n",
    "        p = (1 + (delta_m / (np.average(magnitudes - mc, weights=weights, axis=axis))))\n",
    "        beta = 1 / delta_m * np.log(p)\n",
    "    else:\n",
    "        beta = 1 / np.average((magnitudes - (mc - delta_m / 2)), weights=weights, axis=axis)\n",
    "    return beta\n",
    "\n",
    "\n",
    "def simulate_magnitudes(n, beta, mc):\n",
    "    mags = np.random.uniform(size=n)\n",
    "    mags = (-1 * np.log(1 - mags) / beta) + mc\n",
    "    return mags\n",
    "\n",
    "\n",
    "def fitted_cdf_discrete(sample, mc, delta_m, x_max=None, beta=None):\n",
    "    if beta is None:\n",
    "        beta = estimate_beta_tinti(sample, mc=mc, delta_m=delta_m)\n",
    "\n",
    "    if x_max is None:\n",
    "        sample_bin_n = (sample.max() - mc) / delta_m\n",
    "    else:\n",
    "        sample_bin_n = (x_max - mc) / delta_m\n",
    "    bins = np.arange(sample_bin_n + 1)\n",
    "    cdf = 1 - np.exp(-beta * delta_m * (bins + 1))\n",
    "    x, y = mc + bins * delta_m, cdf\n",
    "\n",
    "    x, y_count = np.unique(x, return_counts=True)\n",
    "    return x, y[np.cumsum(y_count) - 1]\n",
    "\n",
    "\n",
    "def empirical_cdf(sample, weights=None):\n",
    "    try:\n",
    "        sample = sample.values\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        weights = weights.values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sample_idxs_sorted = np.argsort(sample)\n",
    "    sample_sorted = sample[sample_idxs_sorted]\n",
    "    if weights is not None:\n",
    "        weights_sorted = weights[sample_idxs_sorted]\n",
    "        x, y = sample_sorted, np.cumsum(weights_sorted) / weights_sorted.sum()\n",
    "    else:\n",
    "        x, y = sample_sorted, np.arange(1, len(sample) + 1) / len(sample)\n",
    "\n",
    "    # only return one value per bin\n",
    "    x, y_count = np.unique(x, return_counts=True)\n",
    "    return x, y[np.cumsum(y_count) - 1]\n",
    "\n",
    "\n",
    "def ks_test_gr(sample, mc, delta_m, ks_ds=None, n_samples=10000, beta=None):\n",
    "    sample = sample[sample >= mc - delta_m / 2]\n",
    "    if len(sample) == 0:\n",
    "        print(\"no sample\")\n",
    "        return 1, 0, []\n",
    "    if len(np.unique(sample)) == 1:\n",
    "        print(\"sample contains only one value\")\n",
    "        return 1, 0, []\n",
    "    if beta is None:\n",
    "        beta = estimate_beta_tinti(sample, mc=mc, delta_m=delta_m)\n",
    "\n",
    "    if ks_ds is None:\n",
    "        ks_ds = []\n",
    "\n",
    "        n_sample = len(sample)\n",
    "        simulated_all = round_half_up(\n",
    "            simulate_magnitudes(mc=mc - delta_m / 2, beta=beta, n=n_samples * n_sample) / delta_m\n",
    "        ) * delta_m\n",
    "\n",
    "        x_max = np.max(simulated_all)\n",
    "        x_fit, y_fit = fitted_cdf_discrete(sample, mc=mc, delta_m=delta_m, x_max=x_max, beta=beta)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            simulated = simulated_all[n_sample * i:n_sample * (i + 1)].copy()\n",
    "            x_emp, y_emp = empirical_cdf(simulated)\n",
    "            y_fit_int = np.interp(x_emp, x_fit, y_fit)\n",
    "\n",
    "            ks_d = np.max(np.abs(y_emp - y_fit_int))\n",
    "            ks_ds.append(ks_d)\n",
    "    else:\n",
    "        x_fit, y_fit = fitted_cdf_discrete(sample, mc=mc, delta_m=delta_m, beta=beta)\n",
    "\n",
    "    x_emp, y_emp = empirical_cdf(sample)\n",
    "    y_emp_int = np.interp(x_fit, x_emp, y_emp)\n",
    "\n",
    "    orig_ks_d = np.max(np.abs(y_fit - y_emp_int))\n",
    "\n",
    "    return orig_ks_d, sum(ks_ds >= orig_ks_d) / len(ks_ds), ks_ds\n",
    "\n",
    "\n",
    "def estimate_mc(sample, mcs_test, delta_m, p_pass, stop_when_passed=True, verbose=False, beta=None,\n",
    "                n_samples=10000):\n",
    "    \"\"\"\n",
    "    sample: np array of magnitudes to test\n",
    "    mcs_test: completeness magnitudes to test\n",
    "    delta_m: magnitude bins (sample has to be rounded to bins beforehand)\n",
    "    p_pass: p-value with which the test is passed\n",
    "    stop_when_passed: stop calculations when first mc passes the test\n",
    "    verbose: verbose\n",
    "    beta: if beta is 'known', only estimate mc\n",
    "    n_samples: number of magnitude samples to be generated in p-value calculation of KS distance\n",
    "    \"\"\"\n",
    "\n",
    "    ks_ds = []\n",
    "    ps = []\n",
    "    i = 0\n",
    "    for mc in mcs_test:\n",
    "        if verbose:\n",
    "            print('\\ntesting mc', mc)\n",
    "        ks_d, p, _ = ks_test_gr(sample, mc=mc, delta_m=delta_m, n_samples=n_samples, beta=beta)\n",
    "\n",
    "        ks_ds.append(ks_d)\n",
    "        ps.append(p)\n",
    "\n",
    "        i += 1\n",
    "        if verbose:\n",
    "            print('..p-value: ', p)\n",
    "\n",
    "        if p >= p_pass and stop_when_passed:\n",
    "            break\n",
    "    ps = np.array(ps)\n",
    "    if np.any(ps >= p_pass):\n",
    "        best_mc = mcs_test[np.argmax(ps >= p_pass)]\n",
    "        if beta is None:\n",
    "            beta = estimate_beta_tinti(sample[sample >= best_mc - delta_m / 2], mc=best_mc, delta_m=delta_m)\n",
    "        if verbose:\n",
    "            print(\"\\n\\nFirst mc to pass the test:\", best_mc, \"\\nwith a b-value of:\", beta/np.log(10))\n",
    "    else:\n",
    "        best_mc = None\n",
    "        beta = None\n",
    "        if verbose:\n",
    "            print(\"None of the mcs passed the test.\")\n",
    "\n",
    "    # beta is the Tinti beta - so b value is beta/ln10\n",
    "    # return the b-value\n",
    "    return mcs_test, ks_ds, ps, best_mc, beta/np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# inversion of ETAS parameters\n",
    "#\n",
    "# as described by Mizrahi et al., 2021\n",
    "# Leila Mizrahi, Shyam Nandan, Stefan Wiemer;\n",
    "# The Effect of Declustering on the Size Distribution of Mainshocks.\n",
    "# Seismological Research Letters 2021; doi: https://doi.org/10.1785/0220200231\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "# Extract source params from the Coppersmith and Wells empirical relationships\n",
    "def coppersmith(mag, typ):\n",
    "    # result is in km\n",
    "\n",
    "    # typ is one of the following:\n",
    "    # 1: strike slip fault\n",
    "    # 2: reverse fault\n",
    "    # 3: normal fault\n",
    "    # 4: oblique fault\n",
    "\n",
    "    if typ == 1:\n",
    "        # surface rupture length\n",
    "        SRL = np.power(10, (0.74 * mag - 3.55))\n",
    "        # subsurface rupture length\n",
    "        SSRL = np.power(10, (0.62 * mag - 2.57))\n",
    "        # rupture width\n",
    "        RW = np.power(10, (0.27 * mag - 0.76))\n",
    "        # rupture area\n",
    "        RA = np.power(10, (0.9 * mag - 3.42))\n",
    "        # average slip\n",
    "        AD = np.power(10, (0.9 * mag - 6.32))\n",
    "\n",
    "    elif typ == 2:\n",
    "        # surface rupture length\n",
    "        SRL = np.power(10, (0.63 * mag - 2.86))\n",
    "        # subsurface rupture length\n",
    "        SSRL = np.power(10, (0.58 * mag - 2.42))\n",
    "        # rupture width\n",
    "        RW = np.power(10, (0.41 * mag - 1.61))\n",
    "        # rupture area\n",
    "        RA = np.power(10, (0.98 * mag - 3.99))\n",
    "        # average slip\n",
    "        AD = np.power(10, (0.08 * mag - 0.74))\n",
    "\n",
    "    elif typ == 3:\n",
    "        # surface rupture length\n",
    "        SRL = np.power(10, (0.5 * mag - 2.01))\n",
    "        # subsurface rupture length\n",
    "        SSRL = np.power(10, (0.5 * mag - 1.88))\n",
    "        # rupture width\n",
    "        RW = np.power(10, (0.35 * mag - 1.14))\n",
    "        # rupture area\n",
    "        RA = np.power(10, (0.82 * mag - 2.87))\n",
    "        # average slip\n",
    "        AD = np.power(10, (0.63 * mag - 4.45))\n",
    "\n",
    "    elif typ == 4:\n",
    "        # surface rupture length\n",
    "        SRL = np.power(10, (0.69 * mag - 3.22))\n",
    "        # subsurface rupture length\n",
    "        SSRL = np.power(10, (0.59 * mag - 2.44))\n",
    "        # rupture width\n",
    "        RW = np.power(10, (0.32 * mag - 1.01))\n",
    "        # rupture area\n",
    "        RA = np.power(10, (0.91 * mag - 3.49))\n",
    "        # average slip\n",
    "        AD = np.power(10, (0.69 * mag - 4.80))\n",
    "\n",
    "    return {\n",
    "        'SRL': SRL,\n",
    "        'SSRL': SSRL,\n",
    "        'RW': RW,\n",
    "        'RA': RA,\n",
    "        'AD': AD\n",
    "    }\n",
    "\n",
    "\n",
    "def rectangle_surface(lat1, lat2, lon1, lon2):\n",
    "    l = [[lat1, lon1],\n",
    "         [lat2, lon1],\n",
    "         [lat2, lon2],\n",
    "         [lat1, lon2]]\n",
    "    polygon = Polygon(l)\n",
    "\n",
    "    geom_area = ops.transform(\n",
    "        partial(\n",
    "            pyproj.transform,\n",
    "            pyproj.Proj('EPSG:4326'),\n",
    "            pyproj.Proj(\n",
    "                proj='aea',\n",
    "                lat1=polygon.bounds[0],\n",
    "                lat2=polygon.bounds[2])),\n",
    "        polygon)\n",
    "    return geom_area.area / 1e6\n",
    "\n",
    "\n",
    "def polygon_surface(polygon):\n",
    "    geom_area = ops.transform(\n",
    "        partial(\n",
    "            pyproj.transform,\n",
    "            pyproj.Proj('EPSG:4326'),\n",
    "            pyproj.Proj(\n",
    "                proj='aea',\n",
    "                lat_1=polygon.bounds[0],\n",
    "                lat_2=polygon.bounds[2])),\n",
    "        polygon)\n",
    "    return geom_area.area / 1e6\n",
    "\n",
    "\n",
    "def hav(theta):\n",
    "    return np.square(np.sin(theta / 2))\n",
    "\n",
    "\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(\n",
    "        np.sqrt(\n",
    "            hav(lat_rad_1 - lat_rad_2)\n",
    "            + np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)\n",
    "        )\n",
    "    )\n",
    "    return d\n",
    "\n",
    "\n",
    "def branching_ratio(theta, beta):\n",
    "    log10_mu, log10_k0, a, log10_c, omega, log10_tau, log10_d, gamma, rho = theta\n",
    "    k0 = np.power(10, log10_k0)\n",
    "    c = np.power(10, log10_c)\n",
    "    d = np.power(10, log10_d)\n",
    "    tau = np.power(10, log10_tau)\n",
    "\n",
    "    eta = beta * k0 * np.pi * np.power(d, -rho) * np.power(tau, -omega) * np.exp(c / tau) * upper_gamma_ext(-omega,c / tau) / (rho * (-a + beta + gamma * rho))\n",
    "    return eta\n",
    "\n",
    "\n",
    "def to_days(timediff):\n",
    "    return timediff / dt.timedelta(days=1)\n",
    "\n",
    "\n",
    "def upper_gamma_ext(a, x):\n",
    "    if a > 0:\n",
    "        return gammaincc(a, x) * gamma_func(a)\n",
    "    elif a == 0:\n",
    "        return exp1(x)\n",
    "    else:\n",
    "        return (upper_gamma_ext(a + 1, x) - np.power(x, a)*np.exp(-x)) / a\n",
    "\n",
    "\n",
    "def parameter_array2dict(theta):\n",
    "    return dict(zip(\n",
    "        ['log10_mu', 'log10_k0', 'a', 'log10_c', 'omega', 'log10_tau', 'log10_d', 'gamma', 'rho'],\n",
    "        theta\n",
    "    ))\n",
    "\n",
    "\n",
    "def round_nearest_int(num):\n",
    "    if num-0.5 < np.floor(num):\n",
    "        num = np.floor(num)\n",
    "    else:\n",
    "        num = np.ceil(num)\n",
    "    return num\n",
    "\n",
    "def parameter_dict2array(parameters):\n",
    "    order = ['log10_mu', 'log10_k0', 'a', 'log10_c', 'omega', 'log10_tau', 'log10_d', 'gamma', 'rho']\n",
    "    return np.array([\n",
    "        parameters[key] for key in order\n",
    "    ])\n",
    "\n",
    "\n",
    "def set_initial_values(ranges=None):\n",
    "    if ranges is None:\n",
    "        log10_mu_range = (-10, 0)\n",
    "        log10_k0_range = (-4, 0)\n",
    "        a_range = (0.01, 5.)\n",
    "        log10_c_range = (-8, 0)\n",
    "        omega_range = (-0.99, 1)\n",
    "        log10_tau_range = (0.01, 5)\n",
    "        log10_d_range = (-4, 1)\n",
    "        gamma_range = (0.01, 5.)\n",
    "        rho_range = (0.01, 5.)\n",
    "    else:\n",
    "        log10_mu_range, log10_k0_range, a_range, log10_c_range, omega_range, log10_tau_range, log10_d_range, gamma_range, rho_range = ranges\n",
    "\n",
    "    log10_mu = np.random.uniform(*log10_mu_range)\n",
    "    log10_k0 = np.random.uniform(*log10_k0_range)\n",
    "    a = np.random.uniform(*a_range)\n",
    "    log10_c = np.random.uniform(*log10_c_range)\n",
    "    omega = np.random.uniform(*omega_range)\n",
    "    log10_tau = np.random.uniform(*log10_tau_range)\n",
    "    log10_d = np.random.uniform(*log10_d_range)\n",
    "    gamma = np.random.uniform(*gamma_range)\n",
    "    rho = np.random.uniform(*rho_range)\n",
    "\n",
    "    return [\n",
    "        log10_mu,\n",
    "        log10_k0,\n",
    "        a,\n",
    "        log10_c,\n",
    "        omega,\n",
    "        log10_tau,\n",
    "        log10_d,\n",
    "        gamma,\n",
    "        rho\n",
    "    ]\n",
    "\n",
    "\n",
    "def prepare_catalog(data, mc, coppersmith_multiplier, timewindow_start, timewindow_end, earth_radius,\n",
    "                    delta_m=0):\n",
    "    # precalculates distances in time and space between events that are potentially related to each other\n",
    "    calc_start = dt.datetime.now()\n",
    "\n",
    "    # only use data above completeness magnitude\n",
    "    if delta_m > 0:\n",
    "        data[\"mag\"] = round_half_up(data[\"mag\"] / delta_m) * delta_m\n",
    "    relevant = data.query(\"mag >= @mc\").copy()\n",
    "    relevant.sort_values(by='time', inplace=True)\n",
    "\n",
    "    # all entries can be sources, but targets only after timewindow start\n",
    "    targets = relevant.query(\"time>=@timewindow_start\").copy()\n",
    "\n",
    "    # calculate some source stuff - always assume oblique fault to avoid having to know fault type\n",
    "    relevant[\"distance_range_squared\"] = np.square(\n",
    "        coppersmith(relevant[\"mag\"], 4)[\"SSRL\"] * coppersmith_multiplier\n",
    "    )\n",
    "    relevant[\"source_to_end_time_distance\"] = to_days(timewindow_end - relevant[\"time\"])\n",
    "    relevant[\"pos_source_to_start_time_distance\"] = to_days(timewindow_start - relevant[\"time\"]).apply(\n",
    "        lambda x: max(x, 0)\n",
    "    )\n",
    "\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    targets['target_lat_rad'] = np.radians(targets['lat'])\n",
    "    targets['target_lon_rad'] = np.radians(targets['lon'])\n",
    "    targets[\"target_time\"] = targets[\"time\"]\n",
    "    targets[\"target_id\"] = targets.index\n",
    "    targets[\"target_time\"] = targets[\"time\"]\n",
    "    # columns that are needed later\n",
    "    targets[\"source_id\"] = 'i'\n",
    "    targets[\"source_magnitude\"] = 0.0\n",
    "    targets[\"time_distance\"] = 0.0\n",
    "    targets[\"spatial_distance_squared\"] = 0.0\n",
    "    targets[\"source_to_end_time_distance\"] = 0.0\n",
    "    targets[\"pos_source_to_start_time_distance\"] = 0.0\n",
    "\n",
    "    targets = targets.sort_values(by=\"time\")\n",
    "\n",
    "    # define index and columns that are later going to be needed\n",
    "    if pd.__version__ >= '0.24.0':\n",
    "        index = pd.MultiIndex(\n",
    "            levels=[[], []],\n",
    "            names=[\"source_id\", \"target_id\"],\n",
    "            codes=[[], []]\n",
    "        )\n",
    "    else:\n",
    "        index = pd.MultiIndex(\n",
    "            levels=[[], []],\n",
    "            names=[\"source_id\", \"target_id\"],\n",
    "            labels=[[], []]\n",
    "        )\n",
    "\n",
    "    columns = [\n",
    "        \"target_time\",\n",
    "        \"source_magnitude\",\n",
    "        \"spatial_distance_squared\",\n",
    "        \"time_distance\",\n",
    "        \"source_to_end_time_distance\",\n",
    "        \"pos_source_to_start_time_distance\"\n",
    "    ]\n",
    "    res_df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    print('  number of sources:', len(relevant.index))\n",
    "    print('  number of targets:', len(targets.index))\n",
    "    for source in relevant.itertuples():\n",
    "        stime = source.time\n",
    "\n",
    "        # filter potential targets\n",
    "        if source.time < timewindow_start:\n",
    "            potential_targets = targets.copy()\n",
    "        else:\n",
    "            potential_targets = targets.query(\n",
    "                \"time>@stime\"\n",
    "            ).copy()\n",
    "        targets = potential_targets.copy()\n",
    "\n",
    "        if potential_targets.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # get values of source event\n",
    "        slatrad = np.radians(source.lat)\n",
    "        slonrad = np.radians(source.lon)\n",
    "        drs = source.distance_range_squared\n",
    "\n",
    "        # get source id and info of target events\n",
    "        potential_targets[\"source_id\"] = source.Index\n",
    "        potential_targets[\"source_magnitude\"] = source.mag\n",
    "\n",
    "        # calculate space and time distance from source to target event\n",
    "        potential_targets[\"time_distance\"] = to_days(potential_targets[\"target_time\"] - stime)\n",
    "\n",
    "        potential_targets[\"spatial_distance_squared\"] = np.square(\n",
    "            haversine(\n",
    "                slatrad,\n",
    "                potential_targets['target_lat_rad'],\n",
    "                slonrad,\n",
    "                potential_targets['target_lon_rad'],\n",
    "                earth_radius\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # filter for only small enough distances\n",
    "        potential_targets.query(\"spatial_distance_squared <= @drs\", inplace=True)\n",
    "\n",
    "        # calculate time distance from source event to timewindow boundaries for integration later\n",
    "        potential_targets[\"source_to_end_time_distance\"] = source.source_to_end_time_distance\n",
    "        potential_targets[\"pos_source_to_start_time_distance\"] = source.pos_source_to_start_time_distance\n",
    "\n",
    "        # append to resulting dataframe\n",
    "        df_list.append(potential_targets)\n",
    "\n",
    "    res_df = pd.concat(df_list)[[\"source_id\", \"target_id\"] + columns].reset_index().set_index(\n",
    "        [\"source_id\", \"target_id\"])\n",
    "\n",
    "    print('    took', (dt.datetime.now() - calc_start), 'to prepare the distances\\n')\n",
    "\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def triggering_kernel(metrics, params):\n",
    "    # metrics are the time-distance, spatial distance squared, and source magnitude\n",
    "    # params are the ETAS \n",
    "    # given time distance in days and squared space distance in square km and magnitude of target event,\n",
    "    # calculate the (not normalized) likelihood, that source event triggered target event\n",
    "\n",
    "    time_distance, spatial_distance_squared, m = metrics\n",
    "    theta, mc = params\n",
    "\n",
    "    log10_mu, log10_k0, a, log10_c, omega, log10_tau, log10_d, gamma, rho = theta\n",
    "    mu = np.power(10, log10_mu)\n",
    "    k0 = np.power(10, log10_k0)\n",
    "    c = np.power(10, log10_c)\n",
    "    tau = np.power(10, log10_tau)\n",
    "    d = np.power(10, log10_d)\n",
    "\n",
    "    aftershock_number = k0 * np.exp(a * (m - mc))\n",
    "    time_decay = np.exp(-time_distance / tau) / np.power((time_distance + c), (1 + omega))\n",
    "    space_decay = 1 / np.power(\n",
    "        (spatial_distance_squared + d * np.exp(gamma * (m - mc))),\n",
    "        (1 + rho)\n",
    "    )\n",
    "\n",
    "    res = aftershock_number * time_decay * space_decay\n",
    "    return res\n",
    "\n",
    "\n",
    "def expectation_step(distances, target_events, source_events, params, verbose=False):\n",
    "    calc_start = dt.datetime.now()\n",
    "    theta, mc = params\n",
    "    log10_mu, log10_k0, a, log10_c, omega, log10_tau, log10_d, gamma, rho = theta\n",
    "    # print('I am doing the expectation step with parameters', theta)\n",
    "    mu = np.power(10, log10_mu)\n",
    "    k0 = np.power(10, log10_k0)\n",
    "    c = np.power(10, log10_c)\n",
    "    tau = np.power(10, log10_tau)\n",
    "    d = np.power(10, log10_d)\n",
    "\n",
    "    # calculate the triggering density values gij\n",
    "    if verbose:\n",
    "        print('    calculating gij')\n",
    "    Pij_0 = distances.copy()\n",
    "    Pij_0[\"gij\"] = triggering_kernel(\n",
    "        [\n",
    "            Pij_0[\"time_distance\"],\n",
    "            Pij_0[\"spatial_distance_squared\"],\n",
    "            Pij_0[\"source_magnitude\"]\n",
    "        ],\n",
    "        [theta, mc]\n",
    "    )\n",
    "\n",
    "    # calculate muj for each target. currently constant, could be improved - this is a problem!\n",
    "    target_events_0 = target_events.copy()\n",
    "    target_events_0[\"mu\"] = mu\n",
    "\n",
    "    # calculate triggering probabilities Pij\n",
    "    if verbose:\n",
    "        print('    calculating Pij')\n",
    "    Pij_0[\"tot_rates\"] = 0\n",
    "    Pij_0[\"tot_rates\"] = Pij_0[\"tot_rates\"].add(Pij_0[\"gij\"].sum(level=1)).add(target_events_0[\"mu\"])\n",
    "    Pij_0[\"Pij\"] = Pij_0[\"gij\"].div(Pij_0[\"tot_rates\"])\n",
    "\n",
    "    # calculate probabilities of being triggered or background\n",
    "    target_events_0[\"P_triggered\"] = 0\n",
    "    target_events_0[\"P_triggered\"] = target_events_0[\"P_triggered\"].add(Pij_0[\"Pij\"].sum(level=1)).fillna(0)\n",
    "    target_events_0[\"P_background\"] = 1 - target_events_0[\"P_triggered\"]\n",
    "\n",
    "    # calculate expected number of background events\n",
    "    if verbose:\n",
    "        print('    calculating n_hat and l_hat\\n')\n",
    "    n_hat_0 = target_events_0[\"P_background\"].sum()\n",
    "\n",
    "    # calculate aftershocks per source event\n",
    "    source_events_0 = source_events.copy()\n",
    "    source_events_0[\"l_hat\"] = Pij_0[\"Pij\"].sum(level=0)\n",
    "\n",
    "    print('    expectation step took ', dt.datetime.now() - calc_start)\n",
    "    return Pij_0, target_events_0, source_events_0, n_hat_0\n",
    "\n",
    "\n",
    "def expected_aftershocks(event, params, no_start=False, no_end=False):\n",
    "    theta, mc = params\n",
    "\n",
    "    log10_k0, a, log10_c, omega, log10_tau, log10_d, gamma, rho = theta\n",
    "    k0 = np.power(10, log10_k0)\n",
    "    c = np.power(10, log10_c)\n",
    "    tau = np.power(10, log10_tau)\n",
    "    d = np.power(10, log10_d)\n",
    "\n",
    "    if no_start:\n",
    "        if no_end:\n",
    "            event_magnitude = event\n",
    "        else:\n",
    "            event_magnitude, event_time_to_end = event\n",
    "    else:\n",
    "        if no_end:\n",
    "            event_magnitude, event_time_to_start = event\n",
    "        else:\n",
    "            event_magnitude, event_time_to_start, event_time_to_end = event\n",
    "\n",
    "    number_factor = k0 * np.exp(a * (event_magnitude - mc))\n",
    "    area_factor = np.pi * np.power(\n",
    "        d * np.exp(gamma * (event_magnitude - mc)),\n",
    "        -1 * rho\n",
    "    ) / rho\n",
    "\n",
    "    time_factor = np.exp(c/tau) * np.power(tau, -omega)  # * gamma_func(-omega)\n",
    "\n",
    "    if no_start:\n",
    "        time_fraction = upper_gamma_ext(-omega, c/tau)\n",
    "    else:\n",
    "        time_fraction = upper_gamma_ext(-omega, (event_time_to_start + c)/tau)\n",
    "    if not no_end:\n",
    "        time_fraction = time_fraction - upper_gamma_ext(-omega, (event_time_to_end + c)/tau)\n",
    "\n",
    "    time_factor = time_factor * time_fraction\n",
    "\n",
    "    return number_factor * area_factor * time_factor\n",
    "\n",
    "\n",
    "def ll_aftershock_term(l_hat, g):\n",
    "    mask = g != 0\n",
    "    term = -1 * gammaln(l_hat + 1) - g\n",
    "    term = term + l_hat * np.where(mask, np.log(g, where=mask), -300)\n",
    "    return term\n",
    "\n",
    "\n",
    "def neg_log_likelihood(theta, args):\n",
    "    mc, n_hat, Pij, source_events, timewindow_length, area = args\n",
    "\n",
    "    assert Pij.index.names == (\"source_id\", \"target_id\"), \"Pij must have multiindex with names 'source_id', 'target_id'\"\n",
    "    assert source_events.index.name == \"source_id\", \"source_events must have index with name 'source_id'\"\n",
    "\n",
    "    log10_k0, a, log10_c, omega, log10_tau, log10_d, gamma, rho = theta\n",
    "    k0 = np.power(10, log10_k0)\n",
    "    c = np.power(10, log10_c)\n",
    "    tau = np.power(10, log10_tau)\n",
    "    d = np.power(10, log10_d)\n",
    "\n",
    "    source_events[\"G\"] = expected_aftershocks(\n",
    "        [\n",
    "            source_events[\"source_magnitude\"],\n",
    "            source_events[\"pos_source_to_start_time_distance\"],\n",
    "            source_events[\"source_to_end_time_distance\"]\n",
    "        ],\n",
    "        [theta, mc]\n",
    "    )\n",
    "\n",
    "    aftershock_term = ll_aftershock_term(\n",
    "        source_events[\"l_hat\"],\n",
    "        source_events[\"G\"],\n",
    "    ).sum()\n",
    "\n",
    "    # space time distribution term\n",
    "    Pij[\"likelihood_term\"] = (\n",
    "        (omega * np.log(tau) - np.log(upper_gamma_ext(-omega, c/tau))\n",
    "         + np.log(rho) + rho * np.log(\n",
    "            d * np.exp(gamma * (Pij[\"source_magnitude\"] - mc))\n",
    "        ))\n",
    "        - ((1 + rho) * np.log(\n",
    "            Pij[\"spatial_distance_squared\"] + (\n",
    "                d * np.exp(gamma * (Pij[\"source_magnitude\"] - mc))\n",
    "            )\n",
    "        ))\n",
    "        - (1 + omega) * np.log(Pij[\"time_distance\"] + c)\n",
    "        - (Pij[\"time_distance\"] + c) / tau\n",
    "        - np.log(np.pi)\n",
    "\n",
    "    )\n",
    "    distribution_term = Pij[\"Pij\"].mul(Pij[\"likelihood_term\"]).sum()\n",
    "\n",
    "    total = aftershock_term + distribution_term\n",
    "\n",
    "    return -1 * total\n",
    "\n",
    "\n",
    "def optimize_parameters(theta_0, ranges, args):\n",
    "    start_calc = dt.datetime.now()\n",
    "\n",
    "    mc, n_hat, Pij, source_events, timewindow_length, area = args\n",
    "    log10_mu_range, log10_k0_range, a_range, log10_c_range, omega_range, log10_tau_range, log10_d_range, gamma_range, rho_range = ranges\n",
    "\n",
    "    log10_mu, log10_k0, a, log10_c, omega, log10_tau, log10_d, gamma, rho = theta_0\n",
    "    mu = np.power(10, log10_mu)\n",
    "    k0 = np.power(10, log10_k0)\n",
    "    c = np.power(10, log10_c)\n",
    "    tau = np.power(10, log10_tau)\n",
    "    d = np.power(10, log10_d)\n",
    "\n",
    "    # estimate mu independently and remove from parameters\n",
    "    mu_hat = n_hat / (area * timewindow_length)\n",
    "    theta_0_without_mu = log10_k0, a, log10_c, omega, log10_tau, log10_d, gamma, rho\n",
    "\n",
    "    bounds = [\n",
    "        log10_k0_range,\n",
    "        a_range,\n",
    "        log10_c_range,\n",
    "        omega_range,\n",
    "        log10_tau_range,\n",
    "        log10_d_range,\n",
    "        gamma_range,\n",
    "        rho_range\n",
    "    ]\n",
    "\n",
    "    res = minimize(\n",
    "        neg_log_likelihood,\n",
    "        x0=theta_0_without_mu,\n",
    "        bounds=bounds,\n",
    "        args=args,\n",
    "        tol=1e-12,\n",
    "    )\n",
    "\n",
    "    new_theta_without_mu = res.x\n",
    "    new_theta = [np.log10(mu_hat), *new_theta_without_mu]\n",
    "\n",
    "    print(\"    optimization step took \", dt.datetime.now() - start_calc)\n",
    "\n",
    "    return np.array(new_theta)\n",
    "\n",
    "\n",
    "def invert_etas_params(\n",
    "        metadata,\n",
    "        timewindow_end=None,\n",
    "        globe=False,\n",
    "        store_pij=False,\n",
    "        store_results=True\n",
    "):\n",
    "    \"\"\"\n",
    "        Inverts ETAS parameters.\n",
    "        metadata can be either a string (path to json file with stored metadata)\n",
    "        or a dict. accepted & necessary keywords are:\n",
    "            fn_catalog: filename of the catalog (absolute path or filename in current directory)\n",
    "                        catalog is expected to be a csv file with the following columns:\n",
    "                        id, latitude, longitude, time, magnitude\n",
    "                        id needs to contain a unique identifier for each event\n",
    "                        time contains datetime of event occurrence\n",
    "                        see synthetic_catalog.csv for an example\n",
    "            data_path: path where result data will be stored\n",
    "            auxiliary_start: start date of the auxiliary catalog (str or datetime).\n",
    "                             events of the auxiliary catalog act as sources, not as targets\n",
    "            timewindow_start: start date of the primary catalog , end date of auxiliary catalog (str or datetime).\n",
    "                             events of the primary catalog act as sources and as targets\n",
    "            timewindow_end: end date of the primary catalog (str or datetime)\n",
    "            mc: cutoff magnitude. catalog needs to be complete above mc\n",
    "            delta_m: size of magnitude bins\n",
    "            coppersmith_multiplier: events further apart from each other than\n",
    "                                    coppersmith subsurface rupture length * this multiplier\n",
    "                                    are considered to be uncorrelated (to reduce size of distance matrix)\n",
    "            shape_coords: coordinates of the boundary of the region to consider\n",
    "                          (list of lists, i.e. [[lat1, lon1], [lat2, lon2], [lat3, lon3]])\n",
    "    \"\"\"\n",
    "    ####################\n",
    "    # preparing metadata\n",
    "    ####################\n",
    "    print(\"PREPARING METADATA...\\n\")\n",
    "\n",
    "    if isinstance(metadata, str):\n",
    "        # if metadata is a filename, read the file (assuming it's json)\n",
    "        with open(metadata, 'r') as f:\n",
    "            parameters_dict = json.load(f)\n",
    "    else:\n",
    "        parameters_dict = metadata\n",
    "\n",
    "    fn_catalog = parameters_dict[\"fn_catalog\"]\n",
    "    print(\"  using catalog: \" + fn_catalog)\n",
    "\n",
    "    data_path = parameters_dict[\"data_path\"]\n",
    "    if data_path == \"\":\n",
    "        print(\"  Data will be stored in \" + os.path.dirname(os.path.abspath(__file__)))\n",
    "    else:\n",
    "        print(\"  Data will be stored in \" + data_path)\n",
    "\n",
    "    auxiliary_start = pd.to_datetime(parameters_dict[\"auxiliary_start\"])\n",
    "    timewindow_start = pd.to_datetime(parameters_dict[\"timewindow_start\"])\n",
    "    timewindow_end = timewindow_end or pd.to_datetime(parameters_dict[\"timewindow_end\"])\n",
    "    print(\n",
    "        \"  Time Window: \" + str(auxiliary_start)\n",
    "        + \" (aux) - \" + str(timewindow_start) + \" (start) - \" + str(timewindow_end) + \" (end)\"\n",
    "    )\n",
    "\n",
    "    mc = parameters_dict[\"mc\"]\n",
    "    delta_m = parameters_dict[\"delta_m\"]\n",
    "    print(\"  mc is \" + str(mc) + \" and delta_m is \" + str(delta_m))\n",
    "\n",
    "    coppersmith_multiplier = parameters_dict[\"coppersmith_multiplier\"]\n",
    "    print(\"  coppersmith multiplier is \" + str(coppersmith_multiplier))\n",
    "\n",
    "    if globe:\n",
    "        coordinates = []\n",
    "    else:\n",
    "        if type(parameters_dict[\"shape_coords\"]) is str:\n",
    "            coordinates = np.array(eval(parameters_dict[\"shape_coords\"]))\n",
    "        else:\n",
    "            coordinates = np.array(parameters_dict[\"shape_coords\"])\n",
    "        pprint.pprint(\"  Coordinates of region: \" + str(list(coordinates)))\n",
    "\n",
    "    # defining some other stuff here..\n",
    "\n",
    "    timewindow_length = to_days(timewindow_end - timewindow_start)\n",
    "\n",
    "    fn_parameters = data_path + 'parameters.json'\n",
    "    fn_ip = data_path + 'ind_and_bg_probs.csv'\n",
    "    fn_src = data_path + 'sources.csv'\n",
    "    fn_dist = data_path + 'distances.csv'\n",
    "    fn_pij = data_path + 'pij.csv'\n",
    "\n",
    "    # earth radius in km\n",
    "    earth_radius = 6.3781e3\n",
    "\n",
    "    if globe:\n",
    "        area = earth_radius ** 2 * 4 * np.pi\n",
    "    else:\n",
    "        poly = Polygon(coordinates)\n",
    "        area = polygon_surface(poly)\n",
    "    print(\"  Region has \" + str(area) + \" square km\")\n",
    "\n",
    "    # ranges for parameters\n",
    "    log10_mu_range = (-10, 0)\n",
    "    log10_k0_range = (-4, 0)\n",
    "    a_range = (0.01, 5.)\n",
    "    log10_c_range = (-8, 0)\n",
    "    omega_range = (-0.99, 1)\n",
    "    log10_tau_range = (0.01, 5)\n",
    "    log10_d_range = (-4, 3)\n",
    "    gamma_range = (0.01, 5.)\n",
    "    rho_range = (0.01, 5.)\n",
    "\n",
    "    ranges = log10_mu_range, log10_k0_range, a_range, log10_c_range, omega_range, log10_tau_range, log10_d_range, gamma_range, rho_range\n",
    "\n",
    "    # start inversion\n",
    "    print(\"\\n\\nINITIALIZING\\n\")\n",
    "    print(\"  reading data..\\n\")\n",
    "    df_full = pd.read_csv(fn_catalog, index_col=0, dtype={\"url\": str, \"alert\": str})\n",
    "    df_full[\"time\"] = pd.to_datetime(df_full[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    df_full = df_full.infer_objects()\n",
    "    #Create gdf\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_full, geometry=gpd.points_from_xy(df_full.lat, df_full.lon))\n",
    "\n",
    "    # filter for events in region of interest\n",
    "    #if not globe:\n",
    "        #df = gdf[gdf.within(poly)].copy()\n",
    "        #df.drop(\"geometry\", axis=1, inplace=True)\n",
    "    #else:\n",
    "    df = df_full.copy()\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" out of \" + str(len(df_full)) + \" events lie within target region.\")\n",
    "\n",
    "    # filter for events above cutoff magnitude - delta_m/2\n",
    "    if delta_m > 0:\n",
    "        df[\"mag\"] = round_half_up(df[\"mag\"] / delta_m) * delta_m\n",
    "    df.query(\"mag>=@mc-@delta_m/2\", inplace=True)\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" events are above cutoff magnitude\")\n",
    "\n",
    "    # filter for eventsin relevant timewindow\n",
    "    df.query(\"time >= @ auxiliary_start and time < @ timewindow_end\", inplace=True)\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" events are within time window\\n\\n\")\n",
    "\n",
    "    print('  calculating distances..\\n')\n",
    "\n",
    "    distances = prepare_catalog(\n",
    "        df,\n",
    "        mc=mc - delta_m / 2,\n",
    "        coppersmith_multiplier=coppersmith_multiplier,\n",
    "        timewindow_start=timewindow_start,\n",
    "        timewindow_end=timewindow_end,\n",
    "        earth_radius=earth_radius,\n",
    "        delta_m=delta_m\n",
    "    )\n",
    "    # distances.to_csv(fn_dist)\n",
    "\n",
    "    print('  preparing source and target events..\\n')\n",
    "\n",
    "    target_events = df.copy()\n",
    "    target_events.query(\"time > @ timewindow_start\", inplace=True)\n",
    "    target_events.index.name = \"target_id\"\n",
    "\n",
    "    beta = estimate_beta_tinti(target_events[\"mag\"], mc=mc, delta_m=delta_m)\n",
    "    print(\"  beta of primary catalog is\", beta)\n",
    "\n",
    "    source_columns = [\n",
    "        \"source_magnitude\",\n",
    "        \"source_to_end_time_distance\",\n",
    "        \"pos_source_to_start_time_distance\"\n",
    "    ]\n",
    "\n",
    "    source_events = pd.DataFrame(\n",
    "        distances[\n",
    "            source_columns\n",
    "        ].groupby(\"source_id\").first()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print('  using input initial values for theta\\n')\n",
    "        initial_values = parameter_dict2array(\n",
    "            parameters_dict[\"theta_0\"]\n",
    "        )\n",
    "    except KeyError:\n",
    "        print('  randomly chosing initial values for theta\\n')\n",
    "        initial_values = set_initial_values()\n",
    "\n",
    "\n",
    "    #################\n",
    "    # start inversion\n",
    "    #################\n",
    "    print('\\n\\nSTART INVERSION!\\n')\n",
    "\n",
    "    diff_to_before = 100\n",
    "    i = 0\n",
    "    while diff_to_before >= 0.001:\n",
    "        print('iteration ' + str(i) + '\\n')\n",
    "\n",
    "        if i == 0:\n",
    "            parameters = initial_values\n",
    "\n",
    "        print('  expectation\\n')\n",
    "        Pij, target_events, source_events, n_hat = expectation_step(\n",
    "            distances=distances,\n",
    "            target_events=target_events,\n",
    "            source_events=source_events,\n",
    "            params=[parameters, mc - delta_m / 2],\n",
    "            verbose=True\n",
    "        )\n",
    "        print('      n_hat:', n_hat, '\\n')\n",
    "\n",
    "        print('  maximization\\n')\n",
    "        args = [mc - delta_m / 2, n_hat, Pij, source_events, timewindow_length, area]\n",
    "\n",
    "        new_parameters = optimize_parameters(\n",
    "            theta_0=parameters,\n",
    "            args=args,\n",
    "            ranges=ranges\n",
    "        )\n",
    "        print('    new parameters:\\n')\n",
    "        pprint.pprint(\n",
    "            parameter_array2dict(new_parameters),\n",
    "            indent=4\n",
    "        )\n",
    "        diff_to_before = np.sum(np.abs(parameters - new_parameters))\n",
    "        print('\\n    difference to previous:', diff_to_before)\n",
    "\n",
    "        br = branching_ratio(parameters, beta)\n",
    "        print('    branching ratio:', br, '\\n')\n",
    "        parameters = new_parameters\n",
    "        i += 1\n",
    "\n",
    "    print('stopping here. converged after', i, 'iterations.')\n",
    "    print('  last expectation step\\n')\n",
    "    Pij, target_events, source_events, n_hat = expectation_step(\n",
    "        distances=distances,\n",
    "        target_events=target_events,\n",
    "        source_events=source_events,\n",
    "        params=[parameters, mc - delta_m / 2],\n",
    "        verbose=True\n",
    "    )\n",
    "    print('      n_hat:', n_hat)\n",
    "    if store_results:\n",
    "        target_events.to_csv(fn_ip)\n",
    "        source_events.to_csv(fn_src)\n",
    "\n",
    "        all_info = {\n",
    "            \"auxiliary_start\": str(auxiliary_start),\n",
    "            \"timewindow_start\": str(timewindow_start),\n",
    "            \"timewindow_end\": str(timewindow_end),\n",
    "            \"timewindow_length\": timewindow_length,\n",
    "            \"mc\": mc,\n",
    "            \"beta\": beta,\n",
    "            \"n_target_events\": len(target_events),\n",
    "            \"delta_m\": delta_m,\n",
    "            \"shape_coords\": str(list(coordinates)),\n",
    "            \"earth_radius\": earth_radius,\n",
    "            \"area\": area,\n",
    "            \"coppersmith_multiplier\": coppersmith_multiplier,\n",
    "            \"log10_mu_range\": log10_mu_range,\n",
    "            \"log10_k0_range\": log10_k0_range,\n",
    "            \"a_range\": a_range,\n",
    "            \"log10_c_range\": log10_c_range,\n",
    "            \"omega_range\": omega_range,\n",
    "            \"log10_tau_range\": log10_tau_range,\n",
    "            \"log10_d_range\": log10_d_range,\n",
    "            \"gamma_range\": gamma_range,\n",
    "            \"rho_range\": rho_range,\n",
    "            \"ranges\": ranges,\n",
    "            \"fn\": fn_catalog,\n",
    "            \"fn_dist\": fn_dist,\n",
    "            \"fn_ip\": fn_ip,\n",
    "            \"fn_src\": fn_src,\n",
    "            \"calculation_date\": str(dt.datetime.now()),\n",
    "            \"initial_values\": str(parameter_array2dict(initial_values)),\n",
    "            \"final_parameters\": str(parameter_array2dict(new_parameters)),\n",
    "            \"n_iterations\": i\n",
    "        }\n",
    "\n",
    "        info_json = json.dumps(all_info)\n",
    "        f = open(fn_parameters, \"w\")\n",
    "        f.write(info_json)\n",
    "        f.close()\n",
    "\n",
    "    if store_pij:\n",
    "        Pij.to_csv(fn_pij)\n",
    "\n",
    "    return parameter_array2dict(new_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decluster catalog based on the Mizrahi functions\n",
    "# This is simple, threshold-based declustering based on the Mizrahi ETAS-Background algorithm \n",
    "# splits catalog into background and triggered events\n",
    "    \n",
    "def Mizrahi_ETAS_BG_decluster(\n",
    "        metadata,\n",
    "        timewindow_end=None,\n",
    "        globe=False,\n",
    "        store_pij=False,\n",
    "        store_results=True):\n",
    "    \"\"\"\n",
    "        Declusters an earthquake catalog using the ETAS-based approach of Mizrahi et al. (2021).\n",
    "        metadata can be either a string (path to json file with stored metadata)\n",
    "        or a dict. accepted & necessary keywords are:\n",
    "            fn_catalog: filename of the catalog (absolute path or filename in current directory)\n",
    "                        catalog is expected to be a csv file with the following columns:\n",
    "                        id, latitude, longitude, time, magnitude\n",
    "                        id needs to contain a unique identifier for each event\n",
    "                        time contains datetime of event occurrence\n",
    "                        see synthetic_catalog.csv for an example\n",
    "            data_path: path where result data will be stored\n",
    "            auxiliary_start: start date of the auxiliary catalog (str or datetime).\n",
    "                             events of the auxiliary catalog act as sources, not as targets\n",
    "            timewindow_start: start date of the primary catalog , end date of auxiliary catalog (str or datetime).\n",
    "                             events of the primary catalog act as sources and as targets\n",
    "            timewindow_end: end date of the primary catalog (str or datetime)\n",
    "            mc: cutoff magnitude. catalog needs to be complete above mc\n",
    "            delta_m: size of magnitude bins\n",
    "            coppersmith_multiplier: events further apart from each other than\n",
    "                                    coppersmith subsurface rupture length * this multiplier\n",
    "                                    are considered to be uncorrelated (to reduce size of distance matrix)\n",
    "            shape_coords: coordinates of the boundary of the region to consider\n",
    "                          (list of lists, i.e. [[lat1, lon1], [lat2, lon2], [lat3, lon3]])\n",
    "    \"\"\"\n",
    "    ####################\n",
    "    # preparing metadata\n",
    "    ####################\n",
    "    print(\"PREPARING METADATA...\\n\")\n",
    "\n",
    "    if isinstance(metadata, str):\n",
    "        # if metadata is a filename, read the file (assuming it's json)\n",
    "        with open(metadata, 'r') as f:\n",
    "            parameters_dict = json.load(f)\n",
    "    else:\n",
    "        parameters_dict = metadata\n",
    "\n",
    "    fn_catalog = parameters_dict[\"fn_catalog\"]\n",
    "    print(\"  using catalog: \" + fn_catalog)\n",
    "\n",
    "    data_path = parameters_dict[\"data_path\"]\n",
    "    if data_path == \"\":\n",
    "        print(\"  Data will be stored in \" + os.path.dirname(os.path.abspath(__file__)))\n",
    "    else:\n",
    "        print(\"  Data will be stored in \" + data_path)\n",
    "\n",
    "    auxiliary_start = pd.to_datetime(parameters_dict[\"auxiliary_start\"])\n",
    "    timewindow_start = pd.to_datetime(parameters_dict[\"timewindow_start\"])\n",
    "    timewindow_end = timewindow_end or pd.to_datetime(parameters_dict[\"timewindow_end\"])\n",
    "    print(\n",
    "        \"  Time Window: \" + str(auxiliary_start)\n",
    "        + \" (aux) - \" + str(timewindow_start) + \" (start) - \" + str(timewindow_end) + \" (end)\"\n",
    "    )\n",
    "\n",
    "    mc = parameters_dict[\"mc\"]\n",
    "    delta_m = parameters_dict[\"delta_m\"]\n",
    "    print(\"  mc is \" + str(mc) + \" and delta_m is \" + str(delta_m))\n",
    "\n",
    "    coppersmith_multiplier = parameters_dict[\"coppersmith_multiplier\"]\n",
    "    print(\"  coppersmith multiplier is \" + str(coppersmith_multiplier))\n",
    "\n",
    "    if globe:\n",
    "        coordinates = []\n",
    "    else:\n",
    "        if type(parameters_dict[\"shape_coords\"]) is str:\n",
    "            coordinates = np.array(eval(parameters_dict[\"shape_coords\"]))\n",
    "        else:\n",
    "            coordinates = np.array(parameters_dict[\"shape_coords\"])\n",
    "        pprint.pprint(\"  Coordinates of region: \" + str(list(coordinates)))\n",
    "\n",
    "    # defining some other stuff here..\n",
    "\n",
    "    timewindow_length = to_days(timewindow_end - timewindow_start)\n",
    "\n",
    "    fn_parameters = data_path + 'parameters.json'\n",
    "    fn_ip = data_path + 'ind_and_bg_probs.csv'\n",
    "    fn_src = data_path + 'sources.csv'\n",
    "    fn_dist = data_path + 'distances.csv'\n",
    "    fn_pij = data_path + 'pij.csv'\n",
    "    fn_cat = data_path + 'ETAS_declustered_cat.csv'\n",
    "    fn_rejcat = data_path + 'ETAS_rejected_evs.csv'\n",
    "\n",
    "\n",
    "    # earth radius in km\n",
    "    earth_radius = 6.3781e3\n",
    "\n",
    "    if globe:\n",
    "        area = earth_radius ** 2 * 4 * np.pi\n",
    "    else:\n",
    "        poly = Polygon(coordinates)\n",
    "        area = polygon_surface(poly)\n",
    "    print(\"  Region has \" + str(area) + \" square km\")\n",
    "\n",
    "    # Initialise key data columns\n",
    "    print(\"\\n\\nINITIALIZING\\n\")\n",
    "    print(\"  reading data..\\n\")\n",
    "    df_full = pd.read_csv(fn_catalog, index_col=0, dtype={\"url\": str, \"alert\": str})\n",
    "    df_full[\"time\"] = pd.to_datetime(df_full[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "    df_full['depth_km'] *=0.001\n",
    "\n",
    "\n",
    "    #Fix dtypes\n",
    "    df_full = df_full.infer_objects()\n",
    "    #Create gdf\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_full, geometry=gpd.points_from_xy(df_full.lat, df_full.lon))\n",
    "\n",
    "    df = df_full.copy()\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" out of \" + str(len(df_full)) + \" events lie within target region.\")\n",
    "\n",
    "    # filter for events above cutoff magnitude - delta_m/2\n",
    "    print('Cutoff mag is: ', mc-delta_m/2)\n",
    "    if delta_m > 0:\n",
    "        df[\"mag\"] = round_half_up(df[\"mag\"] / delta_m) * delta_m\n",
    "    m_cut = mc-delta_m/2\n",
    "    df.query(\"mag >= @m_cut\", inplace=True)\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" events are above cutoff magnitude\")\n",
    "\n",
    "    # filter for eventsin relevant timewindow\n",
    "    df.query(\"time >= @ auxiliary_start and time < @ timewindow_end\", inplace=True)\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" events are within time window\\n\\n\")\n",
    "\n",
    "    print('  calculating distances..\\n')\n",
    "\n",
    "    distances = prepare_catalog(\n",
    "        df,\n",
    "        mc=mc - delta_m / 2,\n",
    "        coppersmith_multiplier=coppersmith_multiplier,\n",
    "        timewindow_start=timewindow_start,\n",
    "        timewindow_end=timewindow_end,\n",
    "        earth_radius=earth_radius,\n",
    "        delta_m=delta_m\n",
    "    )\n",
    "    # distances.to_csv(fn_dist)\n",
    "\n",
    "    print('  preparing source and target events..\\n')\n",
    "\n",
    "    target_events = df.copy()\n",
    "    target_events.query(\"time > @ timewindow_start\", inplace=True)\n",
    "    target_events.index.name = \"target_id\"\n",
    "\n",
    "    beta = estimate_beta_tinti(target_events[\"mag\"], mc=mc, delta_m=delta_m)\n",
    "    print(\"  beta of primary catalog is\", beta)\n",
    "\n",
    "    source_columns = [\n",
    "        \"source_magnitude\",\n",
    "        \"source_to_end_time_distance\",\n",
    "        \"pos_source_to_start_time_distance\"\n",
    "    ]\n",
    "\n",
    "    source_events = pd.DataFrame(\n",
    "        distances[\n",
    "            source_columns\n",
    "        ].groupby(\"source_id\").first()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print('  using input initial values for theta\\n')\n",
    "        initial_values = parameter_dict2array(\n",
    "            parameters_dict[\"theta_0\"]\n",
    "        )\n",
    "    except KeyError:\n",
    "        print('  randomly chosing initial values for theta\\n')\n",
    "        initial_values = set_initial_values()\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # perform probability calculation\n",
    "    #################################\n",
    "\n",
    "    parameters = initial_values\n",
    "    print('  Performing expectation step calculation\\n')\n",
    "    Pij, target_events, source_events, n_hat = expectation_step(\n",
    "        distances=distances,\n",
    "        target_events=target_events,\n",
    "        source_events=source_events,\n",
    "        params=[parameters, mc - delta_m / 2],\n",
    "        verbose=True)\n",
    "    \n",
    "    print('      n_hat:', n_hat) # n_hat is the no of expected independent events\n",
    "    \n",
    "    ###########################\n",
    "    # perform the declustering\n",
    "    ###########################\n",
    "    # Compute threshold probability for declustering\n",
    "    print('\\nPERFORMING \"ETAS-Background\" DECLUSTERING \\n')\n",
    "    n_est = round_nearest_int(n_hat)\n",
    "    print('Target number of independent events: {}'.format(n_est))\n",
    "    \n",
    "    p_range = np.flip(np.linspace(0.0, 1.0, 101))\n",
    "    diffs = []\n",
    "    p_thresh = None\n",
    "    \n",
    "    for p_indep in p_range:\n",
    "        #print('\\nTesting {} as threshold probability'.format(p_indep))\n",
    "        n_E_ind = len(target_events.loc[target_events['P_background'] >= p_indep ].index)\n",
    "        #print('No of events above p_thresh={}: {}'.format(p_indep, n_E_ind))\n",
    "        diff = np.abs(n_E_ind - n_est)\n",
    "        diffs.append(diff)\n",
    "        if n_E_ind == n_est:\n",
    "            p_thresh = p_indep\n",
    "            print('Fitting threshold probability is: ', p_thresh)\n",
    "            break\n",
    "            \n",
    "    if p_thresh is None:\n",
    "        p_thresh = p_range[diffs.index(min(diffs))]\n",
    "        print('Fitting threshold probability is: ', p_thresh)\n",
    "        \n",
    "    \n",
    "    declustered_cat = target_events.loc[(target_events['P_background'] >= p_thresh)]\n",
    "    rejected_ev = target_events.loc[target_events['P_background'] < p_thresh]\n",
    "    print('{} earthquakes out of {} classified as background events. \\n {:.1f}% of total'.format(len(declustered_cat.index), \n",
    "                            len(target_events.index), (100*len(declustered_cat.index)/len(target_events.index))))\n",
    "    \n",
    "    # Store results to CSV and metadata to json\n",
    "    if store_results:\n",
    "        target_events.to_csv(fn_ip)\n",
    "        source_events.to_csv(fn_src)\n",
    "        declustered_cat.to_csv(fn_cat)\n",
    "        rejected_ev.to_csv(fn_rejcat)\n",
    "\n",
    "        all_info = {\n",
    "            \"auxiliary_start\": str(auxiliary_start),\n",
    "            \"timewindow_start\": str(timewindow_start),\n",
    "            \"timewindow_end\": str(timewindow_end),\n",
    "            \"timewindow_length\": timewindow_length,\n",
    "            \"mc\": mc,\n",
    "            \"beta\": beta,\n",
    "            \"n_target_events\": len(target_events),\n",
    "            \"delta_m\": delta_m,\n",
    "            \"shape_coords\": str(list(coordinates)),\n",
    "            \"earth_radius\": earth_radius,\n",
    "            \"area\": area,\n",
    "            \"coppersmith_multiplier\": coppersmith_multiplier,\n",
    "            \"fn\": fn_catalog,\n",
    "            \"fn_dist\": fn_dist,\n",
    "            \"fn_ip\": fn_ip,\n",
    "            \"fn_src\": fn_src,\n",
    "            \"calculation_date\": str(dt.datetime.now()),\n",
    "            \"initial_values\": str(parameter_array2dict(initial_values)),\n",
    "        }\n",
    "\n",
    "        info_json = json.dumps(all_info)\n",
    "        f = open(fn_parameters, \"w\")\n",
    "        f.write(info_json)\n",
    "        f.close()\n",
    "\n",
    "    if store_pij:\n",
    "        Pij.to_csv(fn_pij)\n",
    "    \n",
    "    return declustered_cat, rejected_ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### South America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversion for the ETAS params\n",
    "theta_0 = {'log10_mu': -5.8,'log10_k0': -2.6, 'a': 1.8, 'log10_c': -2.5, 'omega': -0.02,\n",
    "        'log10_tau': 3.5, 'log10_d': -0.85,'gamma': 1.3,'rho': 0.66}\n",
    "\n",
    "inversion_meta = {\"fn_catalog\": \"SAM_EQ_data/raw_catalog_data.csv\",\"data_path\": \"SAM_EQ_data/Mizrahi_ETAS_inversion/\",\n",
    "                  \"auxiliary_start\": dt.datetime(1970, 1, 1),\"timewindow_start\": dt.datetime(1980, 1, 1),\n",
    "                  \"timewindow_end\": dt.datetime(2021, 7, 1),\n",
    "        \"theta_0\": theta_0,\"mc\": 4.8,\"delta_m\": 0.1,\"coppersmith_multiplier\": 100,\n",
    "\"shape_coords\": [[-25.0, -66.0], [-25.0, -80.0], [-11.0, -66.0], [-11.0, -80.0],[-25.0, -66.0]],}\n",
    "\n",
    "parameters = invert_etas_params(inversion_meta,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dict of ETAS parameters - based on params recovered by inversion\n",
    "theta_0 = parameters\n",
    "\n",
    "# Define metadata\n",
    "decluster_meta = {\"fn_catalog\": \"SAM_EQ_data/raw_catalog_data.csv\",\"data_path\": \"SAM_EQ_data/Mizrahi_ETAS_decluster/\",\n",
    "                  \"auxiliary_start\": dt.datetime(1970, 1, 1),\"timewindow_start\": dt.datetime(1980, 1, 1),\n",
    "                  \"timewindow_end\": dt.datetime(2021, 7, 1),\n",
    "        \"theta_0\": theta_0,\"mc\": 4.8,\"delta_m\": 0.1,\"coppersmith_multiplier\": 100,\n",
    "\"shape_coords\": [[-47.0, -83.0], [-47.0, -60.0], [8.0, -60.0], [8.0, -83.0],[-47.0, -83.0]],}\n",
    "\n",
    "ETAS_declustered_cat, ETAS_rejected_cat = Mizrahi_ETAS_BG_decluster(decluster_meta,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversion for the ETAS params\n",
    "theta_0 = {'log10_mu': -5.8,'log10_k0': -2.6, 'a': 1.8, 'log10_c': -2.5, 'omega': -0.02,\n",
    "        'log10_tau': 3.5, 'log10_d': -0.85,'gamma': 1.3,'rho': 0.66}\n",
    "\n",
    "inversion_meta = {\"fn_catalog\": \"Japan_EQ_data/raw_catalog_data.csv\",\"data_path\":\"Japan_EQ_data/Mizrahi_ETAS_inversion/\",\n",
    "                  \"auxiliary_start\": dt.datetime(1970, 1, 1),\"timewindow_start\": dt.datetime(1980, 1, 1),\n",
    "                  \"timewindow_end\": dt.datetime(2021, 7, 1),\n",
    "        \"theta_0\": theta_0,\"mc\": 3.0,\"delta_m\": 0.1,\"coppersmith_multiplier\": 100,\n",
    "\"shape_coords\": [[30.0,128.0],[36.0, 128.0],[36.0, 135.0],[38.0, 135.0],[38.0, 136.0],[45.0, 136.0], \n",
    "                 [45.0, 152.0],[38.0, 152.0],[38.0, 147.0],[36.0, 147.0],[36.0, 145.0],[30.0, 145.0],[30.0, 128.0]],}\n",
    "\n",
    "parameters = invert_etas_params(inversion_meta,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dict of ETAS parameters - based on params recovered by inversion\n",
    "theta_0 = parameters\n",
    "\n",
    "# Define metadata\n",
    "decluster_meta = {\"fn_catalog\": \"Japan_EQ_data/raw_catalog_data.csv\",\"data_path\": \"Japan_EQ_data/Mizrahi_ETAS_decluster/\",\n",
    "                  \"auxiliary_start\": dt.datetime(1970, 1, 1),\"timewindow_start\": dt.datetime(1980, 1, 1),\n",
    "                  \"timewindow_end\": dt.datetime(2021, 7, 1),\n",
    "        \"theta_0\": theta_0,\"mc\": 3.0,\"delta_m\": 0.1,\"coppersmith_multiplier\": 100,\n",
    "\"shape_coords\": [[128.0, 30.0],[128.0, 36.0],[135.0, 36.0],[135.0, 38.0],[136.0, 38.0],[136.0, 45.0], \n",
    "                 [152.0, 45.0],[152.0, 38.0],[147.0, 38.0],[147.0, 36.0],[145.0, 36.0],[145.0, 30.0],[128.0, 30.0]],}\n",
    "\n",
    "ETAS_declustered_cat, ETAS_rejected_cat = Mizrahi_ETAS_BG_decluster(decluster_meta,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mizrahi-Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Development of Mizrahi method where events get split into clusters - work in progress...\n",
    "def Mizrahi_ETAS_Main_decluster(\n",
    "        metadata,\n",
    "        timewindow_end=None,\n",
    "        globe=False,\n",
    "        store_pij=False,\n",
    "        store_results=True):\n",
    "    \"\"\"\n",
    "        Declusters an earthquake catalog using the ETAS-based approach of Mizrahi et al. (2021).\n",
    "        metadata can be either a string (path to json file with stored metadata)\n",
    "        or a dict. accepted & necessary keywords are:\n",
    "            fn_catalog: filename of the catalog (absolute path or filename in current directory)\n",
    "                        catalog is expected to be a csv file with the following columns:\n",
    "                        id, latitude, longitude, time, magnitude\n",
    "                        id needs to contain a unique identifier for each event\n",
    "                        time contains datetime of event occurrence\n",
    "                        see synthetic_catalog.csv for an example\n",
    "            data_path: path where result data will be stored\n",
    "            auxiliary_start: start date of the auxiliary catalog (str or datetime).\n",
    "                             events of the auxiliary catalog act as sources, not as targets\n",
    "            timewindow_start: start date of the primary catalog , end date of auxiliary catalog (str or datetime).\n",
    "                             events of the primary catalog act as sources and as targets\n",
    "            timewindow_end: end date of the primary catalog (str or datetime)\n",
    "            mc: cutoff magnitude. catalog needs to be complete above mc\n",
    "            delta_m: size of magnitude bins\n",
    "            coppersmith_multiplier: events further apart from each other than\n",
    "                                    coppersmith subsurface rupture length * this multiplier\n",
    "                                    are considered to be uncorrelated (to reduce size of distance matrix)\n",
    "            shape_coords: coordinates of the boundary of the region to consider\n",
    "                          (list of lists, i.e. [[lat1, lon1], [lat2, lon2], [lat3, lon3]])\n",
    "    \"\"\"\n",
    "    ####################\n",
    "    # preparing metadata\n",
    "    ####################\n",
    "    print(\"PREPARING METADATA...\\n\")\n",
    "\n",
    "    if isinstance(metadata, str):\n",
    "        # if metadata is a filename, read the file (assuming it's json)\n",
    "        with open(metadata, 'r') as f:\n",
    "            parameters_dict = json.load(f)\n",
    "    else:\n",
    "        parameters_dict = metadata\n",
    "\n",
    "    fn_catalog = parameters_dict[\"fn_catalog\"]\n",
    "    print(\"  using catalog: \" + fn_catalog)\n",
    "\n",
    "    data_path = parameters_dict[\"data_path\"]\n",
    "    if data_path == \"\":\n",
    "        print(\"  Data will be stored in \" + os.path.dirname(os.path.abspath(__file__)))\n",
    "    else:\n",
    "        print(\"  Data will be stored in \" + data_path)\n",
    "\n",
    "    auxiliary_start = pd.to_datetime(parameters_dict[\"auxiliary_start\"])\n",
    "    timewindow_start = pd.to_datetime(parameters_dict[\"timewindow_start\"])\n",
    "    timewindow_end = timewindow_end or pd.to_datetime(parameters_dict[\"timewindow_end\"])\n",
    "    print(\n",
    "        \"  Time Window: \" + str(auxiliary_start)\n",
    "        + \" (aux) - \" + str(timewindow_start) + \" (start) - \" + str(timewindow_end) + \" (end)\"\n",
    "    )\n",
    "\n",
    "    mc = parameters_dict[\"mc\"]\n",
    "    delta_m = parameters_dict[\"delta_m\"]\n",
    "    print(\"  mc is \" + str(mc) + \" and delta_m is \" + str(delta_m))\n",
    "\n",
    "    coppersmith_multiplier = parameters_dict[\"coppersmith_multiplier\"]\n",
    "    print(\"  coppersmith multiplier is \" + str(coppersmith_multiplier))\n",
    "\n",
    "    if globe:\n",
    "        coordinates = []\n",
    "    else:\n",
    "        if type(parameters_dict[\"shape_coords\"]) is str:\n",
    "            coordinates = np.array(eval(parameters_dict[\"shape_coords\"]))\n",
    "        else:\n",
    "            coordinates = np.array(parameters_dict[\"shape_coords\"])\n",
    "        pprint.pprint(\"  Coordinates of region: \" + str(list(coordinates)))\n",
    "\n",
    "    # defining some other stuff here..\n",
    "\n",
    "    timewindow_length = to_days(timewindow_end - timewindow_start)\n",
    "\n",
    "    fn_parameters = data_path + 'parameters.json'\n",
    "    fn_ip = data_path + 'ind_and_bg_probs.csv'\n",
    "    fn_src = data_path + 'sources.csv'\n",
    "    fn_dist = data_path + 'distances.csv'\n",
    "    fn_pij = data_path + 'pij.csv'\n",
    "    fn_cat = data_path + 'ETAS_declustered_cat.csv'\n",
    "\n",
    "    # earth radius in km\n",
    "    earth_radius = 6.3781e3\n",
    "\n",
    "    if globe:\n",
    "        area = earth_radius ** 2 * 4 * np.pi\n",
    "    else:\n",
    "        poly = Polygon(coordinates)\n",
    "        area = polygon_surface(poly)\n",
    "    print(\"  Region has \" + str(area) + \" square km\")\n",
    "\n",
    "    # Initialise key data columns\n",
    "    print(\"\\n\\nINITIALIZING\\n\")\n",
    "    print(\"  reading data..\\n\")\n",
    "    df_full = pd.read_csv(fn_catalog, index_col=0, dtype={\"url\": str, \"alert\": str})\n",
    "    df_full[\"time\"] = pd.to_datetime(df_full[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    df_full = df_full.infer_objects()\n",
    "    #Create gdf\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_full, geometry=gpd.points_from_xy(df_full.lat, df_full.lon))\n",
    "\n",
    "    df = df_full.copy()\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" out of \" + str(len(df_full)) + \" events lie within target region.\")\n",
    "\n",
    "    # filter for events above cutoff magnitude - delta_m/2\n",
    "    if delta_m > 0:\n",
    "        df[\"mag\"] = round_half_up(df[\"mag\"] / delta_m) * delta_m\n",
    "    df.query(\"mag>=@mc-@delta_m/2\", inplace=True)\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" events are above cutoff magnitude\")\n",
    "\n",
    "    # filter for eventsin relevant timewindow\n",
    "    df.query(\"time >= @ auxiliary_start and time < @ timewindow_end\", inplace=True)\n",
    "\n",
    "    print(\"  \"+str(len(df)) + \" events are within time window\\n\\n\")\n",
    "\n",
    "    print('  calculating distances..\\n')\n",
    "\n",
    "    distances = prepare_catalog(\n",
    "        df,\n",
    "        mc=mc - delta_m / 2,\n",
    "        coppersmith_multiplier=coppersmith_multiplier,\n",
    "        timewindow_start=timewindow_start,\n",
    "        timewindow_end=timewindow_end,\n",
    "        earth_radius=earth_radius,\n",
    "        delta_m=delta_m\n",
    "    )\n",
    "    # distances.to_csv(fn_dist)\n",
    "\n",
    "    print('  preparing source and target events..\\n')\n",
    "\n",
    "    target_events = df.copy()\n",
    "    target_events.query(\"time > @ timewindow_start\", inplace=True)\n",
    "    target_events.index.name = \"target_id\"\n",
    "\n",
    "    beta = estimate_beta_tinti(target_events[\"mag\"], mc=mc, delta_m=delta_m)\n",
    "    print(\"  beta of primary catalog is\", beta)\n",
    "\n",
    "    source_columns = [\n",
    "        \"source_magnitude\",\n",
    "        \"source_to_end_time_distance\",\n",
    "        \"pos_source_to_start_time_distance\"\n",
    "    ]\n",
    "\n",
    "    source_events = pd.DataFrame(\n",
    "        distances[\n",
    "            source_columns\n",
    "        ].groupby(\"source_id\").first()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print('  using input initial values for theta\\n')\n",
    "        initial_values = parameter_dict2array(\n",
    "            parameters_dict[\"theta_0\"]\n",
    "        )\n",
    "    except KeyError:\n",
    "        print('  randomly chosing initial values for theta\\n')\n",
    "        initial_values = set_initial_values()\n",
    "\n",
    "\n",
    "    #################################\n",
    "    # perform probability calculation\n",
    "    #################################\n",
    "\n",
    "    parameters = initial_values\n",
    "    print('  Performing expectation step calculation\\n')\n",
    "    Pij, target_events, source_events, n_hat = expectation_step(\n",
    "        distances=distances,\n",
    "        target_events=target_events,\n",
    "        source_events=source_events,\n",
    "        params=[parameters, mc - delta_m / 2],\n",
    "        verbose=True)\n",
    "    \n",
    "    print('      n_hat:', n_hat) # n_hat is the no of expected independent events\n",
    "    \n",
    "    ###########################\n",
    "    # perform the declustering\n",
    "    ###########################\n",
    "    # \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Store results to CSV and metadata to json\n",
    "    if store_results:\n",
    "        target_events.to_csv(fn_ip)\n",
    "        source_events.to_csv(fn_src)\n",
    "        declustered_cat.to_csv(fn_cat)\n",
    "\n",
    "        all_info = {\n",
    "            \"auxiliary_start\": str(auxiliary_start),\n",
    "            \"timewindow_start\": str(timewindow_start),\n",
    "            \"timewindow_end\": str(timewindow_end),\n",
    "            \"timewindow_length\": timewindow_length,\n",
    "            \"mc\": mc,\n",
    "            \"beta\": beta,\n",
    "            \"n_target_events\": len(target_events),\n",
    "            \"delta_m\": delta_m,\n",
    "            \"shape_coords\": str(list(coordinates)),\n",
    "            \"earth_radius\": earth_radius,\n",
    "            \"area\": area,\n",
    "            \"coppersmith_multiplier\": coppersmith_multiplier,\n",
    "            \"fn\": fn_catalog,\n",
    "            \"fn_dist\": fn_dist,\n",
    "            \"fn_ip\": fn_ip,\n",
    "            \"fn_src\": fn_src,\n",
    "            \"calculation_date\": str(dt.datetime.now()),\n",
    "            \"initial_values\": str(parameter_array2dict(initial_values)),\n",
    "        }\n",
    "\n",
    "        info_json = json.dumps(all_info)\n",
    "        f = open(fn_parameters, \"w\")\n",
    "        f.write(info_json)\n",
    "        f.close()\n",
    "\n",
    "    if store_pij:\n",
    "        Pij.to_csv(fn_pij)\n",
    "    \n",
    "    return declustered_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaliapin et al. (2008) Declustering\n",
    "\n",
    "Using algorithms from Zaliapin & Ben-Zion (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaliapin declustering - wrapper functions\n",
    "\n",
    "# Prepare catalog:\n",
    "def prep_cat_zaliapin(cat_init):\n",
    "    \"\"\" Loads and prepares a raw catalog for further processing\n",
    "    # input cat_init needs to be a file path to a CSV document containing labelled columns:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    \"\"\"\n",
    "    # Load catalog from file:\n",
    "    cat = pd.read_csv(cat_init, index_col=0)\n",
    "    cat = cat.sort_index()\n",
    "    \n",
    "    # Create datetimes\n",
    "    cat[\"time\"] = pd.to_datetime(cat[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    cat = cat.infer_objects()\n",
    "    cat.loc[:, 'depth_km'] *=0.001\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    cat['lat_rad'] = np.radians(cat['lat'])\n",
    "    cat['lon_rad'] = np.radians(cat['lon'])\n",
    "    return cat\n",
    "\n",
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    \"\"\"Haversine function\n",
    "    Takes in arguments in radians\n",
    "    \"\"\"\n",
    "    return np.square(np.sin(theta / 2))\n",
    "\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    \"\"\"Haversine distance in km - calculate distance between 2 pts on a sphere\n",
    "    lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2 must all be in radians\n",
    "    \"\"\"\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(np.sqrt(hav(lat_rad_1 - lat_rad_2)+ np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Zaliapin algorithm\n",
    "\n",
    "def zaliapin_eta_vals(cat, q, d_f, b):\n",
    "    \"\"\" Function to calculate time-space distances for earthquake catalog declustering\n",
    "        using the algorithms by Zaliapin and Ben-Zion (2013)\n",
    "        Based on MATLAB scripts by Dr Richard Walters\n",
    "        cat_path must be a filepath to a CSV file containing raw data from an ISC catalog search\n",
    "        The following columns are expected:\n",
    "        # Index, year, month, day, hour, minute, second, lat,lon, depth, mag (the depth must be in metres)\n",
    "        q is a constant - most studies assume this to be 0.5\n",
    "        d_f is a (fractal) epicentre dimension - often assumed as 1.6\n",
    "        b is the catalog b-value\n",
    "    \"\"\"\n",
    "    calc_start = dt.datetime.now() # time the function\n",
    "    \n",
    "    copy_catalog = cat.copy(deep=True)\n",
    "\n",
    "    # initialise columns to hold values of the Nearest-Neighbour (NN) time-space distances\n",
    "    cat['delTnn'] = 0.0 \n",
    "    cat['delRnn'] = 0.0\n",
    "    cat['etann'] = 0.0\n",
    "    \n",
    "    print('Now looping over catalog to compute eta values...')\n",
    "    for triggered in cat.itertuples():\n",
    "        # get values of source event\n",
    "        ttime = triggered.time\n",
    "        #print('Triggered event time:', ttime)\n",
    "        tlatrad = triggered.lat_rad\n",
    "        tlonrad = triggered.lon_rad\n",
    "        \n",
    "        # Only consider events before the suspected triggered event (these can be potential parents)\n",
    "        potential_triggers = copy_catalog.loc[copy_catalog[\"time\"] < ttime]\n",
    "        \n",
    "        # Calculate time diffs in yrs\n",
    "        potential_triggers['delt'] = (1./(24.*60.*60.*365.25))*(ttime - potential_triggers['time']).dt.total_seconds()\n",
    "        #potential_triggers['delt'] = (ttime - potential_triggers['time']).dt.total_seconds()\n",
    "        mindelt = potential_triggers.loc[potential_triggers['delt'] > 0].min()\n",
    "        potential_triggers.loc[potential_triggers['delt'] == 0] = mindelt\n",
    "        \n",
    "        # Calculate spatial distances\n",
    "        potential_triggers['delr'] = haversine(tlatrad,potential_triggers['lat_rad'],tlonrad,potential_triggers['lon_rad'])\n",
    "        mindelr = potential_triggers.loc[potential_triggers['delr'] > 0]['delr'].min()\n",
    "        #print(mindelr)\n",
    "        #print(potential_triggers)\n",
    "        potential_triggers.loc[potential_triggers['delr'] == 0] = mindelr\n",
    "        \n",
    "        # Compute scaled time and space distance and eta\n",
    "        potential_triggers['delT'] = potential_triggers['delt'] * np.power(10, (-q*b*potential_triggers['mag']))\n",
    "        potential_triggers['delR'] = (np.power(potential_triggers['delr'], d_f) * \n",
    "                                        np.power(10, (-(1-q)*b*potential_triggers['mag'])))\n",
    "        potential_triggers['eta'] = potential_triggers['delT'] * potential_triggers['delR']\n",
    "        potential_triggers.loc[potential_triggers['eta'] == 0] = 1e9\n",
    "        \n",
    "        #print(potential_triggers)\n",
    "        # Now pick the NND T-R distance for the suspected triggered event and its index\n",
    "        # And assign values to the suspected child in the master catalog\n",
    "        cat.loc[triggered.Index, 'etann'] = potential_triggers['eta'].min()\n",
    "        try:\n",
    "            idx = potential_triggers['eta'].idxmin()\n",
    "            cat.loc[triggered.Index, 'delTnn'] = potential_triggers.loc[idx, 'delT']\n",
    "            cat.loc[triggered.Index, 'delRnn'] = potential_triggers.loc[idx, 'delR']\n",
    "        except:\n",
    "            cat.loc[triggered.Index, 'delTnn'] = 1.0\n",
    "            cat.loc[triggered.Index, 'delRnn'] = 1.0\n",
    "            cat.loc[triggered.Index, 'etann'] = 1.0\n",
    "    \n",
    "    print('    took', (dt.datetime.now() - calc_start), 'for calculating eta values \\n')\n",
    "    return cat # the catalog with eta values\n",
    "\n",
    "def Gaussian_mixture_zaliapin(cat):\n",
    "    \"\"\" Function to calculate declustering threshold eta_0 for the Zaliapin and Ben-Zion (2013) method - we find the \n",
    "        eta value on a histogram where the probability of an event being a background and being a triggered event is\n",
    "        equal, based on the pt of intersection of 2 fitted Gaussians\n",
    "        # Cat must be an earthquake catalog as a df containing an 'etann' column for values of time-space distance for each event\n",
    "    \"\"\"\n",
    "    x = np.log10(cat['etann'])\n",
    "    f = np.ravel(x).astype(np.float)\n",
    "    f=f.reshape(-1,1)\n",
    "    \n",
    "    # fit a Gaussian mixture Model with 2 components: background and mixed\n",
    "    g = mixture.GaussianMixture(n_components=2,covariance_type='full') \n",
    "    g.fit(f)\n",
    "    \n",
    "    # Get the weights, means, and covariances for each of the 2 fitted Gaussians\n",
    "    weights = g.weights_\n",
    "    means = g.means_\n",
    "    covars = g.covariances_\n",
    "    #print(weights, means, covars)\n",
    "    \n",
    "    # Set up figure for plotting\n",
    "    fig, ax = plt.subplots(figsize =(10, 5))\n",
    "    \n",
    "    # Compute a high res histogram from the eta data\n",
    "    n, bins = np.histogram(np.log10(cat['etann']), 200, density=True)\n",
    "    binscenters = np.array([0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)])\n",
    "    \n",
    "    f_axis = f.copy().ravel()\n",
    "    f_axis.sort()\n",
    "    # Plot Gaussian for the clustered mode\n",
    "    ax.plot(f_axis,weights[1]*ss.norm.pdf(f_axis,means[1],np.sqrt(covars[1])).ravel(), c='red', label='Clustered')\n",
    "    # Plot Gaussian for the background mode\n",
    "    ax.plot(f_axis,weights[0]*ss.norm.pdf(f_axis,means[0],np.sqrt(covars[0])).ravel(), c='blue', label='Background')\n",
    "    # Plot the histogram for comparison\n",
    "    ax.plot(binscenters, n, 'k-')\n",
    "    ax.set_xlabel('$log_{10} \\eta$')\n",
    "    ax.set_ylabel('density')\n",
    "    \n",
    "    ax.grid()\n",
    "    \n",
    "    \n",
    "    y1 = weights[1]*ss.norm.pdf(f_axis,means[1],np.sqrt(covars[1])).ravel() # the clustered mode\n",
    "    y2 = weights[0]*ss.norm.pdf(f_axis,means[0],np.sqrt(covars[0])).ravel() # the background mode\n",
    "    x = f_axis\n",
    "    # Find indexes of points where the 2 Gaussians intercept\n",
    "    idxs=np.argwhere(np.diff(np.sign(y1 - y2))).flatten()\n",
    "    # Display the intersections\n",
    "    #plt.figure(figsize=[2.5,2.5])\n",
    "    #ax=plt.subplot()\n",
    "    #ax.plot(x,y1,color='r',label='Clustered',alpha=0.5)\n",
    "    #ax.plot(x,y2,color='b',label='Background',alpha=0.5)\n",
    "    _=[ax.axvline(x[i],color='k') for i in idxs]\n",
    "    _=[ax.text(x[i],ax.get_ylim()[1],f\"{x[i]:1.2f}\",ha='center',va='bottom') for i in idxs]\n",
    "    #ax.legend(bbox_to_anchor=[1,1])\n",
    "    #ax.set(xlabel='x',ylabel='density')\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "    # pick out the first intersection and compute threshold:\n",
    "    etathresh = np.power(10, f_axis[idxs[0]])\n",
    "    print('Threshold eta for declustering:', etathresh)\n",
    "    return etathresh\n",
    "    \n",
    "# Optional histogram visualisation    \n",
    "def zaliapin_histogram(cat):\n",
    "    \"\"\" Plot a histogram for Zaliapin-type declustering - OPTIONAL\n",
    "        Requires a catalog that contains eta, delTnn, delRnn columns\n",
    "    \"\"\"\n",
    "    # Creating bins\n",
    "    delTnn_bins = np.linspace(np.log10(cat['delTnn'].min()), np.log10(cat['delTnn'].max()), 50)\n",
    "    delRnn_bins = np.linspace(np.log10(cat['delRnn'].min()), np.log10(cat['delRnn'].max()), 50)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize =(20, 10))\n",
    "    # Creating plot\n",
    "    im = ax1.hist2d(np.log10(cat['delTnn']), np.log10(cat['delRnn']), bins =[delTnn_bins, delRnn_bins], \n",
    "               cmap = 'viridis')\n",
    "    ax1.set_title(\"Spatio-temporal scaled distances\", fontsize=16)\n",
    "\n",
    "    # Adding color bar\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.6, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    fig.colorbar(im[3], cax=cax, orientation='horizontal')\n",
    "\n",
    "    xmin, xmax = ax1.get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "    # Plot the cutoff lines\n",
    "    #ax1.plot(x, (-1*x+ np.log(etathresh)), 'r', label='Walters cutoff')\n",
    "\n",
    "    #for eta_thresh, color in zip([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7], ['r', 'g', 'y', 'o', 'w', 'm', 'c']):\n",
    "        #ax1.plot(x, (-1*x+ np.log(eta_thresh)), color, label=eta_thresh)\n",
    "\n",
    "    ax1.legend(loc='best')\n",
    "\n",
    "    ax1.set_xlabel('$log_{10}(T_{nn})$', fontsize=14) \n",
    "    ax1.set_ylabel('$log_{10}(R_{nn})$', fontsize=14)\n",
    "\n",
    "    # the histogram of the data\n",
    "    n, bins, patches = ax2.hist(np.log10(cat['etann']), 100, density=True, facecolor='b', alpha=0.75)\n",
    "    ax2.set_xlabel('$ln(\\eta)$', fontsize=14)\n",
    "    ax2.set_ylabel('Frequency', fontsize=14)\n",
    "    ax2.set_title('Histogram of $\\eta_{nn}$', fontsize=16)\n",
    "\n",
    "    # show plot\n",
    "    plt.show()\n",
    "    fig.savefig('zaliapin_histogram.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Zaliapin declustering routine\n",
    "def zaliapin_decluster(params_dict, impose_mc_b=True, save_meta=True):\n",
    "    \"\"\" Function to decluster an earthquake catalog using the algorithms by Zaliapin and Ben-Zion (2013)\n",
    "        Based on MATLAB scripts by Dr Richard Walters\n",
    "        Takes in dict with the following required keywords:\n",
    "        fn_catalog must be a filepath to a CSV file containing raw data from an ISC catalog search\n",
    "        The following columns are expected:\n",
    "        # Index, year, month, day, hour, minute, second, lat,lon, depth, mag (the depth must be in metres)\n",
    "        timewindow_start is a datetime object marking the start date past which events are kept for the final catalog\n",
    "        # entire catalog is used for declustering but we throw away the first few years\n",
    "        q is a constant - most studies assume this to be 0.5\n",
    "        d_f is a (fractal) epicentre dimension - often assumed as 1.6\n",
    "        outpath is a filepath to the directory where output results are saved\n",
    "        Options:\n",
    "        impose_mc_b: If True, impose a chosen Mc and a b-value of 1.0 (following Hicks (2011)). If False, Mc and b-value\n",
    "                     are calculated using the Mizrahi algorithm\n",
    "        save_meta = If True, information about the declustering operation is saved in a json file\n",
    "    \"\"\"\n",
    "    calc_start = dt.datetime.now() # time the function\n",
    "    \n",
    "    # Initialise function params\n",
    "    fn_catalog = params_dict[\"fn_catalog\"]\n",
    "    timewindow_start = params_dict[\"timewindow_start\"]\n",
    "    q = params_dict[\"q\"]\n",
    "    d_f = params_dict[\"d_f\"]\n",
    "    outpath = params_dict[\"outpath\"]\n",
    "    \n",
    "    # Output filenames\n",
    "    output_cat_fn = outpath + 'Zaliapin_decluster/declustered_catalog_data.csv'\n",
    "    output_rej_fn = outpath + 'Zaliapin_decluster/rejected_ev.csv'\n",
    "    output_calc_fn = outpath + 'Zaliapin_decluster/zaliapin_output_data.csv'\n",
    "    param_fn = outpath + 'parameters.json'\n",
    "    \n",
    "    # Preprocess catalog to convert lat,lon to radians and create datetimes\n",
    "    print('Preprocessing catalog...')\n",
    "    cat_zaliapin = prep_cat_zaliapin(fn_catalog)\n",
    "    print('Done. {} events read in.'.format(len(cat_zaliapin.index)))\n",
    "    \n",
    "    if impose_mc_b:\n",
    "        mc_winner = params_dict[\"mc\"]\n",
    "        b_value_winner = 1.0 # Following Hicks (2011)\n",
    "    else:\n",
    "        # Extract magnitudes for b-value calculation\n",
    "        print('\\nb-value estimation')\n",
    "        magnitude_sample = cat_zaliapin['mag'].values\n",
    "        mcs = round_half_up(np.arange(3.0, 5.5, 0.1), 1)\n",
    "        # Estimate the catalog b-value using the Mizrahi algorithm\n",
    "        mcs_tested, ks_distances, p_values, mc_winner, b_value_winner = estimate_mc(magnitude_sample,mcs,delta_m=0.1,\n",
    "            p_pass=0.04, stop_when_passed=False,verbose=True,n_samples=1000)\n",
    "    \n",
    "    # Calculate the eta values\n",
    "    print('\\nCalculating eta values')\n",
    "    delta_m=0.1\n",
    "    m_cut = mc_winner-delta_m/2\n",
    "    cat_zaliapin = cat_zaliapin.loc[cat_zaliapin['mag'] >= m_cut] # perform calc on events above Mc only\n",
    "    cat_etavals = zaliapin_eta_vals(cat_zaliapin, q, d_f, b_value_winner)\n",
    "    cat_etavals.to_csv(output_calc_fn)\n",
    "    print('Done')\n",
    "    \n",
    "    # Estimate Gaussian mixture model parameters and calculate declustering threshold:\n",
    "    eta_thr = Gaussian_mixture_zaliapin(cat_etavals)\n",
    "    \n",
    "    # Decluster the catalog\n",
    "    # Also throw away some datatime for background rates to reach true values\n",
    "    cat_etavals = cat_etavals.loc[cat_etavals[\"time\"] >= timewindow_start]\n",
    "    declustered_cat = cat_etavals.loc[(cat_etavals['etann'] >= eta_thr)]\n",
    "    rejected_ev = cat_etavals.loc[(cat_etavals['etann'] < eta_thr)]\n",
    "    declustered_cat.to_csv(output_cat_fn)\n",
    "    rejected_ev.to_csv(output_rej_fn)\n",
    "    print('Background earthquakes retained: {} out of {} - {:.1f}% of total'.format(len(declustered_cat.index), \n",
    "                                        len(cat_etavals.index), (100*len(declustered_cat.index)/len(cat_etavals.index))))\n",
    "    print('    took', (dt.datetime.now() - calc_start), 'for declustering catalog \\n')\n",
    "    \n",
    "    if save_meta:\n",
    "        all_info = {\n",
    "            \"region\": fn_catalog.split(sep='/')[0].split(sep='_')[0],\n",
    "            \"auxiliary_start\": str(auxiliary_start),\n",
    "            \"timewindow_start\": str(timewindow_start),\n",
    "            \"timewindow_end\": str(timewindow_end),\n",
    "            \"timewindow_length\": timewindow_length,\n",
    "            \"mc\": mc_winner,\n",
    "            \"beta\": b_value_winner,\n",
    "            \"q\": q,\n",
    "            \"d_f\": d_f,\n",
    "            \"total_events\": len(cat_etavals.index),\n",
    "            \"n_background_ev\": len(declustered_cat.index),\n",
    "            \"n_rejected_ev\": len(rejected_ev.index),\n",
    "            \"eta_thr\": eta_thr\n",
    "            \"delta_m\": delta_m,\n",
    "            \"fn\": fn_catalog,\n",
    "            \"calculation_date\": str(dt.datetime.now()),\n",
    "        }\n",
    "\n",
    "        info_json = json.dumps(all_info)\n",
    "        f = open(fn_parameters, \"w\")\n",
    "        f.write(info_json)\n",
    "        f.close()\n",
    "    return declustered_cat, rejected_ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### South America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decluster_dict_SAM = {\"fn_catalog\": \"SAM_EQ_data/raw_catalog_data.csv\",\"outpath\": \"SAM_EQ_data/\",\n",
    "                  \"auxiliary_start\": dt.datetime(1970, 1, 1),\"timewindow_start\": dt.datetime(1980, 1, 1),\n",
    "                  \"timewindow_end\": dt.datetime(2021, 7, 1), \"q\": 0.5,\"d_f\": 1.6, \"mc\": 4.5}\n",
    "\n",
    "SAM_Zdeclustered, SAM_Zrejected = zaliapin_decluster(decluster_dict_SAM,impose_mc_b=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decluster_dict_JAP = {\"fn_catalog\": \"Japan_EQ_data/raw_catalog_data.csv\",\"outpath\": \"Japan_EQ_data/\",\n",
    "                  \"auxiliary_start\": dt.datetime(1970, 1, 1),\"timewindow_start\": dt.datetime(1980, 1, 1),\n",
    "                  \"timewindow_end\": dt.datetime(2021, 7, 1), \"q\": 0.5,\"d_f\": 1.6, \"mc\": 3.0}\n",
    "\n",
    "JAP_Zdeclustered, JAP_Zrejected = zaliapin_decluster(decluster_dict_JAP,impose_mc_b=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Zaliapin routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option for a histogram - like Z-BZ 2013\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# fit an array of size [Ndim, Nsamples]\n",
    "data = np.vstack([x, y])\n",
    "kde = gaussian_kde(data)\n",
    "\n",
    "# evaluate on a regular grid\n",
    "xgrid = np.linspace(-3.5, 3.5, 40)\n",
    "ygrid = np.linspace(-6, 6, 40)\n",
    "Xgrid, Ygrid = np.meshgrid(xgrid, ygrid)\n",
    "Z = kde.evaluate(np.vstack([Xgrid.ravel(), Ygrid.ravel()]))\n",
    "\n",
    "# Plot the result as an image\n",
    "plt.imshow(Z.reshape(Xgrid.shape),\n",
    "           origin='lower', aspect='auto',\n",
    "           extent=[-3.5, 3.5, -6, 6],\n",
    "           cmap='Blues')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3D Zaliapin (TESTED - is FULLY FUNCTIONAL)\n",
    "def zaliapin_eta_vals3D(cat, q, d_f, b):\n",
    "    \"\"\" Function to calculate time-space (3D) distances for earthquake catalog declustering\n",
    "        extending the algorithms by Zaliapin and Ben-Zion (2013)\n",
    "        cat_path must be a filepath to a CSV file containing raw data from an ISC catalog search\n",
    "        The following columns are expected:\n",
    "        # Index, year, month, day, hour, minute, second, lat,lon, depth, mag (the depth must be in metres)\n",
    "        q is a constant - most studies assume this to be 0.5\n",
    "        d_f is a (fractal) epicentre dimension - often assumed as 1.6\n",
    "        b is the catalog b-value\n",
    "    \"\"\"\n",
    "    calc_start = dt.datetime.now() # time the function\n",
    "    \n",
    "    copy_catalog = cat.copy(deep=True)\n",
    "\n",
    "    # initialise columns to hold values of the Nearest-Neighbour (NN) time-space distances\n",
    "    cat['delTnn'] = 0.0 \n",
    "    cat['delRnn'] = 0.0\n",
    "    cat['etann'] = 0.0\n",
    "    \n",
    "    print('Now looping over catalog to compute eta values...')\n",
    "    for triggered in cat.itertuples():\n",
    "        # get values of source event\n",
    "        ttime = triggered.time\n",
    "        #print('Triggered event time:', ttime)\n",
    "        tlatrad = triggered.lat_rad\n",
    "        tlonrad = triggered.lon_rad\n",
    "        tdepth = triggered.depth_km\n",
    "        # Only consider events before the suspected triggered event (these can be potential parents)\n",
    "        potential_triggers = copy_catalog.loc[copy_catalog[\"time\"] < ttime]\n",
    "        \n",
    "        # Calculate time diffs in yrs\n",
    "        potential_triggers['delt'] = (1./(24.*60.*60.*365.25))*(ttime - potential_triggers['time']).dt.total_seconds()\n",
    "        #potential_triggers['delt'] = (ttime - potential_triggers['datetime']).dt.total_seconds()\n",
    "        mindelt = potential_triggers.loc[potential_triggers['delt'] > 0].min()\n",
    "        potential_triggers.loc[potential_triggers['delt'] == 0] = mindelt\n",
    "        \n",
    "        # Calculate spatial distances in 3D\n",
    "        potential_triggers['delr'] = np.sqrt(np.square(haversine(tlatrad,potential_triggers['lat_rad'],\n",
    "                                                         tlonrad,potential_triggers['lon_rad']))\n",
    "                                             + np.square(tdepth - potential_triggers['depth_km']))\n",
    "        mindelr = potential_triggers.loc[potential_triggers['delr'] > 0].min()\n",
    "        potential_triggers.loc[potential_triggers['delr'] == 0] = mindelr\n",
    "        \n",
    "        # Compute scaled time and space distance and eta\n",
    "        potential_triggers['delT'] = potential_triggers['delt'] * np.power(10, (-q*b*potential_triggers['mag']))\n",
    "        potential_triggers['delR'] = (np.power(potential_triggers['delr'], d_f) * \n",
    "                                        np.power(10, (-(1-q)*b*potential_triggers['mag'])))\n",
    "        potential_triggers['eta'] = potential_triggers['delT'] * potential_triggers['delR']\n",
    "        potential_triggers.loc[potential_triggers['eta'] == 0] = 1e9\n",
    "        \n",
    "        #print(potential_triggers)\n",
    "        # Now pick the NND T-R distance for the suspected triggered event and its index\n",
    "        # And assign values to the suspected child in the master catalog\n",
    "        cat.loc[triggered.Index, 'etann'] = potential_triggers['eta'].min()\n",
    "        try:\n",
    "            idx = potential_triggers['eta'].idxmin()\n",
    "            cat.loc[triggered.Index, 'delTnn'] = potential_triggers.loc[idx, 'delT']\n",
    "            cat.loc[triggered.Index, 'delRnn'] = potential_triggers.loc[idx, 'delR']\n",
    "        except:\n",
    "            cat.loc[triggered.Index, 'delTnn'] = 1.0\n",
    "            cat.loc[triggered.Index, 'delRnn'] = 1.0\n",
    "            cat.loc[triggered.Index, 'etann'] = 1.0\n",
    "    \n",
    "    print('    took', (dt.datetime.now() - calc_start), 'for calculating eta values \\n')\n",
    "    return cat # the catalog with eta values\n",
    "\n",
    "\n",
    "## 3D version of the declustering routine\n",
    "def zaliapin_decluster3D(cat_path, q, d_f, output):\n",
    "    \"\"\" Function to decluster an earthquake catalog based on 3D version of algorithms by Zaliapin and Ben-Zion (2013)\n",
    "        Based on MATLAB scripts by Dr Richard Walters\n",
    "        cat_path must be a filepath to a CSV file containing raw data from an ISC catalog search\n",
    "        The following columns are expected:\n",
    "        # Index, year, month, day, hour, minute, second, lat,lon, depth, mag (the depth must be in metres)\n",
    "        q is a constant - most studies assume this to be 0.5\n",
    "        d_f is a (fractal) epicentre dimension - often assumed as 1.6\n",
    "        output must be a str containing the filename for the declustered catalog output CSV file\n",
    "    \"\"\"\n",
    "    calc_start = datetime.now() # time the function\n",
    "\n",
    "    # Preprocess catalog to convert lat,lon to radians and create datetimes\n",
    "    print('Preprocessing catalog...')\n",
    "    cat_zaliapin = prep_cat_zaliapin(cat_path)\n",
    "    print('Done.')\n",
    "    \n",
    "    # Extract magnitudes for b-value calculation\n",
    "    print('\\nb-value estimation')\n",
    "    magnitude_sample = cat_zaliapin['mag'].values\n",
    "    mcs = round_half_up(np.arange(3.0, 5.5, 0.1), 1)\n",
    "    # Estimate the catalog b-value using the Mizrahi algorithm\n",
    "    mcs_tested, ks_distances, p_values, mc_winner, b_value_winner = estimate_mc(magnitude_sample,mcs,delta_m=0.1,p_pass=0.05,\n",
    "            stop_when_passed=False,verbose=True,n_samples=1000)\n",
    "    \n",
    "    # Calculate the eta values\n",
    "    print('\\nCalculating eta values')\n",
    "    cat_etavals = zaliapin_eta_vals3D(cat_zaliapin, q, d_f, b_value_winner)\n",
    "    cat_etavals.to_csv(output)\n",
    "    print('Done')\n",
    "    \n",
    "    # Estimate Gaussian mixture model parameters and calculate declustering threshold:\n",
    "    eta_thr = Gaussian_mixture_zaliapin(cat_etavals)\n",
    "    \n",
    "    # Decluster the catalog\n",
    "    declustered_cat = cat_etavals.loc[cat_etavals['etann'] > eta_thr]\n",
    "    rejected_ev_cat = cat_etavals.loc[cat_etavals['etann'] < eta_thr]\n",
    "    declustered_cat.to_csv(output)\n",
    "    print('Background earthquakes retained: {:.1f}% of total'.format(100*len(declustered_cat.index)/len(cat_zaliapin.index)))\n",
    "    print('    took', (dt.datetime.now() - calc_start), 'for declustering catalog \\n')\n",
    "    return declustered_cat, rejected_ev_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_zaliapin_declustered3D, cat_rejected_ev3D = zaliapin_decluster3D('Jara_raw_catalog_data.csv', 0.5, 1.6, 'Jara_3Ddeclustered_catalog_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    " - Hicks, A.L. (2011). Clustering in Multidimensional Spaces with Applications to Statistical Analysis of Earthquake Clustering. MS Thesis, University of Nevada, Reno.  <br>\n",
    " - Jara, J., Socquet, A., Marsan, D., Bouchon, M., 2017. Long-Term Interactions Between Intermediate Depth and Shallow Seismicity in North Chile Subduction Zone. *Geophysical Research Letters 44*, 9283–9292. https://doi.org/10.1002/2017GL075029 <br>\n",
    " - Marsan, D., Bouchon, M., Gardonio, B., Perfettini, H., Socquet, A., Enescu, B., 2017. Change in seismicity along the Japan trench, 1990–2011, and its relationship with seismic coupling. *Journal of Geophysical Research: Solid Earth 122*, 4645–4659. https://doi.org/10.1002/2016JB013715 <br>\n",
    " - Marsan, D., Reverso, T., Helmstetter, A., Enescu, B., 2013. Slow slip and aseismic deformation episodes associated with the subducting Pacific plate offshore Japan, revealed by changes in seismicity. *Journal of Geophysical Research: Solid Earth 118*, 4900–4909. https://doi.org/10.1002/jgrb.50323 <br>\n",
    " - Mizrahi, L., Nandan, S., Wiemer, S., 2021. The Effect of Declustering on the Size Distribution of Mainshocks. *Seismological Research Letters*. https://doi.org/10.1785/0220200231 <br>\n",
    " - Zaliapin, I., Ben-Zion, Y., 2016. A global classification and characterization of earthquake clusters. *Geophysical Journal International 207*, 608–634. https://doi.org/10.1093/gji/ggw300 <br>\n",
    " - Zaliapin, I., Ben-Zion, Y., 2013. Earthquake clusters in southern California I: Identification and stability. *Journal of Geophysical Research: Solid Earth 118*, 2847–2864. https://doi.org/10.1002/jgrb.50179 <br>\n",
    " - Zaliapin, I., Gabrielov, A., Keilis-Borok, V., Wong, H., 2008. Clustering Analysis of Seismicity and Aftershock Identification. *Phys. Rev. Lett. 101*, 018501. https://doi.org/10.1103/PhysRevLett.101.018501 <br>\n",
    " - Zhuang, J., Ogata, Y., Vere-Jones, D., 2004. Analyzing earthquake clustering features by using stochastic reconstruction. *Journal of Geophysical Research: Solid Earth 109*. https://doi.org/10.1029/2003JB002879 <br>\n",
    " - Zhuang, J., Ogata, Y., Vere-Jones, D., 2002. Stochastic Declustering of Space-Time Earthquake Occurrences. *Journal of the American Statistical Association 97*, 369–380. https://doi.org/10.1198/016214502760046925 <br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py-test",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
