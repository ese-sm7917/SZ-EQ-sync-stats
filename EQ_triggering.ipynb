{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package imports ##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from sklearn import mixture\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "#Check shapely speedups are enabled\n",
    "from shapely import speedups\n",
    "speedups.enabled\n",
    "\n",
    "#Set geopandas settings\n",
    "#gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "#gpd.io.file.fiona.drvsupport.supported_drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Ingest data from the International Seismological Centre (ISC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISC web search params\n",
    "start_time = datetime(1970,1,1)\n",
    "end_time = datetime(2021, 6, 21)\n",
    "min_latitude = -25\n",
    "max_latitude = -11\n",
    "min_longitude = -80\n",
    "max_longitude = -66\n",
    "min_mag = 4.5\n",
    "max_mag = None\n",
    "min_depth = None\n",
    "max_depth = None\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ISC Catalog stepwise search ###\n",
    "# ObsPy plugin breaks when searching for too many events, so we perform search in steps of 1 year\n",
    "\n",
    "# Start search\n",
    "t1 = start_time\n",
    "t2 = t1 + relativedelta(years=1)\n",
    "#print('Processing:', t1, t2)\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "# Initialise catalog\n",
    "cat_init = client.get_events(starttime=t1,endtime=t2,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "\n",
    "# Set up loop for stepwise search\n",
    "t1=t2\n",
    "t2+=relativedelta(years=1)\n",
    "#print('Beginning loop', t1, t2)\n",
    "cat = cat_init\n",
    "while t2 < end_time:\n",
    "    try:\n",
    "        #print('Loop Processing', t1, t2)\n",
    "        catalogue = client.get_events(starttime=t1,endtime=t2,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "        cat=cat.__add__(catalogue)\n",
    "        t1=t2\n",
    "        t2+=relativedelta(years=1)\n",
    "    except:\n",
    "        import sys\n",
    "        print(\"Oops!\", sys.exc_info()[0], \"occurred for \", t1, \" - \", t2)\n",
    "        print('FDSN Web Search failure - finalising catalog...')\n",
    "        final_cat = cat\n",
    "        break\n",
    "    \n",
    "# Add final time step and add to main catalog    \n",
    "assert t1 < end_time    \n",
    "try:\n",
    "    cat1 = client.get_events(starttime=t1,endtime=end_time,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "    final_cat = cat.__add__(cat1)\n",
    "    print('Final cat', final_cat)\n",
    "except:\n",
    "    import sys\n",
    "    print(\"Reminder:\", sys.exc_info()[0], \"occurred.\")\n",
    "    print('FDSN Web Search failure - catalog now finalised.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_cat.__str__(print_all=True))\n",
    "print(final_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Small test catalog ###\n",
    "client = Client(\"IRIS\")\n",
    "cat = client.get_events(starttime=UTCDateTime(\"2008-01-01\"),endtime=UTCDateTime(\"2013-01-01\"),\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cat.plot(projection=\"local\", label=None, method=\"cartopy\", title=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE CATALOG DATAFRAME ##\n",
    "\n",
    "# Create empty lists\n",
    "year = []\n",
    "month = []\n",
    "day = []\n",
    "hour = []\n",
    "minute = []\n",
    "second = []\n",
    "lat = []\n",
    "lon = []\n",
    "dep = []\n",
    "mag = []\n",
    "time = []\n",
    "\n",
    "# Loop over each event in the catalogue\n",
    "for event in final_cat: \n",
    "    year.append(event.origins[0].time.year)\n",
    "    month.append(event.origins[0].time.month)\n",
    "    day.append(event.origins[0].time.day)\n",
    "    hour.append(event.origins[0].time.hour)\n",
    "    minute.append(event.origins[0].time.minute)\n",
    "    second.append(event.origins[0].time.second)\n",
    "    lat.append(event.origins[0].latitude)\n",
    "    lon.append(event.origins[0].longitude)\n",
    "    dep.append(event.origins[0].depth)\n",
    "    mag.append(event.magnitudes[0].mag)\n",
    "\n",
    "# Create the dataframe\n",
    "data = pd.DataFrame(np.array([year, month, day, hour, minute, second, lat, lon, dep, mag]).T, \n",
    "             columns=[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\",\n",
    "                      \"lat\", \"lon\", \"depth_km\", \"mag\"])\n",
    "\n",
    "# Save raw data to csv\n",
    "data.to_csv('Jara_raw_catalog_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(catalog[\"mag\"],log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epidemic Type Aftershock Sequence (ETAS) Declustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAS Implementation by Marsan et al. (2017)\n",
    "\n",
    "Omori Law parameters are fixed as follows:\n",
    "$$ \\alpha = 2, p = 1, c = 10^{-3} days, \\gamma = 2 $$ <br>\n",
    "\n",
    "Algorithm can be described as follows: <br>\n",
    "1. Compute triggering rate from catalog as $\\nu(x_i,y_i,t_i)/K $ <br>\n",
    "2. Compute background rate ($ \\mu(x_i, y_i, t_i) $), under the assumption of $\\omega_i = 0.5$\n",
    "3. Compute initial total rate as $ \\lambda(x_i,y_i,t_i) = \\mu(x_i, y_i, t_i) + \\nu(x_i, y_i, t_i) $\n",
    "4. Compute $\\omega_i = \\frac{\\mu(x_i, y_i, t_i)}{\\lambda(x_i,y_i,t_i)}$\n",
    "5. Use $\\omega_i $ to compute ML estimate of K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations\n",
    "$$ \\lambda(x,y,t) = \\mu(x,y,t) + \\nu(x,y,t) $$ <br>\n",
    "where $\\lambda(x,y,t)$ is the total seismicity rate, $\\mu(x,y,t)$ is the background seismicity rate, and $\\nu(x,y,t)$ is the triggering rate <br>\n",
    "\n",
    "The triggering rate: <br>\n",
    "$$ \\nu(x,y,t) = \\displaystyle\\sum_{i/t_i < t}^{} \\frac{Ke^{\\alpha m_i}}{(t+c-t_i)} \\times \\frac{\\gamma - 1}{2\\pi} \\times \\frac{L_i^{\\gamma-1}}{\\left((x-x_i)^2 + (y-y_i)^2 + L_i^2 \\right )^{\\frac{\\gamma + 1}{2}}} $$ <br>\n",
    "\n",
    "The background rate: <br>\n",
    "$$ \\mu(x,y,t) = \\displaystyle\\sum_{i}^{} \\omega_i e^{-\\sqrt{(x-x_i)^2 + (y-y_i)^2}/\\ell} e^{-|t-t_i|/\\tau} \\times \\frac{1}{2 \\pi \\ell^2 a_i} $$ <br>\n",
    "\n",
    "$$ a_i = 2\\tau - \\tau \\left( e^{-\\frac{t_s - t_i}{\\tau}} - e^{\\frac{t_s - t_i}{\\tau}} \\right) $$ <br>\n",
    "where $t_s, t_e$ are the start and end times of the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the catalog for processing\n",
    "def prep_catalog(cat_init, cat_start, cat_end): \n",
    "    \"\"\" Loads and prepares the catalog for further processing\n",
    "    # input cat_init needs to be a file path to a CSV document containing labelled columns:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    \"\"\"\n",
    "    # Load catalog from file:\n",
    "    cat = pd.read_csv(cat_init, index_col=0)\n",
    "    \n",
    "    # Apply datetimes\n",
    "    cat[\"datetime\"] = pd.to_datetime(cat[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    cat = cat.infer_objects()\n",
    "    cat.dtypes\n",
    "    cat.loc[:, 'depth_km'] *=0.001\n",
    "\n",
    "    # Define a geodataframe using the EQ catalog from above\n",
    "    cat_gdf = gpd.GeoDataFrame(cat, geometry=gpd.points_from_xy(cat.lon, cat.lat))\n",
    "    cat_gdf = cat_gdf.set_crs(\"EPSG:4326\")\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    cat_gdf['lat_rad'] = np.radians(cat_gdf['lat'])\n",
    "    cat_gdf['lon_rad'] = np.radians(cat_gdf['lon'])\n",
    "    # Compute the time difference between event occurrence times and the start and end times of the catalog, in days\n",
    "    cat_gdf['t_diff_e'] = (1./(24.*60.*60.))*((cat_gdf['datetime']- cat_end).dt.total_seconds())\n",
    "    cat_gdf['t_diff_s'] = (1./(24.*60.*60.))*((cat_gdf['datetime'] - cat_start).dt.total_seconds())\n",
    "    return cat_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    \"\"\"Haversine function\n",
    "    Takes in arguments in radians\n",
    "    \"\"\"\n",
    "    return np.square(np.sin(theta / 2))\n",
    "\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    #Haversine distance in km - calculate distance between 2 pts on a sphere\n",
    "    # lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2 must all be in radians\n",
    "    ####################################################################\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(\n",
    "        np.sqrt(\n",
    "            hav(lat_rad_1 - lat_rad_2)\n",
    "            + np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic length/rupture radius in km\n",
    "def L_i(m, L_0):\n",
    "    return L_0*np.power(10, 0.5*(m-2))\n",
    "\n",
    "# a coefficients\n",
    "# Need to fix abs values in exponents\n",
    "def a_coeff(t_diff_e,t_diff_s,tau): # tau is the temporal smoothing param; t_diff_e=t_catend-tevent; t_diff_s=t_catstart-tevent\n",
    "    a = 2*tau - tau*(np.exp(-(np.abs(t_diff_s)/(tau))) - np.exp(-(np.abs(t_diff_e)/(tau)))) # times in days\n",
    "    return a\n",
    "\n",
    "# Calculate triggering rate\n",
    "def nu_calc(c, alpha, p, gamma, K, m, time_diffs, r_sq, L_0):\n",
    "    # Numerical calculations for nu\n",
    "    T1 = K*np.exp(alpha*m)/np.power((time_diffs), p)\n",
    "    #T1 = np.exp(alpha*m)\n",
    "    T2 = (gamma-1)/2*np.pi\n",
    "    T3 = np.power(L_i(m, L_0), (gamma-1))\n",
    "    T4 = np.power((r_sq + np.power(L_i(m, L_0),2)), (gamma+1)/2)\n",
    "    return T1*T2*(T3/T4)\n",
    "    \n",
    "# Calculate the background rate:\n",
    "def mu_calc(r_sq, t_diff, omega, tau, l, a_coeffs): # tau, l are the temporal and spatial smoothing params\n",
    "    T1 = omega*np.exp(-np.sqrt(r_sq)/l)\n",
    "    T2 = np.exp(-np.abs(t_diff)/tau) # times need to be in days\n",
    "    T3 = 1/(2*np.pi*(l**2)*a_coeffs)\n",
    "    lam = T1*T2*T3\n",
    "    return lam\n",
    "\n",
    "# Triggering rate from catalog\n",
    "def nuK(catalog, c, alpha, p, gamma, K, L_0):\n",
    "    \"\"\" Calculates nu(xi,yi,ti) - triggering rate at time and place of each event in catalog\n",
    "    # L_0 is the reference length\n",
    "    # If K=1, then function actually returns nu(xi,yi,ti)/K\n",
    "    \"\"\"\n",
    "    func_start = datetime.now() # time the function\n",
    "    cat = catalog.copy(deep=True)\n",
    "    cat['nuK'] = 0.0\n",
    "    #print('Looping through catalog...')\n",
    "    for triggered in cat.itertuples():\n",
    "        # get values of source event\n",
    "        ttime = triggered.datetime\n",
    "        #print('Triggered event time:', ttime)\n",
    "        tlatrad = triggered.lat_rad\n",
    "        tlonrad = triggered.lon_rad\n",
    "        potential_triggers = cat.loc[cat[\"datetime\"] < ttime]\n",
    "        potential_triggers['c'] = c\n",
    "        potential_triggers['t_diffs'] = (1./(24.*60.*60.))*(ttime - potential_triggers['datetime']).dt.total_seconds()\n",
    "        potential_triggers['t_denom'] = potential_triggers['t_diffs'] + potential_triggers['c']\n",
    "        #print(potential_triggers['t_denom'])\n",
    "        #print(potential_triggers)\n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        potential_triggers['r_squared'] = np.square(haversine(tlatrad,potential_triggers['lat_rad'],tlonrad,potential_triggers['lon_rad']))\n",
    "        #print(len(potential_triggers['mag']), len(potential_triggers['r_squared']), len(potential_triggers['t_denom']))\n",
    "        # Calculate triggering rate nu for each event i.e. nu(xi,yi,ti)\n",
    "        nuK_array = nu_calc(c, alpha, p, gamma, K, potential_triggers['mag'], potential_triggers['t_denom'], potential_triggers['r_squared'], L_0)\n",
    "        cat.loc[triggered.Index, 'nuK'] = nuK_array.sum()\n",
    "    print('    took', (datetime.now() - func_start), 'to compute nu(xi,yi,ti)/K \\n')\n",
    "    return cat\n",
    "\n",
    "# Background rate from catalog\n",
    "def mu(catalog, tau, l):\n",
    "    cat = catalog.copy(deep=True)\n",
    "    cat['mu_i'] = 0.0\n",
    "    #cat['omega_i'] = 0.5\n",
    "    for event in cat.itertuples():\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        evtime = event.datetime\n",
    "        #print(evtime)\n",
    "        #print('Triggered event time:', ttime)\n",
    "        evlatrad = event.lat_rad\n",
    "        evlonrad = event.lon_rad\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((evtime - temp_cat['datetime']).dt.total_seconds())\n",
    "        \n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(evlatrad,temp_cat['lat_rad'],evlonrad,temp_cat['lon_rad']))\n",
    "        temp_cat['a_coeffs'] = a_coeff(temp_cat['t_diff_e'], temp_cat['t_diff_s'], tau)\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_i'],tau,l,temp_cat['a_coeffs'])\n",
    "        #print(temp_cat)\n",
    "        cat.loc[event.Index, 'mu_i'] = temp_cat['mu_indiv'].sum()\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returned_cat = nuK(catalog_gdf_d_filter, c=0.001, alpha=2, gamma=2, K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(returned_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = datetime(1970,1,1)\n",
    "#end_time = datetime(2021, 6, 21)\n",
    "#cat_stats = mu(returned_cat, 100.0, 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cat_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameter F_i for individual events\n",
    "def F_i(alpha, t_diff, c, m):\n",
    "    return np.exp(alpha*m)*(np.log(t_diff) - np.log(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declustering function ##\n",
    "def decluster(path, cat_start, cat_end, tau, l, c, alpha, p, gamma, L_0, atol):\n",
    "    \"\"\"\n",
    "    # Function to estimate normalisation parameter K and best estimates for omega\n",
    "    # path must be a filepath (str) to a CSV file containing an output ISC search result with cols:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    # tau, l are temporal and spatial smoothing params, to be given in days and km\n",
    "    # c, alpha, gamma are Omori-Utsu Law and power spectral density constants\n",
    "    # L_0 is the reference rupture length\n",
    "    # atol is the tolerance level for convergence in the MLE estimate of K\n",
    "    \"\"\"\n",
    "    calc_start = datetime.now() # time the function\n",
    "    assert cat_start < cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    \n",
    "    print('\\nCatalog start time: ', cat_start, ' Catalog end time:', cat_end, \n",
    "          '\\nNumber of events in catalog: ', len(cat.index),\n",
    "          '\\nSmoothing time:', tau, 'days '\n",
    "          'Smoothing length: ', l, 'km ', '\\nOmori Law constants: \\nc:',  c, 'days', ' alpha: ', alpha, ' p: ', \n",
    "          p, ' gamma:', gamma, '\\nReference rupture length:', L_0, 'km')\n",
    "    \n",
    "    # Load catalog from file and prepare for processing\n",
    "    print('Preparing catalog for processing...')\n",
    "    cat_preprocessed = prep_catalog(path, cat_start, cat_end)\n",
    "    \n",
    "    print('Now processing catalog...')\n",
    "    # Now calculate nu(xi,yi,ti)/K for all events:\n",
    "    print('Calculating initial triggering rate...')\n",
    "    nuK_cat = nuK(cat_preprocessed, c, alpha, p, gamma, 1.0, L_0)\n",
    "    \n",
    "    nuK_cat['omega_i'] = 0.5\n",
    "    \n",
    "    # Calculate background rates at time and place of each event, assuming omega is 0.5\n",
    "    print('Estimating a priori background rates...')\n",
    "    initial_mu_cat = mu(nuK_cat, tau, l)\n",
    "    \n",
    "    # initial_mu_cat should now contain both a nu and a mu for each event\n",
    "    # Now compute updated omega:\n",
    "    initial_mu_cat['lambda_i'] = initial_mu_cat['mu_i'] + initial_mu_cat['nuK']\n",
    "    initial_mu_cat['omega_i'] = initial_mu_cat['mu_i'] / initial_mu_cat['nuK']\n",
    "    \n",
    "    # Initialise some columns for the MLE estimate\n",
    "    initial_mu_cat['c'] = c # in days\n",
    "    initial_mu_cat['t_diffs'] = (1./(24.*60.*60.))*(cat_end - initial_mu_cat['datetime']).dt.total_seconds() # in days\n",
    "    initial_mu_cat['t_quantity'] = initial_mu_cat['t_diffs'] + initial_mu_cat['c']\n",
    "    initial_mu_cat['K_num'] = 1 - (initial_mu_cat['omega_i'])\n",
    "    initial_mu_cat['F_i'] = F_i(alpha, initial_mu_cat['t_quantity'], c, initial_mu_cat['mag'])\n",
    "    \n",
    "    # Initialise K - an initial guess\n",
    "    K = initial_mu_cat['K_num'].sum() / initial_mu_cat['F_i'].sum()\n",
    "    fevals = 0 # record number of function evaluations so we can later compare methods\n",
    "    K_prev = K + 2*atol # initialise the previous K simply so that while loop argument is initially true\n",
    "    updated_cat = initial_mu_cat.copy(deep=True)\n",
    "    print('Starting iteration for MLE estimate of K')\n",
    "    while abs(K - K_prev) > atol:\n",
    "        K_prev = K\n",
    "        \n",
    "        # Compute updated nu based on new value of K\n",
    "        updated_cat = nuK(updated_cat, c, alpha, p, gamma, K, L_0)\n",
    "        \n",
    "        # Now compute updated omega:\n",
    "        updated_cat['lambda_i'] = updated_cat['mu_i'] + updated_cat['nuK']\n",
    "        updated_cat['omega_i'] = updated_cat['mu_i'] / updated_cat['nuK']\n",
    "        updated_cat = mu(updated_cat, tau, l)\n",
    "        updated_cat['K_num'] = 1 - (updated_cat['omega_i'])\n",
    "        \n",
    "        # Compute K using updated omega:\n",
    "        K =  updated_cat['K_num'].sum() / updated_cat['F_i'].sum()\n",
    "        fevals += 1\n",
    "        #print('Current iteration solution: ',K)\n",
    "    print('The final value of K is', K)\n",
    "    print('\\n', fevals, 'function evaluations were required for K convergence')\n",
    "    \n",
    "    # Using final K value calculate a final triggering rate nu(xi,yi,ti):\n",
    "    final_cat = nuK(updated_cat, c, alpha, p, gamma, K, L_0)\n",
    "    \n",
    "    # now the final catalog should contain the correct omegas, which can be used to estimate a background seismicity rate curve\n",
    "    # Check this visually using a histogram\n",
    "    plt.hist(final_cat[\"omega_i\"],log=True)\n",
    "    print('    took', (datetime.now() - calc_start), 'for declustering \\n')\n",
    "    return final_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING ###\n",
    "cat_start = datetime(1970,1,1)\n",
    "cat_end = datetime(2021, 6, 21)\n",
    "cat = catalog_gdf\n",
    "tau=100; l=100; c=0.001; alpha=2; p=1; gamma=2; L_0=1.78; atol=0.01\n",
    "print('\\nCatalog start time: ', cat_start, ' Catalog end time:', cat_end, \n",
    "          '\\nNumber of events in catalog: ', len(cat.index),\n",
    "          '\\nSmoothing time:', tau, 'days '\n",
    "          'Smoothing length: ', l, 'km ', '\\nOmori Law constants: \\nc:',  c, 'days', ' alpha: ', alpha, ' p: ', \n",
    "          p, ' gamma:', gamma, '\\nReference rupture length:', L_0, 'km')\n",
    "\n",
    "# Preprocess\n",
    "cat_preprocessed = prep_catalog(cat, cat_start, cat_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuK_cat = nuK(cat_preprocessed, c, alpha, p, gamma, 1.0, L_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuK_cat.loc[:, 'omega_i'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mu_cat = mu(nuK_cat, tau, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_calc(r_sq, t_diff, omega, tau, l, a_coeffs)\n",
    "print(mu_calc(729811.462763, -35.386771, 0.5, 100., 100., 132.751616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_preprocessed.head()\n",
    "#print(nuK_cat)\n",
    "#print(initial_mu_cat)\n",
    "initial_mu_cat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final estimate of background seismicity rate ##\n",
    "def mu_final(x,y,cat_start, cat_end, cat, tau, l):\n",
    "    #########################################################\n",
    "    # Function computes timeseries of the background seismicity rate\n",
    "    # x,y refer to a spatial reference point - should be the centroid of the study area\n",
    "    # Function will build an array of datetime objects with a timestep of 1 day using the cat_start, cat_end times\n",
    "    # catalog should contain omega, a_coeff values for each event - the output of func decluster\n",
    "    #########################################################\n",
    "    assert t_cat_start < t_cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    \n",
    "    # Time steps for calculating background rate\n",
    "    times = np.arange(start_time, end_time, timedelta(days=1)).astype(datetime)\n",
    "    mu_t_series = []\n",
    "    \n",
    "    # Compute background rate at each time step\n",
    "    for t_step in times:\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((t_step - temp_cat['datetime']).dt.total_seconds())\n",
    "        \n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(x, temp_cat['lat_rad'], y, temp_cat['lon_rad']))\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_i'],tau,l,temp_cat['a_coeffs'])\n",
    "        mu_t_series.append(temp_cat['mu_indiv'].sum())\n",
    "    return times, mu_t_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declustering implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISC web search params\n",
    "start_time = datetime(1970,1,1)\n",
    "end_time = datetime(2021, 6, 21)\n",
    "min_latitude = -25\n",
    "max_latitude = -11\n",
    "min_longitude = -80\n",
    "max_longitude = -66\n",
    "min_mag = 4.5\n",
    "max_mag = None\n",
    "min_depth = None\n",
    "max_depth = None\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_attempt = decluster(catalog_gdf, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, p=1, gamma=2, L_0=1.78, atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECLUSTERING ###\n",
    "\n",
    "# Separate catalog into deep and shallow following Jara et al. (2017)\n",
    "catalog_shallow = catalog_gdf.loc[catalog_gdf['depth_km'] < 40.0]\n",
    "catalog_deep = catalog_gdf.loc[catalog_gdf['depth_km'] > 80.0]\n",
    "declustered_shallow_cat = decluster(catalog_shallow, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "declustered_deep_cat = decluster(catalog_deep, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "\n",
    "# And calculate rates:\n",
    "times, deep_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_deep_cat, tau=100, l=100)\n",
    "times, shallow_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_shallow_cat, tau=100, l=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zaliapin et al. (2008) Declustering\n",
    "\n",
    "Using algorithms from Zaliapin & Ben-Zion (2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First find the best-fitting completeness magnitude and the best-fitting b-value\n",
    "## Using code by Mizrahi et al. (2021) as sourced from https://github.com/lmizrahi/etas/blob/main/mc_b_est.py\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# joint beta and completeness magnitude estimation\n",
    "# using p-value of Kolmogorov-Smirnov distance to fitted Gutenberg-Richter law\n",
    "#\n",
    "# as described by Mizrahi et al., 2021\n",
    "# Leila Mizrahi, Shyam Nandan, Stefan Wiemer;\n",
    "# The Effect of Declustering on the Size Distribution of Mainshocks.\n",
    "# Seismological Research Letters 2021; doi: https://doi.org/10.1785/0220200231\n",
    "# inspired by method of Clauset et al., 2009\n",
    "##############################################################################\n",
    "\n",
    "# mc is the binned completeness magnitude,\n",
    "# so the 'true' completeness magnitude is mc - delta_m / 2\n",
    "\n",
    "\n",
    "def round_half_up(n, decimals=0):\n",
    "    # this is because numpy does weird rounding.\n",
    "    multiplier = 10 ** decimals\n",
    "    return np.floor(n * multiplier + 0.5) / multiplier\n",
    "\n",
    "\n",
    "def estimate_beta_tinti(magnitudes, mc, weights=None, axis=None, delta_m=0):\n",
    "    # Tinti, S., & Mulargia, F. (1987). Confidence intervals of b values for grouped magnitudes.\n",
    "    # Bulletin of the Seismological Society of America, 77(6), 2125-2134.\n",
    "\n",
    "    if delta_m > 0:\n",
    "        p = (1 + (delta_m / (np.average(magnitudes - mc, weights=weights, axis=axis))))\n",
    "        beta = 1 / delta_m * np.log(p)\n",
    "    else:\n",
    "        beta = 1 / np.average((magnitudes - (mc - delta_m / 2)), weights=weights, axis=axis)\n",
    "    return beta\n",
    "\n",
    "\n",
    "def simulate_magnitudes(n, beta, mc):\n",
    "    mags = np.random.uniform(size=n)\n",
    "    mags = (-1 * np.log(1 - mags) / beta) + mc\n",
    "    return mags\n",
    "\n",
    "\n",
    "def fitted_cdf_discrete(sample, mc, delta_m, x_max=None, beta=None):\n",
    "    if beta is None:\n",
    "        beta = estimate_beta_tinti(sample, mc=mc, delta_m=delta_m)\n",
    "\n",
    "    if x_max is None:\n",
    "        sample_bin_n = (sample.max() - mc) / delta_m\n",
    "    else:\n",
    "        sample_bin_n = (x_max - mc) / delta_m\n",
    "    bins = np.arange(sample_bin_n + 1)\n",
    "    cdf = 1 - np.exp(-beta * delta_m * (bins + 1))\n",
    "    x, y = mc + bins * delta_m, cdf\n",
    "\n",
    "    x, y_count = np.unique(x, return_counts=True)\n",
    "    return x, y[np.cumsum(y_count) - 1]\n",
    "\n",
    "\n",
    "def empirical_cdf(sample, weights=None):\n",
    "    try:\n",
    "        sample = sample.values\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        weights = weights.values\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sample_idxs_sorted = np.argsort(sample)\n",
    "    sample_sorted = sample[sample_idxs_sorted]\n",
    "    if weights is not None:\n",
    "        weights_sorted = weights[sample_idxs_sorted]\n",
    "        x, y = sample_sorted, np.cumsum(weights_sorted) / weights_sorted.sum()\n",
    "    else:\n",
    "        x, y = sample_sorted, np.arange(1, len(sample) + 1) / len(sample)\n",
    "\n",
    "    # only return one value per bin\n",
    "    x, y_count = np.unique(x, return_counts=True)\n",
    "    return x, y[np.cumsum(y_count) - 1]\n",
    "\n",
    "\n",
    "def ks_test_gr(sample, mc, delta_m, ks_ds=None, n_samples=10000, beta=None):\n",
    "    sample = sample[sample >= mc - delta_m / 2]\n",
    "    if len(sample) == 0:\n",
    "        print(\"no sample\")\n",
    "        return 1, 0, []\n",
    "    if len(np.unique(sample)) == 1:\n",
    "        print(\"sample contains only one value\")\n",
    "        return 1, 0, []\n",
    "    if beta is None:\n",
    "        beta = estimate_beta_tinti(sample, mc=mc, delta_m=delta_m)\n",
    "\n",
    "    if ks_ds is None:\n",
    "        ks_ds = []\n",
    "\n",
    "        n_sample = len(sample)\n",
    "        simulated_all = round_half_up(\n",
    "            simulate_magnitudes(mc=mc - delta_m / 2, beta=beta, n=n_samples * n_sample) / delta_m\n",
    "        ) * delta_m\n",
    "\n",
    "        x_max = np.max(simulated_all)\n",
    "        x_fit, y_fit = fitted_cdf_discrete(sample, mc=mc, delta_m=delta_m, x_max=x_max, beta=beta)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            simulated = simulated_all[n_sample * i:n_sample * (i + 1)].copy()\n",
    "            x_emp, y_emp = empirical_cdf(simulated)\n",
    "            y_fit_int = np.interp(x_emp, x_fit, y_fit)\n",
    "\n",
    "            ks_d = np.max(np.abs(y_emp - y_fit_int))\n",
    "            ks_ds.append(ks_d)\n",
    "    else:\n",
    "        x_fit, y_fit = fitted_cdf_discrete(sample, mc=mc, delta_m=delta_m, beta=beta)\n",
    "\n",
    "    x_emp, y_emp = empirical_cdf(sample)\n",
    "    y_emp_int = np.interp(x_fit, x_emp, y_emp)\n",
    "\n",
    "    orig_ks_d = np.max(np.abs(y_fit - y_emp_int))\n",
    "\n",
    "    return orig_ks_d, sum(ks_ds >= orig_ks_d) / len(ks_ds), ks_ds\n",
    "\n",
    "\n",
    "def estimate_mc(sample, mcs_test, delta_m, p_pass, stop_when_passed=True, verbose=False, beta=None,\n",
    "                n_samples=10000):\n",
    "    \"\"\"\n",
    "    sample: np array of magnitudes to test\n",
    "    mcs_test: completeness magnitudes to test\n",
    "    delta_m: magnitude bins (sample has to be rounded to bins beforehand)\n",
    "    p_pass: p-value with which the test is passed\n",
    "    stop_when_passed: stop calculations when first mc passes the test\n",
    "    verbose: verbose\n",
    "    beta: if beta is 'known', only estimate mc\n",
    "    n_samples: number of magnitude samples to be generated in p-value calculation of KS distance\n",
    "    \"\"\"\n",
    "\n",
    "    ks_ds = []\n",
    "    ps = []\n",
    "    i = 0\n",
    "    for mc in mcs_test:\n",
    "        if verbose:\n",
    "            print('\\ntesting mc', mc)\n",
    "        ks_d, p, _ = ks_test_gr(sample, mc=mc, delta_m=delta_m, n_samples=n_samples, beta=beta)\n",
    "\n",
    "        ks_ds.append(ks_d)\n",
    "        ps.append(p)\n",
    "\n",
    "        i += 1\n",
    "        if verbose:\n",
    "            print('..p-value: ', p)\n",
    "\n",
    "        if p >= p_pass and stop_when_passed:\n",
    "            break\n",
    "    ps = np.array(ps)\n",
    "    if np.any(ps >= p_pass):\n",
    "        best_mc = mcs_test[np.argmax(ps >= p_pass)]\n",
    "        if beta is None:\n",
    "            beta = estimate_beta_tinti(sample[sample >= best_mc - delta_m / 2], mc=best_mc, delta_m=delta_m)\n",
    "        if verbose:\n",
    "            print(\"\\n\\nFirst mc to pass the test:\", best_mc, \"\\nwith a b-value of:\", beta/np.log(10))\n",
    "    else:\n",
    "        best_mc = None\n",
    "        beta = None\n",
    "        if verbose:\n",
    "            print(\"None of the mcs passed the test.\")\n",
    "\n",
    "    # beta is the Tinti beta - so b value is beta/ln10\n",
    "    # return the b-value\n",
    "    return mcs_test, ks_ds, ps, best_mc, beta/np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the catalog\n",
    "cat_zaliapin = prep_cat_zaliapin('Jara_raw_catalog_data.csv')\n",
    "# And extract magnitudes\n",
    "magnitude_sample = cat_zaliapin['mag'].values\n",
    "\n",
    "mcs = round_half_up(np.arange(2.0, 5.5, 0.1), 1)\n",
    "mcs_tested, ks_distances, p_values, mc_winner, b_value_winner = estimate_mc(magnitude_sample,mcs,delta_m=0.1,p_pass=0.05,\n",
    "        stop_when_passed=False,verbose=True,n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zaliapin and Ben-Zion declustering implementation by Dr Richard Walters\n",
    "# Direct Python translation of MATLAB scripts/functions\n",
    "\n",
    "import numpy.matlib\n",
    "import gc\n",
    "## Load catalog from file:\n",
    "cat_start = datetime(1970,1,1)\n",
    "cat_end = datetime(2021, 6, 21)\n",
    "fname = 'Jara_raw_catalog_data.csv'\n",
    "input_cat = prep_catalog(fname, cat_start, cat_end)\n",
    "input_cat = cat_preprocessed.sort_index()\n",
    "\n",
    "\n",
    "# df = 1.6; b = 1; q = 0.5; etathresh = 10^-5;\n",
    "\n",
    "\n",
    "lon = input_cat['lon_rad'].values # latitude\n",
    "lat = input_cat['lat_rad'].values #longitude\n",
    "t = input_cat['datetime'].values #here they could be datetimes\n",
    "Mw = input_cat['mag'].values #magnitudes\n",
    "z = input_cat['depth_km'].values # depths\n",
    "nqks = len(input_cat.index) # len of catalog\n",
    "\n",
    "\n",
    "# Create long arrays of repeating lat and lon for time-space distance calculations\n",
    "latmat = np.matlib.repmat(lat,1,nqks);\n",
    "lonmat = np.matlib.repmat(lon,1,nqks);\n",
    "# a' in matlab means the transpose of the matrix a!\n",
    "\n",
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    \"\"\"Haversine function\n",
    "    Takes in arguments in radians\n",
    "    \"\"\"\n",
    "    return np.square(np.sin(theta / 2))\n",
    "\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    \"\"\"Haversine distance in km - calculate distance between 2 pts on a sphere\n",
    "    lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2 must all be in radians\n",
    "    \"\"\"\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(np.sqrt(hav(lat_rad_1 - lat_rad_2)+ np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d\n",
    "## Memory issues!!\n",
    "#Haversine distances\n",
    "delr = haversine(latmat.T, latmat, lonmat.T, lonmat)\n",
    "mindelr = np.min(delr[delr>0]) # find min positive value of all the haversine distances\n",
    "delr[delr==0]=mindelr # set all values equal to zero to the min positive value\n",
    "\n",
    "# Time distances - in years\n",
    "tmat = np.matlib.repmat(t,1,nqks)\n",
    "delt = tmat.T - tmat\n",
    "mindelt = np.min(delt[delt>0])\n",
    "delt[delt==0]=mindelt\n",
    "\n",
    "#delta M calculation - not used in regular Z&B-Z but left in here in case it's of use...\n",
    "Mwmat = np.matlib.repmat(Mw,1,nqks)\n",
    "delM = Mwmat.T - Mwmat;\n",
    "\n",
    "delT = delt*np.power(10.,(-q*b*(Mwmat-0))) # compute scaled time distance\n",
    "delR = np.power(delr, df)*np.power(10, (-(1-q)*b*(Mwmat-0))) # compute scaled spatial distance\n",
    "eta = delT*delR # compute the eta param\n",
    "\n",
    "utri_eta = np.triu(eta,1) # get upper triangle of matrix eta\n",
    "utri_eta[utri_eta==0]=1e9;\n",
    "mineta = np.amin(utri_eta, axis=0)\n",
    "ids = np.argmin(arr2D, axis=0)\n",
    "#for each child, find the nnd 'nearest' parent from all qks that comes before\n",
    "idchild = [i for i in range(2,nqks+1)] #each child is unique\n",
    "idparent = ids[2:] \n",
    "\n",
    "#convert to a single index to pull values from the full matrices delTnn = delT(idx)\n",
    "#%delT values for each earthquake, treating it as a 'child'\n",
    "#idx = sub2ind(size(delT), idparent,idchild)\n",
    "idx = np.ravel_multi_index(np.array([idparent, idchild]), np.shape(delT), order='F')\n",
    "delRnn = delR[idx]; #delR values for each earthquake as a 'child'\n",
    "#eta values for each earthquake as a 'child'. Those with eta<threshold have a valid parent and are aftershocks\n",
    "#those with eta>=threshold are 'orphans' or individual earthquakes/'singles'\n",
    "etann = eta[idx]\n",
    "#-ve = smaller aftershock, 0 = same sized aftershock, +ve = larger aftershock delrnn = delr(idx); deltnn = delt(idx);\n",
    "delMnn = delM[idx] \n",
    "\n",
    "# output histogram data\n",
    "#histoutput = [idparent.T idchild.T delTnn.T delRnn.T etann.T deltnn.T delrnn.T delMnn.T]\n",
    "\n",
    "#fp = fopen('dep_histoutput.txt','w');\n",
    "#fprintf(fp, '%f %f %4.9f %4.9f %4.9f %f %f %f\\n', histoutput'); fclose(fp);\n",
    "\n",
    "\n",
    "# plot 2d and 1d histograms\n",
    "#figure; histogram2(log10(delTnn),log10(delRnn),'FaceColor','flat')\n",
    "#hold on\n",
    "#plot3([-7 -5 -1],[-7 -5 -1]*-1  + log10(etathresh) ,[800 800 800],'r')\n",
    "\n",
    "#figure; histogram(log10(etann))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaliapin declustering - wrapper functions\n",
    "\n",
    "# Prepare catalog:\n",
    "def prep_cat_zaliapin(cat_init):\n",
    "    \"\"\" Loads and prepares the catalog for further processing\n",
    "    # input cat_init needs to be a file path to a CSV document containing labelled columns:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    \"\"\"\n",
    "    # Load catalog from file:\n",
    "    cat = pd.read_csv(cat_init, index_col=0)\n",
    "    cat = cat.sort_index()\n",
    "    \n",
    "    # Create datetimes\n",
    "    cat[\"datetime\"] = pd.to_datetime(cat[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    cat = cat.infer_objects()\n",
    "    cat.loc[:, 'depth_km'] *=0.001\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    cat['lat_rad'] = np.radians(cat['lat'])\n",
    "    cat['lon_rad'] = np.radians(cat['lon'])\n",
    "    return cat\n",
    "\n",
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    \"\"\"Haversine function\n",
    "    Takes in arguments in radians\n",
    "    \"\"\"\n",
    "    return np.square(np.sin(theta / 2))\n",
    "\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    \"\"\"Haversine distance in km - calculate distance between 2 pts on a sphere\n",
    "    lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2 must all be in radians\n",
    "    \"\"\"\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(np.sqrt(hav(lat_rad_1 - lat_rad_2)+ np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Zaliapin algorithm\n",
    "\n",
    "def zaliapin_eta_vals(cat, q, d_f, b):\n",
    "    \"\"\" Function to calculate time-space distances for earthquake catalog declustering\n",
    "        using the algorithms by Zaliapin and Ben-Zion (2013)\n",
    "        Based on MATLAB scripts by Dr Richard Walters\n",
    "        cat_path must be a filepath to a CSV file containing raw data from an ISC catalog search\n",
    "        The following columns are expected:\n",
    "        # Index, year, month, day, hour, minute, second, lat,lon, depth, mag (the depth must be in metres)\n",
    "        q is a constant - most studies assume this to be 0.5\n",
    "        d_f is a (fractal) epicentre dimension - often assumed as 1.6\n",
    "        b is the catalog b-value\n",
    "    \"\"\"\n",
    "    calc_start = datetime.now() # time the function\n",
    "    \n",
    "    copy_catalog = cat.copy(deep=True)\n",
    "\n",
    "    # initialise columns to hold values of the Nearest-Neighbour (NN) time-space distances\n",
    "    cat['delTnn'] = 0.0 \n",
    "    cat['delRnn'] = 0.0\n",
    "    cat['etann'] = 0.0\n",
    "    \n",
    "    print('Now looping over catalog to compute eta values...')\n",
    "    for triggered in cat.itertuples():\n",
    "        # get values of source event\n",
    "        ttime = triggered.datetime\n",
    "        #print('Triggered event time:', ttime)\n",
    "        tlatrad = triggered.lat_rad\n",
    "        tlonrad = triggered.lon_rad\n",
    "        \n",
    "        # Only consider events before the suspected triggered event (these can be potential parents)\n",
    "        potential_triggers = copy_catalog.loc[copy_catalog[\"datetime\"] < ttime]\n",
    "        \n",
    "        # Calculate time diffs in yrs\n",
    "        potential_triggers['delt'] = (1./(24.*60.*60.*365.25))*(ttime - potential_triggers['datetime']).dt.total_seconds()\n",
    "        mindelt = potential_triggers.loc[potential_triggers['delt'] > 0].min()\n",
    "        potential_triggers.loc[potential_triggers['delt'] == 0] = mindelt\n",
    "        \n",
    "        # Calculate spatial distances\n",
    "        potential_triggers['delr'] = haversine(tlatrad,potential_triggers['lat_rad'],tlonrad,potential_triggers['lon_rad'])\n",
    "        mindelr = potential_triggers.loc[potential_triggers['delr'] > 0].min()\n",
    "        potential_triggers.loc[potential_triggers['delr'] == 0] = mindelr\n",
    "        \n",
    "        # Compute scaled time and space distance and eta\n",
    "        potential_triggers['delT'] = potential_triggers['delt'] * np.power(10, (-q*b*potential_triggers['mag']))\n",
    "        potential_triggers['delR'] = (np.power(potential_triggers['delr'], d_f) * \n",
    "                                        np.power(10, (-(1-q)*b*potential_triggers['mag'])))\n",
    "        potential_triggers['eta'] = potential_triggers['delT'] * potential_triggers['delR']\n",
    "        potential_triggers.loc[potential_triggers['eta'] == 0] = 1e9\n",
    "        \n",
    "        #print(potential_triggers)\n",
    "        # Now pick the NND T-R distance for the suspected triggered event and its index\n",
    "        # And assign values to the suspected child in the master catalog\n",
    "        cat.loc[triggered.Index, 'etann'] = potential_triggers['eta'].min()\n",
    "        try:\n",
    "            idx = potential_triggers['eta'].idxmin()\n",
    "            cat.loc[triggered.Index, 'delTnn'] = potential_triggers.loc[idx, 'delT']\n",
    "            cat.loc[triggered.Index, 'delRnn'] = potential_triggers.loc[idx, 'delR']\n",
    "        except:\n",
    "            cat.loc[triggered.Index, 'delTnn'] = 1.0\n",
    "            cat.loc[triggered.Index, 'delRnn'] = 1.0\n",
    "            cat.loc[triggered.Index, 'etann'] = 1.0\n",
    "    \n",
    "    print('    took', (datetime.now() - calc_start), 'for calculating eta values \\n')\n",
    "    return cat # the catalog with eta values\n",
    "\n",
    "def Gaussian_mixture_zaliapin(cat):\n",
    "    \"\"\" Function to calculate declustering threshold eta_0 for the Zaliapin and Ben-Zion (2013) method\n",
    "        Cat must be an earthquake catalog as a df containing an 'etann' column for values of time-space distance for each event\n",
    "    \"\"\"\n",
    "    x = np.log(cat['etann'])\n",
    "    f = np.ravel(x).astype(np.float)\n",
    "    f=f.reshape(-1,1)\n",
    "    \n",
    "    # fit a Gaussian mixture Model with 2 components: background and mixed\n",
    "    g = mixture.GaussianMixture(n_components=2,covariance_type='full') \n",
    "    g.fit(f)\n",
    "    \n",
    "    # Get the weights, means, and covariances for each of the 2 fitted Gaussians\n",
    "    weights = g.weights_\n",
    "    means = g.means_\n",
    "    covars = g.covariances_\n",
    "    #print(weights, means, covars)\n",
    "    \n",
    "    # Set up figure for plotting\n",
    "    fig, ax = plt.subplots(figsize =(10, 5))\n",
    "    \n",
    "    # Compute a high res histogram from the eta data\n",
    "    n, bins = np.histogram(np.log(cat['etann']), 200, density=True)\n",
    "    binscenters = np.array([0.5 * (bins[i] + bins[i+1]) for i in range(len(bins)-1)])\n",
    "    \n",
    "    f_axis = f.copy().ravel()\n",
    "    f_axis.sort()\n",
    "    # Plot Gaussian for the clustered mode\n",
    "    ax.plot(f_axis,weights[1]*ss.norm.pdf(f_axis,means[1],np.sqrt(covars[1])).ravel(), c='red')\n",
    "    # Plot Gaussian for the background mode\n",
    "    ax.plot(f_axis,weights[0]*ss.norm.pdf(f_axis,means[0],np.sqrt(covars[0])).ravel(), c='blue')\n",
    "    # Plot the histogram for comparison\n",
    "    ax.plot(binscenters, n, 'k-')\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    y1 = weights[1]*ss.norm.pdf(f_axis,means[1],np.sqrt(covars[1])).ravel() # the clustered mode\n",
    "    y2 = weights[0]*ss.norm.pdf(f_axis,means[0],np.sqrt(covars[0])).ravel() # the background mode\n",
    "    x = f_axis\n",
    "    # Find indexes of points where the 2 Gaussians intercept\n",
    "    idxs=np.argwhere(np.diff(np.sign(y1 - y2))).flatten()\n",
    "    # Display the intersections\n",
    "    plt.figure(figsize=[2.5,2.5])\n",
    "    ax=plt.subplot()\n",
    "    ax.plot(x,y1,color='r',label='line1',alpha=0.5)\n",
    "    ax.plot(x,y2,color='b',label='line2',alpha=0.5)\n",
    "    _=[ax.axvline(x[i],color='k') for i in idxs]\n",
    "    _=[ax.text(x[i],ax.get_ylim()[1],f\"{x[i]:1.1f}\",ha='center',va='bottom') for i in idxs]\n",
    "    ax.legend(bbox_to_anchor=[1,1])\n",
    "    ax.set(xlabel='x',ylabel='density')\n",
    "    # pick out the first intersection and compute threshold:\n",
    "    etathresh = np.exp(f_axis[idxs[0]])\n",
    "    print('Threshold eta for declustering:', etathresh)\n",
    "    return etathresh\n",
    "    \n",
    "# Optional histogram visualisation    \n",
    "def zaliapin_histogram(cat):\n",
    "    \"\"\" Plot a histogram for Zaliapin-type declustering - OPTIONAL\n",
    "        Requires a catalog that contains eta, delTnn, delRnn columns\n",
    "    \"\"\"\n",
    "    # Creating bins\n",
    "    delTnn_bins = np.linspace(np.log(cat['delTnn'].min()), np.log(cat['delTnn'].max()), 50)\n",
    "    delRnn_bins = np.linspace(np.log(cat['delRnn'].min()), np.log(cat['delRnn'].max()), 50)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize =(20, 10))\n",
    "    # Creating plot\n",
    "    im = ax1.hist2d(np.log(cat['delTnn']), np.log(cat['delRnn']), bins =[delTnn_bins, delRnn_bins], \n",
    "               cmap = 'viridis')\n",
    "    ax1.set_title(\"Spatio-temporal scaled distances\", fontsize=16)\n",
    "\n",
    "    # Adding color bar\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.6, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    fig.colorbar(im[3], cax=cax, orientation='horizontal')\n",
    "\n",
    "    xmin, xmax = ax1.get_xlim()\n",
    "    x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "    # Plot the cutoff lines\n",
    "    #ax1.plot(x, (-1*x+ np.log(etathresh)), 'r', label='Walters cutoff')\n",
    "\n",
    "    #for eta_thresh, color in zip([1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7], ['r', 'g', 'y', 'o', 'w', 'm', 'c']):\n",
    "        #ax1.plot(x, (-1*x+ np.log(eta_thresh)), color, label=eta_thresh)\n",
    "\n",
    "    ax1.legend(loc='best')\n",
    "\n",
    "    ax1.set_xlabel('$ln(T_{nn})$', fontsize=14) \n",
    "    ax1.set_ylabel('$ln(R_{nn})$', fontsize=14)\n",
    "\n",
    "    # the histogram of the data\n",
    "    n, bins, patches = ax2.hist(np.log(cat['etann']), 100, density=True, facecolor='b', alpha=0.75)\n",
    "    ax2.set_xlabel('$ln(\\eta)$', fontsize=14)\n",
    "    ax2.set_ylabel('Frequency', fontsize=14)\n",
    "    ax2.set_title('Histogram of $\\eta_{nn}$', fontsize=16)\n",
    "\n",
    "    # show plot\n",
    "    plt.show()\n",
    "    fig.savefig('zaliapin_histogram.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Zaliapin declustering routine\n",
    "def zaliapin_decluster(cat_path, q, d_f):\n",
    "    \"\"\" Function to decluster an earthquake catalog using the algorithms by Zaliapin and Ben-Zion (2013)\n",
    "        Based on MATLAB scripts by Dr Richard Walters\n",
    "        cat_path must be a filepath to a CSV file containing raw data from an ISC catalog search\n",
    "        The following columns are expected:\n",
    "        # Index, year, month, day, hour, minute, second, lat,lon, depth, mag (the depth must be in metres)\n",
    "        q is a constant - most studies assume this to be 0.5\n",
    "        d_f is a (fractal) epicentre dimension - often assumed as 1.6\n",
    "    \"\"\"\n",
    "    calc_start = datetime.now() # time the function\n",
    "\n",
    "    # Preprocess catalog to convert lat,lon to radians and create datetimes\n",
    "    print('Preprocessing catalog...')\n",
    "    cat_zaliapin = prep_cat_zaliapin(cat_path)\n",
    "    print('Done.')\n",
    "    \n",
    "    # Extract magnitudes for b-value calculation\n",
    "    print('\\nb-value estimation')\n",
    "    magnitude_sample = cat_zaliapin['mag'].values\n",
    "    mcs = round_half_up(np.arange(2.0, 5.5, 0.1), 1)\n",
    "    # Estimate the catalog b-value using the Mizrahi algorithm\n",
    "    mcs_tested, ks_distances, p_values, mc_winner, b_value_winner = estimate_mc(magnitude_sample,mcs,delta_m=0.1,p_pass=0.05,\n",
    "            stop_when_passed=False,verbose=True,n_samples=1000)\n",
    "    \n",
    "    # Calculate the eta values\n",
    "    print('\\nCalculating eta values')\n",
    "    cat_etavals = zaliapin_eta_vals(cat_zaliapin, q, d_f, b_value_winner)\n",
    "    print('Done')\n",
    "    \n",
    "    # Estimate Gaussian mixture model parameters and calculate declustering threshold:\n",
    "    eta_thr = Gaussian_mixture_zaliapin(cat_etavals)\n",
    "    \n",
    "    # Decluster the catalog\n",
    "    declustered_cat = cat_etavals.loc[cat_etavals['etann'] > eta_thr]\n",
    "    \n",
    "    print('    took', (datetime.now() - calc_start), 'for declustering catalog \\n')\n",
    "    return declustered_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_zaliapin_declustered = zaliapin_decluster('Jara_raw_catalog_data.csv', 0.5, 1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option for a histogram - like Z-BZ 2013\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# fit an array of size [Ndim, Nsamples]\n",
    "data = np.vstack([x, y])\n",
    "kde = gaussian_kde(data)\n",
    "\n",
    "# evaluate on a regular grid\n",
    "xgrid = np.linspace(-3.5, 3.5, 40)\n",
    "ygrid = np.linspace(-6, 6, 40)\n",
    "Xgrid, Ygrid = np.meshgrid(xgrid, ygrid)\n",
    "Z = kde.evaluate(np.vstack([Xgrid.ravel(), Ygrid.ravel()]))\n",
    "\n",
    "# Plot the result as an image\n",
    "plt.imshow(Z.reshape(Xgrid.shape),\n",
    "           origin='lower', aspect='auto',\n",
    "           extent=[-3.5, 3.5, -6, 6],\n",
    "           cmap='Blues')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(\"density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting routines for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load catalog from file:\n",
    "cat_start = datetime(1970,1,1)\n",
    "cat_end = datetime(2021, 6, 21)\n",
    "fname = 'Jara_raw_catalog_data.csv'\n",
    "cat_preprocessed = prep_cat_zaliapin(fname)\n",
    "cat_preprocessed = cat_preprocessed.sort_index()\n",
    "cat_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spatial reference point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "lat_point_list = [min_latitude, max_latitude, min_latitude]\n",
    "lon_point_list = [min_longitude, max_longitude, min_longitude]\n",
    "\n",
    "search_area = Polygon(zip(lon_point_list, lat_point_list))\n",
    "crs = {'init': 'epsg:4326'}\n",
    "search_area = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])       \n",
    "#print(polygon.geometry)\n",
    "\n",
    "x_cent = np.radians(search_area.centroid.x)\n",
    "y_cent = np.radians(search_area.centroid.y)\n",
    "print('Geographic ref point: ', x_cent, y_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create df of important events:\n",
    "fname = 'large_eq_Jara.csv'\n",
    "large_ev = prep_cat_zaliapin(fname)\n",
    "large_ev = large_ev.set_index('datetime')\n",
    "large_ev = large_ev.sort_index()\n",
    "large_ev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_zaliapin_declustered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOTTING ####\n",
    "\n",
    "# Counting\n",
    "cum_cat = cat_preprocessed.copy(deep=True)\n",
    "cum_cat = cum_cat.set_index('datetime')\n",
    "cum_cat['count'] = 1.0\n",
    "cum_cat = cum_cat.cumsum()\n",
    "\n",
    "# And repeat on declustered catalog\n",
    "cum_cat_declustered = cat_zaliapin_declustered.copy(deep=True)\n",
    "cum_cat_declustered = cum_cat_declustered.set_index('datetime')\n",
    "cum_cat_declustered['count'] = 1.0\n",
    "cum_cat_declustered = cum_cat_declustered.cumsum()\n",
    "\n",
    "# Fit quadratics\n",
    "from numpy.polynomial import Polynomial as P\n",
    "time_diffs_orig = (1./(24.*60.*60.))*(cum_cat.index - cat_start).total_seconds()\n",
    "counts_orig = cum_cat['count'].values\n",
    "p_orig = P.fit(time_diffs_orig,counts_orig,2)\n",
    "time_diffs_declust = (1./(24.*60.*60.))*(cum_cat_declustered.index - cat_start).total_seconds()\n",
    "counts_declust = cum_cat_declustered['count'].values\n",
    "p_declust = P.fit(time_diffs_declust,counts_declust,2)\n",
    "\n",
    "\n",
    "# Plot cumulative no of events with time and fit a quadratic\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "ax1.plot(cum_cat.index.values, cum_cat['count'], 'b', label='Original catalog')\n",
    "ax1.plot(cum_cat_declustered.index.values, cum_cat_declustered['count'], 'r', label='Declustered catalog')\n",
    "#ax1.plot(cum_cat.index.values, p_orig(time_diffs_orig), 'steelblue', label='quadratic fit - original')\n",
    "#ax1.plot(cum_cat_declustered.index.values, p_declust(time_diffs_declust), 'salmon', label='quadratic fit - declustered')\n",
    "\n",
    "\n",
    "# Plot important large evs\n",
    "bottom, top = ax1.get_ylim()\n",
    "# these are matplotlib.patch.Patch properties\n",
    "props = dict(boxstyle='round', facecolor='white', alpha=1.0)\n",
    "indices = large_ev.index.values\n",
    "for date, name, m in zip(large_ev.index.values, large_ev['name'], large_ev['mag']):\n",
    "    #ind_ev = indices.where(indices == date)\n",
    "    ax1.plot([date, date], [bottom, top], 'k--')\n",
    "    #ax.arrow(date, 0, 0, cum_cat['count'].values[ind_ev], head_width=0.05, head_length=0.1, fc='k', ec='k')\n",
    "    ax1.text(date, bottom+1000, (name+' $M_w$'+str(m)), size=\"large\", rotation=90,\n",
    "                     horizontalalignment='center', verticalalignment='center',\n",
    "                     rotation_mode='anchor', bbox=props)\n",
    "\n",
    "# Tidy up plot\n",
    "ax1.xaxis.set_tick_params(labelsize=14)\n",
    "ax1.yaxis.set_tick_params(labelsize=14)\n",
    "ax1.set_ylabel('Number of events', fontsize=14)\n",
    "#ax1.set_title('All earthquakes', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "plt.show()\n",
    "fig.savefig('cum_events.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOTTING ####\n",
    "\n",
    "alt_cat = cat_preprocessed.set_index('datetime')\n",
    "# Plot events magnitude and time\n",
    "fig, ax2 = plt.subplots(figsize=(15, 10))\n",
    "for date, magnitude in zip(alt_cat.index.values, alt_cat['mag']):\n",
    "    ax2.plot([date, date],[0, magnitude],'k')\n",
    "ax2.plot(alt_cat.index.values, alt_cat['mag'], 'ko')\n",
    "ax2.set_ylim(bottom=4.0)\n",
    "ax2.xaxis.set_tick_params(labelsize=14)\n",
    "ax2.yaxis.set_tick_params(labelsize=14)\n",
    "ax2.set_xlabel('Years', fontsize=14)\n",
    "ax2.set_ylabel('Magnitude', fontsize=14)\n",
    "\n",
    "# Plot events with latitude and time\n",
    "fig, ax3 = plt.subplots(figsize=(15, 10))\n",
    "im = ax3.scatter(alt_cat.index.values, alt_cat['lat'], c=alt_cat['depth_km'], s=2*((alt_cat['mag'])**2.5), cmap='viridis')\n",
    "ax3.xaxis.set_tick_params(labelsize=14)\n",
    "ax3.yaxis.set_tick_params(labelsize=14)\n",
    "ax3.set_xlabel('Years', fontsize=14)\n",
    "ax3.set_ylabel('Latitude', fontsize=14)\n",
    "\n",
    "# produce a legend with sizes from the scatter\n",
    "kw = dict(prop=\"sizes\", num=8, color='k', fmt=\"{x:.1f}\",\n",
    "          func=lambda s: (s/2)**(1/2.5))\n",
    "legend2 = ax3.legend(*im.legend_elements(**kw),\n",
    "                    loc=\"best\", title=\"Magnitude\", bbox_to_anchor=(1.3, 1.0), fontsize=12)\n",
    "legend2.get_title().set_fontsize('12')\n",
    "\n",
    "# Add a colorbar for depth\n",
    "cbar = fig.colorbar(im, ax=ax3, label='depth[km]')\n",
    "cbar.set_label('depth[km]', fontsize=12)\n",
    "\n",
    "#plt.show()\n",
    "#fig.savefig('rate_graph.pdf', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAG-FREQ PLOT\n",
    "\n",
    "def G_R_plotting(cat, dmag):\n",
    "    \"\"\" Plot the Gutenberg-Richter plot for a catalog\n",
    "        # cat must be a dataframe containing a column for event magnitudes (col label ='mag')\n",
    "        # dmag is the magnitude step\n",
    "    \"\"\"\n",
    "    # Define magnitude bins (includes left edge of first bin and right edge of last bin)\n",
    "    mag_bins = np.arange(cat.mag.min(),cat.mag.max()+dmag,dmag)\n",
    "\n",
    "    # Count up number of earthquakes in each bin (in ascending order)\n",
    "    mag_counts = cat['mag'].value_counts(bins=mag_bins, ascending=True)\n",
    "\n",
    "    # Cumulative sum of counts to get N\n",
    "    N_ascend = mag_counts.cumsum()\n",
    "\n",
    "    # Flip N to descending magnitude to match mag_bins\n",
    "    N = N_ascend.values[::-1]\n",
    "\n",
    "    # Make sure all values of N are finite (since can't plot zero on log scale)\n",
    "    ind_finite = N > 0\n",
    "    N = N[ind_finite]\n",
    "    M = mag_bins[:-1][ind_finite]\n",
    "\n",
    "    # Estimate G-R params\n",
    "    log_N = np.log10(N)\n",
    "    polycoeffs = np.polyfit(M, log_N, 1)\n",
    "    p1 = np.poly1d(polycoeffs)\n",
    "\n",
    "    # Plot G-R plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(M, p1(M), label='$\\log(N) = $%.3f $%.3f$M' % (polycoeffs[1], polycoeffs[0]))\n",
    "    ax.plot(M, np.log10(N), 'bx')\n",
    "    ax.xaxis.set_tick_params(labelsize=14)\n",
    "    ax.yaxis.set_tick_params(labelsize=14)\n",
    "    ax.set_xlabel('Magnitude', fontsize=14)\n",
    "    ax.set_ylabel('$log(N)$', fontsize=14)\n",
    "    ax.legend(loc='best', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_R_plotting(alt_cat, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "gis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
