{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Package imports ##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "#Check shapely speedups are enabled\n",
    "from shapely import speedups\n",
    "speedups.enabled\n",
    "\n",
    "#Set geopandas settings\n",
    "#gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "#gpd.io.file.fiona.drvsupport.supported_drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Ingest data from the International Seismological Centre (ISC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970-01-01 00:00:00 2021-06-21 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ISC web search params\n",
    "start_time = datetime(1970,1,1)\n",
    "end_time = datetime(2021, 6, 21)\n",
    "min_latitude = -25\n",
    "max_latitude = -11\n",
    "min_longitude = -80\n",
    "max_longitude = -66\n",
    "min_mag = 4.5\n",
    "max_mag = None\n",
    "min_depth = None\n",
    "max_depth = None\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops! <class 'obspy.clients.fdsn.header.FDSNNoDataException'> occurred at  2016-01-01 00:00:00  -  2017-01-01 00:00:00\n",
      "FDSN Web Search failure - finalising catalog...\n",
      "Reminder: <class 'obspy.clients.fdsn.header.FDSNNoDataException'> occurred.\n",
      "FDSN Web Search failure - catalog now finalised.\n"
     ]
    }
   ],
   "source": [
    "### ISC Catalog stepwise search ###\n",
    "# ObsPy plugin breaks when searching for too many events, so we perform search in steps of 1 year\n",
    "\n",
    "# Start search\n",
    "t1 = start_time\n",
    "t2 = t1 + relativedelta(years=1)\n",
    "#print('Processing:', t1, t2)\n",
    "client = Client(\"IRIS\")\n",
    "\n",
    "# Initialise catalog\n",
    "cat_init = client.get_events(starttime=t1,endtime=t2,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "\n",
    "# Set up loop for stepwise search\n",
    "t1=t2\n",
    "t2+=relativedelta(years=1)\n",
    "#print('Beginning loop', t1, t2)\n",
    "cat = cat_init\n",
    "while t2 < end_time:\n",
    "    try:\n",
    "        #print('Loop Processing', t1, t2)\n",
    "        catalogue = client.get_events(starttime=t1,endtime=t2,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "        cat=cat.__add__(catalogue)\n",
    "        t1=t2\n",
    "        t2+=relativedelta(years=1)\n",
    "    except:\n",
    "        import sys\n",
    "        print(\"Oops!\", sys.exc_info()[0], \"occurred at \", t1, \" - \", t2)\n",
    "        print('FDSN Web Search failure - finalising catalog...')\n",
    "        final_cat = cat\n",
    "        break\n",
    "    \n",
    "# Add final time step and add to main catalog    \n",
    "assert t1 < end_time    \n",
    "try:\n",
    "    cat1 = client.get_events(starttime=t1,endtime=end_time,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "    final_cat = cat.__add__(cat1)\n",
    "    print('Final cat', final_cat)\n",
    "except:\n",
    "    import sys\n",
    "    print(\"Reminder:\", sys.exc_info()[0], \"occurred.\")\n",
    "    print('FDSN Web Search failure - catalog now finalised.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_cat.__str__(print_all=True))\n",
    "print(final_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Small test catalog ###\n",
    "client = Client(\"IRIS\")\n",
    "cat = client.get_events(starttime=UTCDateTime(\"2008-01-01\"),endtime=UTCDateTime(\"2013-01-01\"),\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cat.plot(projection=\"local\", label=None, method=\"cartopy\", title=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>depth_km</th>\n",
       "      <th>mag</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-70.93</td>\n",
       "      <td>118.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1970-01-05 06:58:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>-24.07</td>\n",
       "      <td>-66.91</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1970-01-08 06:13:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>-21.05</td>\n",
       "      <td>-68.60</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1970-01-27 15:22:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>-67.84</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1970-02-03 21:23:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>-12.72</td>\n",
       "      <td>-77.30</td>\n",
       "      <td>45.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1970-02-09 16:15:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  minute  second    lat    lon  depth_km  mag  \\\n",
       "0  1970      1    5     6      58      58 -17.31 -70.93     118.0  4.8   \n",
       "1  1970      1    8     6      13      21 -24.07 -66.91     200.0  4.8   \n",
       "2  1970      1   27    15      22      13 -21.05 -68.60     117.0  4.8   \n",
       "3  1970      2    3    21      23       9 -24.48 -67.84     113.0  4.7   \n",
       "4  1970      2    9    16      15      55 -12.72 -77.30      45.0  4.7   \n",
       "\n",
       "             datetime  \n",
       "0 1970-01-05 06:58:58  \n",
       "1 1970-01-08 06:13:21  \n",
       "2 1970-01-27 15:22:13  \n",
       "3 1970-02-03 21:23:09  \n",
       "4 1970-02-09 16:15:55  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CREATE CATALOG DATAFRAME ##\n",
    "\n",
    "# Create empty lists\n",
    "year = []\n",
    "month = []\n",
    "day = []\n",
    "hour = []\n",
    "minute = []\n",
    "second = []\n",
    "lat = []\n",
    "lon = []\n",
    "dep = []\n",
    "mag = []\n",
    "time = []\n",
    "\n",
    "# Loop over each event in the catalogue\n",
    "for event in final_cat: \n",
    "    year.append(event.origins[0].time.year)\n",
    "    month.append(event.origins[0].time.month)\n",
    "    day.append(event.origins[0].time.day)\n",
    "    hour.append(event.origins[0].time.hour)\n",
    "    minute.append(event.origins[0].time.minute)\n",
    "    second.append(event.origins[0].time.second)\n",
    "    lat.append(event.origins[0].latitude)\n",
    "    lon.append(event.origins[0].longitude)\n",
    "    dep.append(event.origins[0].depth)\n",
    "    mag.append(event.magnitudes[0].mag)\n",
    "\n",
    "# Create the dataframe\n",
    "data = pd.DataFrame(np.array([year, month, day, hour, minute, second, lat, lon, dep, mag]).T, \n",
    "             columns=[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\",\n",
    "                      \"lat\", \"lon\", \"depth_km\", \"mag\"])\n",
    "\n",
    "# Save raw data to csv\n",
    "data.to_csv('Jara_raw_catalog_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(catalog[\"mag\"],log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epidemic Type Aftershock Sequence (ETAS) Declustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAS Implementation by Marsan et al. (2017)\n",
    "\n",
    "Omori Law parameters are fixed as follows:\n",
    "$$ \\alpha = 2, p = 1, c = 10^{-3} days, \\gamma = 2 $$ <br>\n",
    "\n",
    "Algorithm can be described as follows: <br>\n",
    "1. Compute triggering rate from catalog as $\\nu(x_i,y_i,t_i)/K $ <br>\n",
    "2. Compute background rate ($ \\mu(x_i, y_i, t_i) $), under the assumption of $\\omega_i = 0.5$\n",
    "3. Compute initial total rate as $ \\lambda(x_i,y_i,t_i) = \\mu(x_i, y_i, t_i) + \\nu(x_i, y_i, t_i) $\n",
    "4. Compute $\\omega_i = \\frac{\\mu(x_i, y_i, t_i)}{\\lambda(x_i,y_i,t_i)}$\n",
    "5. Use $\\omega_i $ to compute ML estimate of K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations\n",
    "$$ \\lambda(x,y,t) = \\mu(x,y,t) + \\nu(x,y,t) $$ <br>\n",
    "where $\\lambda(x,y,t)$ is the total seismicity rate, $\\mu(x,y,t)$ is the background seismicity rate, and $\\nu(x,y,t)$ is the triggering rate <br>\n",
    "\n",
    "The triggering rate: <br>\n",
    "$$ \\nu(x,y,t) = \\displaystyle\\sum_{i/t_i < t}^{} \\frac{Ke^{\\alpha m_i}}{(t+c-t_i)} \\times \\frac{\\gamma - 1}{2\\pi} \\times \\frac{L_i^{\\gamma-1}}{\\left((x-x_i)^2 + (y-y_i)^2 + L_i^2 \\right )^{\\frac{\\gamma + 1}{2}}} $$ <br>\n",
    "\n",
    "The background rate: <br>\n",
    "$$ \\mu(x,y,t) = \\displaystyle\\sum_{i}^{} \\omega_i e^{-\\sqrt{(x-x_i)^2 + (y-y_i)^2}/\\ell} e^{-|t-t_i|/\\tau} \\times \\frac{1}{2 \\pi \\ell^2 a_i} $$ <br>\n",
    "\n",
    "$$ a_i = 2\\tau - \\tau \\left( e^{-\\frac{t_s - t_i}{\\tau}} - e^{\\frac{t_s - t_i}{\\tau}} \\right) $$ <br>\n",
    "where $t_s, t_e$ are the start and end times of the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the catalog for processing\n",
    "def prep_catalog(cat_init, cat_start, cat_end): \n",
    "    \"\"\" Loads and prepares the catalog for further processing\n",
    "    # input cat_init needs to be a file path to a CSV document containing labelled columns:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    \"\"\"\n",
    "    # Load catalog from file:\n",
    "    cat = pd.read_csv(cat_init, index_col=0)\n",
    "    \n",
    "    # Apply datetimes\n",
    "    cat[\"datetime\"] = pd.to_datetime(cat[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "    #Fix dtypes\n",
    "    cat = cat.infer_objects()\n",
    "    cat.dtypes\n",
    "    cat.loc[:, 'depth_km'] *=0.001\n",
    "\n",
    "    # Define a geodataframe using the EQ catalog from above\n",
    "    cat_gdf = gpd.GeoDataFrame(cat, geometry=gpd.points_from_xy(cat.lon, cat.lat))\n",
    "    cat_gdf = cat_gdf.set_crs(\"EPSG:4326\")\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    cat_gdf['lat_rad'] = np.radians(cat_gdf['lat'])\n",
    "    cat_gdf['lon_rad'] = np.radians(cat_gdf['lon'])\n",
    "    # Compute the time difference between event occurrence times and the start and end times of the catalog, in days\n",
    "    cat_gdf['t_diff_e'] = (1./(24.*60.*60.))*((cat_gdf['datetime']- cat_end).dt.total_seconds())\n",
    "    cat_gdf['t_diff_s'] = (1./(24.*60.*60.))*((cat_gdf['datetime'] - cat_start).dt.total_seconds())\n",
    "    return cat_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    return np.square(np.sin(theta / 2))\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(\n",
    "        np.sqrt(\n",
    "            hav(lat_rad_1 - lat_rad_2)\n",
    "            + np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic length/rupture radius in km\n",
    "def L_i(m, L_0):\n",
    "    return L_0*np.power(10, 0.5*(m-2))\n",
    "\n",
    "# a coefficients\n",
    "# Need to fix abs values in exponents\n",
    "def a_coeff(t_diff_e,t_diff_s,tau): # tau is the temporal smoothing param; t_diff_e=t_catend-tevent; t_diff_s=t_catstart-tevent\n",
    "    a = 2*tau - tau*(np.exp(-(np.abs(t_diff_s)/(tau))) - np.exp(-(np.abs(t_diff_e)/(tau)))) # times in days\n",
    "    return a\n",
    "\n",
    "# Calculate triggering rate\n",
    "def nu_calc(c, alpha, p, gamma, K, m, time_diffs, r_sq, L_0):\n",
    "    # Numerical calculations for nu\n",
    "    T1 = K*np.exp(alpha*m)/np.power((time_diffs), p)\n",
    "    #T1 = np.exp(alpha*m)\n",
    "    T2 = (gamma-1)/2*np.pi\n",
    "    T3 = np.power(L_i(m, L_0), (gamma-1))\n",
    "    T4 = np.power((r_sq + np.power(L_i(m, L_0),2)), (gamma+1)/2)\n",
    "    return T1*T2*(T3/T4)\n",
    "    \n",
    "# Calculate the background rate:\n",
    "def mu_calc(r_sq, t_diff, omega, tau, l, a_coeffs): # tau, l are the temporal and spatial smoothing params\n",
    "    T1 = omega*np.exp(-np.sqrt(r_sq)/l)\n",
    "    T2 = np.exp(-np.abs(t_diff)/tau) # times need to be in days\n",
    "    T3 = 1/(2*np.pi*(l**2)*a_coeffs)\n",
    "    lam = T1*T2*T3\n",
    "    return lam\n",
    "\n",
    "# Triggering rate from catalog\n",
    "def nuK(catalog, c, alpha, p, gamma, K, L_0):\n",
    "    \"\"\" Calculates nu(xi,yi,ti) - triggering rate at time and place of each event in catalog\n",
    "    # L_0 is the reference length\n",
    "    # If K=1, then function actually returns nu(xi,yi,ti)/K\n",
    "    \"\"\"\n",
    "    func_start = datetime.now() # time the function\n",
    "    cat = catalog.copy(deep=True)\n",
    "    cat['nuK'] = 0.0\n",
    "    #print('Looping through catalog...')\n",
    "    for triggered in cat.head(-1).itertuples():\n",
    "        # get values of source event\n",
    "        ttime = triggered.datetime\n",
    "        #print('Triggered event time:', ttime)\n",
    "        tlatrad = triggered.lat_rad\n",
    "        tlonrad = triggered.lon_rad\n",
    "        potential_triggers = cat.loc[cat[\"datetime\"] < ttime]\n",
    "        potential_triggers['c'] = c\n",
    "        potential_triggers['t_diffs'] = (1./(24.*60.*60.))*(ttime - potential_triggers['datetime']).dt.total_seconds()\n",
    "        potential_triggers['t_denom'] = potential_triggers['t_diffs'] + potential_triggers['c']\n",
    "        #print(potential_triggers['t_denom'])\n",
    "        #print(potential_triggers)\n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        potential_triggers['r_squared'] = np.square(haversine(tlatrad,potential_triggers['lat_rad'],tlonrad,potential_triggers['lon_rad']))\n",
    "        #print(len(potential_triggers['mag']), len(potential_triggers['r_squared']), len(potential_triggers['t_denom']))\n",
    "        # Calculate triggering rate nu for each event i.e. nu(xi,yi,ti)\n",
    "        nuK_array = nu_calc(c, alpha, p, gamma, K, potential_triggers['mag'], potential_triggers['t_denom'], potential_triggers['r_squared'], L_0)\n",
    "        cat.loc[triggered.Index, 'nuK'] = nuK_array.sum()\n",
    "    print('    took', (datetime.now() - func_start), 'to compute nu(xi,yi,ti)/K \\n')\n",
    "    return cat\n",
    "\n",
    "# Background rate from catalog\n",
    "def mu(catalog, tau, l):\n",
    "    cat = catalog.copy(deep=True)\n",
    "    cat['mu_i'] = 0.0\n",
    "    #cat['omega_i'] = 0.5\n",
    "    for event in cat.head(-1).itertuples():\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        evtime = event.datetime\n",
    "        #print(evtime)\n",
    "        #print('Triggered event time:', ttime)\n",
    "        evlatrad = event.lat_rad\n",
    "        evlonrad = event.lon_rad\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((evtime - temp_cat['datetime']).dt.total_seconds())\n",
    "        \n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(evlatrad,temp_cat['lat_rad'],evlonrad,temp_cat['lon_rad']))\n",
    "        temp_cat['a_coeffs'] = a_coeff(temp_cat['t_diff_e'], temp_cat['t_diff_s'], tau)\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_i'],tau,l,temp_cat['a_coeffs'])\n",
    "        #print(temp_cat)\n",
    "        cat.loc[event.Index, 'mu_i'] = temp_cat['mu_indiv'].sum()\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returned_cat = nuK(catalog_gdf_d_filter, c=0.001, alpha=2, gamma=2, K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(returned_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = datetime(1970,1,1)\n",
    "#end_time = datetime(2021, 6, 21)\n",
    "#cat_stats = mu(returned_cat, 100.0, 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cat_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameter F_i for individual events\n",
    "def F_i(alpha, t_diff, c, m):\n",
    "    return np.exp(alpha*m)*(np.log(t_diff) - np.log(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declustering function ##\n",
    "def decluster(path, cat_start, cat_end, tau, l, c, alpha, p, gamma, L_0, atol):\n",
    "    \"\"\"\n",
    "    # Function to estimate normalisation parameter K and best estimates for omega\n",
    "    # path must be a filepath (str) to a CSV file containing an output ISC search result with cols:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    # tau, l are temporal and spatial smoothing params, to be given in days and km\n",
    "    # c, alpha, gamma are Omori-Utsu Law and power spectral density constants\n",
    "    # L_0 is the reference rupture length\n",
    "    # atol is the tolerance level for convergence in the MLE estimate of K\n",
    "    \"\"\"\n",
    "    calc_start = datetime.now() # time the function\n",
    "    assert cat_start < cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    \n",
    "    print('\\nCatalog start time: ', cat_start, ' Catalog end time:', cat_end, \n",
    "          '\\nNumber of events in catalog: ', len(cat.index),\n",
    "          '\\nSmoothing time:', tau, 'days '\n",
    "          'Smoothing length: ', l, 'km ', '\\nOmori Law constants: \\nc:',  c, 'days', ' alpha: ', alpha, ' p: ', \n",
    "          p, ' gamma:', gamma, '\\nReference rupture length:', L_0, 'km')\n",
    "    \n",
    "    # Load catalog from file and prepare for processing\n",
    "    print('Preparing catalog for processing...')\n",
    "    cat_preprocessed = prep_catalog(path, cat_start, cat_end)\n",
    "    \n",
    "    print('Now processing catalog...')\n",
    "    # Now calculate nu(xi,yi,ti)/K for all events:\n",
    "    print('Calculating initial triggering rate...')\n",
    "    nuK_cat = nuK(cat_preprocessed, c, alpha, p, gamma, 1.0, L_0)\n",
    "    \n",
    "    nuK_cat['omega_i'] = 0.5\n",
    "    \n",
    "    # Calculate background rates at time and place of each event, assuming omega is 0.5\n",
    "    print('Estimating a priori background rates...')\n",
    "    initial_mu_cat = mu(nuK_cat, tau, l)\n",
    "    \n",
    "    # initial_mu_cat should now contain both a nu and a mu for each event\n",
    "    # Now compute updated omega:\n",
    "    initial_mu_cat['lambda_i'] = initial_mu_cat['mu_i'] + initial_mu_cat['nuK']\n",
    "    initial_mu_cat['omega_i'] = initial_mu_cat['mu_i'] / initial_mu_cat['nuK']\n",
    "    \n",
    "    # Initialise some columns for the MLE estimate\n",
    "    initial_mu_cat['c'] = c # in days\n",
    "    initial_mu_cat['t_diffs'] = (1./(24.*60.*60.))*(cat_end - initial_mu_cat['datetime']).dt.total_seconds() # in days\n",
    "    initial_mu_cat['t_quantity'] = initial_mu_cat['t_diffs'] + initial_mu_cat['c']\n",
    "    initial_mu_cat['K_num'] = 1 - (initial_mu_cat['omega_i'])\n",
    "    initial_mu_cat['F_i'] = F_i(alpha, initial_mu_cat['t_quantity'], c, initial_mu_cat['mag'])\n",
    "    \n",
    "    # Initialise K - an initial guess\n",
    "    K = initial_mu_cat['K_num'].sum() / initial_mu_cat['F_i'].sum()\n",
    "    fevals = 0 # record number of function evaluations so we can later compare methods\n",
    "    K_prev = K + 2*atol # initialise the previous K simply so that while loop argument is initially true\n",
    "    updated_cat = initial_mu_cat.copy(deep=True)\n",
    "    print('Starting iteration for MLE estimate of K')\n",
    "    while abs(K - K_prev) > atol:\n",
    "        K_prev = K\n",
    "        \n",
    "        # Compute updated nu based on new value of K\n",
    "        updated_cat = nuK(updated_cat, c, alpha, p, gamma, K, L_0)\n",
    "        \n",
    "        # Now compute updated omega:\n",
    "        updated_cat['lambda_i'] = updated_cat['mu_i'] + updated_cat['nuK']\n",
    "        updated_cat['omega_i'] = updated_cat['mu_i'] / updated_cat['nuK']\n",
    "        updated_cat = mu(updated_cat, tau, l)\n",
    "        updated_cat['K_num'] = 1 - (updated_cat['omega_i'])\n",
    "        \n",
    "        # Compute K using updated omega:\n",
    "        K =  updated_cat['K_num'].sum() / updated_cat['F_i'].sum()\n",
    "        fevals += 1\n",
    "        #print('Current iteration solution: ',K)\n",
    "    print('The final value of K is', K)\n",
    "    print('\\n', fevals, 'function evaluations were required for K convergence')\n",
    "    \n",
    "    # Using final K value calculate a final triggering rate nu(xi,yi,ti):\n",
    "    final_cat = nuK(updated_cat, c, alpha, p, gamma, K, L_0)\n",
    "    \n",
    "    # now the final catalog should contain the correct omegas, which can be used to estimate a background seismicity rate curve\n",
    "    # Check this visually using a histogram\n",
    "    plt.hist(final_cat[\"omega_i\"],log=True)\n",
    "    print('    took', (datetime.now() - calc_start), 'for declustering \\n')\n",
    "    return final_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_start = datetime(1970,1,1)\n",
    "cat_end = datetime(2021, 6, 21)\n",
    "cat = catalog_gdf\n",
    "tau=100; l=100; c=0.001; alpha=2; p=1; gamma=2; L_0=1.78; atol=0.01\n",
    "print('\\nCatalog start time: ', cat_start, ' Catalog end time:', cat_end, \n",
    "          '\\nNumber of events in catalog: ', len(cat.index),\n",
    "          '\\nSmoothing time:', tau, 'days '\n",
    "          'Smoothing length: ', l, 'km ', '\\nOmori Law constants: \\nc:',  c, 'days', ' alpha: ', alpha, ' p: ', \n",
    "          p, ' gamma:', gamma, '\\nReference rupture length:', L_0, 'km')\n",
    "\n",
    "# Preprocess\n",
    "cat_preprocessed = prep_catalog(cat, cat_start, cat_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuK_cat = nuK(cat_preprocessed, c, alpha, p, gamma, 1.0, L_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuK_cat.loc[:, 'omega_i'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_mu_cat = mu(nuK_cat, tau, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_calc(r_sq, t_diff, omega, tau, l, a_coeffs)\n",
    "print(mu_calc(729811.462763, -35.386771, 0.5, 100., 100., 132.751616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_preprocessed.head()\n",
    "#print(nuK_cat)\n",
    "#print(initial_mu_cat)\n",
    "initial_mu_cat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final estimate of background seismicity rate ##\n",
    "def mu_final(x,y,cat_start, cat_end, cat, tau, l):\n",
    "    #########################################################\n",
    "    # Function computes timeseries of the background seismicity rate\n",
    "    # x,y refer to a spatial reference point - should be the centroid of the study area\n",
    "    # Function will build an array of datetime objects with a timestep of 1 day using the cat_start, cat_end times\n",
    "    # catalog should contain omega, a_coeff values for each event - the output of func decluster\n",
    "    #########################################################\n",
    "    assert t_cat_start < t_cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    \n",
    "    # Time steps for calculating background rate\n",
    "    times = np.arange(start_time, end_time, timedelta(days=1)).astype(datetime)\n",
    "    mu_t_series = []\n",
    "    \n",
    "    # Compute background rate at each time step\n",
    "    for t_step in times:\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((t_step - temp_cat['datetime']).dt.total_seconds())\n",
    "        \n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(x, temp_cat['lat_rad'], y, temp_cat['lon_rad']))\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_i'],tau,l,temp_cat['a_coeffs'])\n",
    "        mu_t_series.append(temp_cat['mu_indiv'].sum())\n",
    "    return times, mu_t_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declustering implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISC web search params\n",
    "start_time = datetime(1970,1,1)\n",
    "end_time = datetime(2021, 6, 21)\n",
    "min_latitude = -25\n",
    "max_latitude = -11\n",
    "min_longitude = -80\n",
    "max_longitude = -66\n",
    "min_mag = 4.5\n",
    "max_mag = None\n",
    "min_depth = None\n",
    "max_depth = None\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>depth_km</th>\n",
       "      <th>mag</th>\n",
       "      <th>datetime</th>\n",
       "      <th>geometry</th>\n",
       "      <th>lat_rad</th>\n",
       "      <th>lon_rad</th>\n",
       "      <th>t_diff_e</th>\n",
       "      <th>t_diff_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>-17.31</td>\n",
       "      <td>-70.93</td>\n",
       "      <td>118.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1970-01-05 06:58:58</td>\n",
       "      <td>POINT (-70.93000 -17.31000)</td>\n",
       "      <td>-0.302116</td>\n",
       "      <td>-1.237962</td>\n",
       "      <td>-18794.709051</td>\n",
       "      <td>4.290949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>-24.07</td>\n",
       "      <td>-66.91</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1970-01-08 06:13:21</td>\n",
       "      <td>POINT (-66.91000 -24.07000)</td>\n",
       "      <td>-0.420101</td>\n",
       "      <td>-1.167800</td>\n",
       "      <td>-18791.740729</td>\n",
       "      <td>7.259271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>-21.05</td>\n",
       "      <td>-68.60</td>\n",
       "      <td>117.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1970-01-27 15:22:13</td>\n",
       "      <td>POINT (-68.60000 -21.05000)</td>\n",
       "      <td>-0.367392</td>\n",
       "      <td>-1.197296</td>\n",
       "      <td>-18772.359572</td>\n",
       "      <td>26.640428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>-24.48</td>\n",
       "      <td>-67.84</td>\n",
       "      <td>113.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1970-02-03 21:23:09</td>\n",
       "      <td>POINT (-67.84000 -24.48000)</td>\n",
       "      <td>-0.427257</td>\n",
       "      <td>-1.184031</td>\n",
       "      <td>-18765.108924</td>\n",
       "      <td>33.891076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>-12.72</td>\n",
       "      <td>-77.30</td>\n",
       "      <td>45.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1970-02-09 16:15:55</td>\n",
       "      <td>POINT (-77.30000 -12.72000)</td>\n",
       "      <td>-0.222006</td>\n",
       "      <td>-1.349140</td>\n",
       "      <td>-18759.322280</td>\n",
       "      <td>39.677720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  minute  second    lat    lon  depth_km  mag  \\\n",
       "0  1970      1    5     6      58      58 -17.31 -70.93     118.0  4.8   \n",
       "1  1970      1    8     6      13      21 -24.07 -66.91     200.0  4.8   \n",
       "2  1970      1   27    15      22      13 -21.05 -68.60     117.0  4.8   \n",
       "3  1970      2    3    21      23       9 -24.48 -67.84     113.0  4.7   \n",
       "4  1970      2    9    16      15      55 -12.72 -77.30      45.0  4.7   \n",
       "\n",
       "             datetime                     geometry   lat_rad   lon_rad  \\\n",
       "0 1970-01-05 06:58:58  POINT (-70.93000 -17.31000) -0.302116 -1.237962   \n",
       "1 1970-01-08 06:13:21  POINT (-66.91000 -24.07000) -0.420101 -1.167800   \n",
       "2 1970-01-27 15:22:13  POINT (-68.60000 -21.05000) -0.367392 -1.197296   \n",
       "3 1970-02-03 21:23:09  POINT (-67.84000 -24.48000) -0.427257 -1.184031   \n",
       "4 1970-02-09 16:15:55  POINT (-77.30000 -12.72000) -0.222006 -1.349140   \n",
       "\n",
       "       t_diff_e   t_diff_s  \n",
       "0 -18794.709051   4.290949  \n",
       "1 -18791.740729   7.259271  \n",
       "2 -18772.359572  26.640428  \n",
       "3 -18765.108924  33.891076  \n",
       "4 -18759.322280  39.677720  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load catalog from file:\n",
    "cat_start = datetime(1970,1,1)\n",
    "cat_end = datetime(2021, 6, 21)\n",
    "fname = 'Jara_raw_catalog_data.csv'\n",
    "cat_preprocessed = prep_catalog(fname, cat_start, cat_end)\n",
    "cat_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Catalog start time:  1970-01-01 00:00:00  Catalog end time: 2021-06-21 00:00:00 \n",
      "Number of events in catalog:  6311 \n",
      "Smoothing time: 100 days Smoothing length:  100 km  \n",
      "Omori Law constants: \n",
      "c: 0.001 days  alpha:  2  p:  1  gamma: 2 \n",
      "Reference rupture length: 1.78 km\n"
     ]
    }
   ],
   "source": [
    "cat_start = datetime(1970,1,1)\n",
    "cat_end = datetime(2021, 6, 21)\n",
    "#cat = catalog_gdf\n",
    "tau=100; l=100; c=0.001; alpha=2; p=1; gamma=2; L_0=1.78; atol=0.01\n",
    "print('\\nCatalog start time: ', cat_start, ' Catalog end time:', cat_end, \n",
    "          '\\nNumber of events in catalog: ', len(cat.index),\n",
    "          '\\nSmoothing time:', tau, 'days '\n",
    "          'Smoothing length: ', l, 'km ', '\\nOmori Law constants: \\nc:',  c, 'days', ' alpha: ', alpha, ' p: ', \n",
    "          p, ' gamma:', gamma, '\\nReference rupture length:', L_0, 'km')\n",
    "\n",
    "# Preprocess\n",
    "cat_preprocessed = prep_catalog(cat_gdf, cat_start, cat_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_attempt = decluster(catalog_gdf, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, p=1, gamma=2, L_0=1.78, atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spatial reference point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "lat_point_list = [min_latitude, max_latitude, min_latitude]\n",
    "lon_point_list = [min_longitude, max_longitude, min_longitude]\n",
    "\n",
    "search_area = Polygon(zip(lon_point_list, lat_point_list))\n",
    "crs = {'init': 'epsg:4326'}\n",
    "search_area = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])       \n",
    "#print(polygon.geometry)\n",
    "\n",
    "x_cent = np.radians(search_area.centroid.x)\n",
    "y_cent = np.radians(search_area.centroid.y)\n",
    "print('Geographic ref point: ', x_cent, y_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECLUSTERING ###\n",
    "\n",
    "# Separate catalog into deep and shallow following Jara et al. (2017)\n",
    "catalog_shallow = catalog_gdf.loc[catalog_gdf['depth_km'] < 40.0]\n",
    "catalog_deep = catalog_gdf.loc[catalog_gdf['depth_km'] > 80.0]\n",
    "declustered_shallow_cat = decluster(catalog_shallow, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "declustered_deep_cat = decluster(catalog_deep, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "\n",
    "# And calculate rates:\n",
    "times, deep_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_deep_cat, tau=100, l=100)\n",
    "times, shallow_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_shallow_cat, tau=100, l=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOTTING ####\n",
    "\n",
    "# Counting\n",
    "new_cat = catalog_gdf.drop(columns=['geometry'])\n",
    "new_cat = new_cat.set_index('datetime')\n",
    "new_cat['count'] = 1.0\n",
    "new_cat = new_cat.cumsum()\n",
    "\n",
    "# Fit quadratic\n",
    "from numpy.polynomial import Polynomial as P\n",
    "time_diffs = (1./(24.*60.*60.))*(new_cat.index - start_time).total_seconds()\n",
    "counts = new_cat['count'].values\n",
    "p = P.fit(time_diffs,counts,2)\n",
    "\n",
    "\n",
    "# Plot:\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.plot(new_cat.index.values, new_cat['count'])\n",
    "ax.plot(new_cat.index.values, p(time_diffs), 'r', label='quadratic fit')\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)\n",
    "ax.set_ylabel('Number of events', fontsize=14)\n",
    "ax.set_title('All earthquakes', fontsize=16)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "plt.show()\n",
    "#fig.savefig('rate_graph.pdf', dpi=200, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "gis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
