{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AeronavFAA': 'r',\n",
       " 'ARCGEN': 'r',\n",
       " 'BNA': 'raw',\n",
       " 'DXF': 'raw',\n",
       " 'CSV': 'raw',\n",
       " 'OpenFileGDB': 'r',\n",
       " 'ESRIJSON': 'r',\n",
       " 'ESRI Shapefile': 'raw',\n",
       " 'GeoJSON': 'rw',\n",
       " 'GeoJSONSeq': 'rw',\n",
       " 'GPKG': 'rw',\n",
       " 'GML': 'raw',\n",
       " 'GPX': 'raw',\n",
       " 'GPSTrackMaker': 'raw',\n",
       " 'Idrisi': 'r',\n",
       " 'MapInfo File': 'raw',\n",
       " 'DGN': 'raw',\n",
       " 'PCIDSK': 'r',\n",
       " 'S57': 'r',\n",
       " 'SEGY': 'r',\n",
       " 'SUA': 'r',\n",
       " 'TopoJSON': 'r',\n",
       " 'KML': 'rw'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Package imports ##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "#Check shapely speedups are enabled\n",
    "from shapely import speedups\n",
    "speedups.enabled\n",
    "\n",
    "#Set geopandas settings\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Ingest data from the International Seismological Centre (ISC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISC web search params\n",
    "start_time = datetime(1970,1,1)\n",
    "end_time = datetime(2021, 6, 21)\n",
    "min_latitude = -25\n",
    "max_latitude = -11\n",
    "min_longitude = -80\n",
    "max_longitude = -66\n",
    "min_mag = 4.5\n",
    "max_mag = None\n",
    "min_depth = None\n",
    "max_depth = None\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ISC Catalog stepwise search ###\n",
    "# ObsPy plugin breaks when searching for too many events, so we perform search in steps of 1 year\n",
    "# Start search\n",
    "t1 = start_time\n",
    "t2 = t1 + relativedelta(years=1)\n",
    "#print('Processing:', t1, t2)\n",
    "client = Client(\"IRIS\")\n",
    "# Initialise catalog\n",
    "cat_init = client.get_events(starttime=t1,endtime=t2,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "# Set up loop for stepwise search\n",
    "t1=t2\n",
    "t2+=relativedelta(years=1)\n",
    "#print('Beginning loop', t1, t2)\n",
    "cat = cat_init\n",
    "while t2 < end_time:\n",
    "    try:\n",
    "        #print('Loop Processing', t1, t2)\n",
    "        catalogue = client.get_events(starttime=t1,endtime=t2,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "        cat=cat.__add__(catalogue)\n",
    "        t1=t2\n",
    "        t2+=relativedelta(years=1)\n",
    "    except:\n",
    "        import sys\n",
    "        print(\"Oops!\", sys.exc_info()[0], \"occurred at \", t1, \" - \", t2)\n",
    "        print('FDSN Web Search failure - finalising catalog...')\n",
    "        final_cat = cat\n",
    "        break\n",
    "    \n",
    "# Add final time step and add to main catalog    \n",
    "assert t1 < end_time    \n",
    "try:\n",
    "    cat1 = client.get_events(starttime=t1,endtime=end_time,\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "    final_cat = cat.__add__(cat1)\n",
    "    print('Final cat', final_cat)\n",
    "except:\n",
    "    import sys\n",
    "    print(\"Reminder:\", sys.exc_info()[0], \"occurred.\")\n",
    "    print('FDSN Web Search failure - catalog now finalised.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_cat.__str__(print_all=True))\n",
    "print(final_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Small test catalog ###\n",
    "client = Client(\"IRIS\")\n",
    "cat = client.get_events(starttime=UTCDateTime(\"2008-01-01\"),endtime=UTCDateTime(\"2013-01-01\"),\n",
    "                              minlatitude=min_latitude,maxlatitude=max_latitude,\n",
    "                              minlongitude=min_longitude,maxlongitude=max_longitude,\n",
    "                              minmagnitude=min_mag, maxmagnitude=max_mag,\n",
    "                              mindepth=min_depth, maxdepth=max_depth, catalog=\"ISC\", orderby=\"time-asc\")\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cat.plot(projection=\"local\", label=None, method=\"cartopy\", title=\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE CATALOG DATAFRAME ##\n",
    "# Create empty lists\n",
    "year = []\n",
    "month = []\n",
    "day = []\n",
    "hour = []\n",
    "minute = []\n",
    "second = []\n",
    "lat = []\n",
    "lon = []\n",
    "dep = []\n",
    "mag = []\n",
    "time = []\n",
    "\n",
    "# Loop over each event in the catalogue\n",
    "for event in final_cat: \n",
    "    year.append(event.origins[0].time.year)\n",
    "    month.append(event.origins[0].time.month)\n",
    "    day.append(event.origins[0].time.day)\n",
    "    hour.append(event.origins[0].time.hour)\n",
    "    minute.append(event.origins[0].time.minute)\n",
    "    second.append(event.origins[0].time.second)\n",
    "    lat.append(event.origins[0].latitude)\n",
    "    lon.append(event.origins[0].longitude)\n",
    "    dep.append(event.origins[0].depth)\n",
    "    mag.append(event.magnitudes[0].mag)\n",
    "\n",
    "data = pd.DataFrame(np.array([year, month, day, hour, minute, second, lat, lon, dep, mag]).T, \n",
    "             columns=[\"year\", \"month\", \"day\", \"hour\", \"minute\", \"second\",\n",
    "                      \"lat\", \"lon\", \"depth_km\", \"mag\"])\n",
    "data[\"datetime\"] = pd.to_datetime(data[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "data.head()\n",
    "\n",
    "#Fix dtypes\n",
    "catalog = data.infer_objects()\n",
    "catalog.dtypes\n",
    "catalog.loc[:, 'depth_km'] *=0.001\n",
    "catalog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare geodataframe ##\n",
    "# Define a geodataframe using the EQ catalog from above\n",
    "catalog_gdf = gpd.GeoDataFrame(catalog, geometry=gpd.points_from_xy(catalog.lon, catalog.lat))\n",
    "catalog_gdf.head()\n",
    "catalog_gdf = targets.set_crs(\"EPSG:4326\")\n",
    "#catalog_gdf.head(-1)\n",
    "catalog_gdf.count()\n",
    "#print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(catalog[\"mag\"],log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epidemic Type Aftershock Sequence (ETAS) Declustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haversine formula for computing spherical distances\n",
    "def hav(theta):\n",
    "    return np.square(np.sin(theta / 2))\n",
    "def haversine(lat_rad_1, lat_rad_2, lon_rad_1, lon_rad_2, earth_radius=6.3781e3):\n",
    "    # to calculate distance on a sphere\n",
    "    d = 2 * earth_radius * np.arcsin(\n",
    "        np.sqrt(\n",
    "            hav(lat_rad_1 - lat_rad_2)\n",
    "            + np.cos(lat_rad_1)\n",
    "            * np.cos(lat_rad_2)\n",
    "            * hav(lon_rad_1 - lon_rad_2)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETAS Implementation by Marsan et al. (2017)\n",
    "\n",
    "Omori Law parameters are fixed as follows:\n",
    "$$ \\alpha = 2, p = 1, c = 10^{-3} days, \\gamma = 2 $$ <br>\n",
    "\n",
    "Algorithm can be described as follows: <br>\n",
    "1. Compute triggering rate from catalog as $\\nu(x_i,y_i,t_i)/K $ <br>\n",
    "2. Compute background rate ($ \\mu(x_i, y_i, t_i) $), under the assumption of $\\omega_i = 0.5$\n",
    "3. Compute initial total rate as $ \\lambda(x_i,y_i,t_i) = \\mu(x_i, y_i, t_i) + \\nu(x_i, y_i, t_i) $\n",
    "4. Compute $\\omega_i = \\frac{\\mu(x_i, y_i, t_i)}{\\lambda(x_i,y_i,t_i)}$\n",
    "5. Use $\\omega_i $ to compute ML estimate of K\n",
    "6. Finally, use computed K to find the background rate $ \\mu(x, y, t) $ using centroid of study area as reference point, using time steps of 1 day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations\n",
    "$$ \\lambda(x,y,t) = \\mu(x,y,t) + \\nu(x,y,t) $$ <br>\n",
    "where $\\lambda(x,y,t)$ is the total seismicity rate, $\\mu(x,y,t)$ is the background seismicity rate, and $\\nu(x,y,t)$ is the triggering rate <br>\n",
    "\n",
    "The triggering rate: <br>\n",
    "$$ \\nu(x,y,t) = \\displaystyle\\sum_{i/t_i < t}^{} \\frac{Ke^{\\alpha m_i}}{(t+c-t_i)} \\times \\frac{\\gamma - 1}{2\\pi} \\times \\frac{L_i^{\\gamma-1}}{\\left((x-x_i)^2 + (y-y_i)^2 + L_i^2 \\right )^{\\frac{\\gamma + 1}{2}}} $$ <br>\n",
    "\n",
    "The background rate: <br>\n",
    "$$ \\mu(x,y,t) = \\displaystyle\\sum_{i}^{} \\omega_i e^{-\\sqrt{(x-x_i)^2 + (y-y_i)^2}/\\ell} e^{-|t-t_i|/\\tau} \\times \\frac{1}{2 \\pi \\ell^2 a_i} $$ <br>\n",
    "\n",
    "$$ a_i = 2\\tau - \\tau \\left( e^{-\\frac{t_s - t_i}{\\tau}} - e^{\\frac{t_s - t_i}{\\tau}} \\right) $$ <br>\n",
    "where $t_s, t_e$ are the start and end times of the catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the catalog for processing\n",
    "def prep_catalog(cat_init, cat_start, cat_start): \n",
    "    ############################################\n",
    "    # Prepares the catalog for further processing\n",
    "    # input cat_init needs to be a pandas dataframe, preferably a geodataframe with cols being:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag, datetime\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    ###########################################\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    cat_init['lat_rad'] = np.radians(cat_init['lat'])\n",
    "    cat_init['lon_rad'] = np.radians(cat_init['lon'])\n",
    "    # Compute the time difference between event occurrence times and the start and end times of the catalog, in days\n",
    "    cat_init['t_diff_e'] = (1./(24.*60.*60.))*((cat_start - cat_init['datetime']).dt.total_seconds())\n",
    "    cat_init['t_diff_s'] = (1./(24.*60.*60.))*((cat_start - cat_init['datetime']).dt.total_seconds())\n",
    "    return cat_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characteristic length/rupture radius in km\n",
    "def L_i(m):\n",
    "    return np.power(10, 0.5*(m-2))\n",
    "\n",
    "# a coefficients\n",
    "def a_coeff(t_diff_e,t_diff_s,tau): # tau is the temporal smoothing param; t_diff_e=t_catend-tevent; t_diff_s=t_catstart-tevent\n",
    "    a = 2*tau - tau*(np.exp(-(t_diff_s/(tau))) - np.exp(-(t_diff_e/(tau)))) # times in days\n",
    "    return a\n",
    "\n",
    "# Calculate triggering rate\n",
    "def nu_calc(c, alpha, gamma, K, m, time_diffs, r_sq):\n",
    "    # Numerical calculations for nu\n",
    "    T1 = K*np.exp(alpha*m)/(time_diffs)\n",
    "    #T1 = np.exp(alpha*m)\n",
    "    T2 = (gamma-1)/2*np.pi\n",
    "    T3 = np.power(L_i(m), (gamma-1))\n",
    "    T4 = np.power((r_sq + np.power(L_i(m),2)), (gamma+1)/2)\n",
    "    return T1*T2*(T3/T4)\n",
    "    \n",
    "# Calculate the background rate:\n",
    "def mu_calc(r_sq, t_diff, omega, tau, l, a_coeffs): # tau, l are the temporal and spatial smoothing params\n",
    "    T1 = omega*np.exp(-np.sqrt(r_sq)/l)\n",
    "    T2 = np.exp(-np.abs(t_diff)/tau) # times need to be in days\n",
    "    T3 = 1/(2*np.pi*(l**2)*a_coeffs)\n",
    "    lam = T1*T2*T3\n",
    "    return lam\n",
    "\n",
    "# Triggering rate from catalog\n",
    "def nuK(catalog, c, alpha, gamma, K):\n",
    "    # Calculates nu(xi,yi,ti) - triggering rate at time and place of each event in catalog\n",
    "    # If K=1, then function actually returns nu(xi,yi,ti)/K\n",
    "    calc_start = datetime.now() # time the function\n",
    "    cat = catalog.copy(deep=True)\n",
    "    # translate target lat, lon to radians for spherical distance calculation\n",
    "    #cat['lat_rad'] = np.radians(cat['lat'])\n",
    "    #cat['lon_rad'] = np.radians(cat['lon'])\n",
    "    cat['nuK'] = 0.0\n",
    "    print('Looping through catalog...')\n",
    "    for triggered in cat.head(-1).itertuples():\n",
    "        # get values of source event\n",
    "        ttime = triggered.datetime\n",
    "        #print('Triggered event time:', ttime)\n",
    "        tlatrad = triggered.lat_rad\n",
    "        tlonrad = triggered.lon_rad\n",
    "        potential_triggers = cat.loc[cat[\"datetime\"] < ttime]\n",
    "        potential_triggers['c'] = c\n",
    "        potential_triggers['t_diffs'] = (1./(24.*60.*60.))*(ttime - potential_triggers['datetime']).dt.total_seconds()\n",
    "        potential_triggers['t_denom'] = potential_triggers['t_diffs'] + potential_triggers['c']\n",
    "        #print(potential_triggers['t_denom'])\n",
    "        #print(potential_triggers)\n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        potential_triggers['r_squared'] = np.square(haversine(tlatrad,potential_triggers['lat_rad'],tlonrad,potential_triggers['lon_rad']))\n",
    "        #print(len(potential_triggers['mag']), len(potential_triggers['r_squared']), len(potential_triggers['t_denom']))\n",
    "        # Calculate triggering rate nu for each event i.e. nu(xi,yi,ti)\n",
    "        nuK_array = nu_calc(c, alpha, gamma, K, potential_triggers['mag'], potential_triggers['t_denom'], potential_triggers['r_squared'])\n",
    "        cat.loc[triggered.Index, 'nuK'] = nuK_array.sum()\n",
    "    print('    took', (datetime.now() - calc_start), 'to compute nu(xi,yi,ti)/K \\n')\n",
    "    return cat\n",
    "\n",
    "# Background rate from catalog\n",
    "def mu(catalog, tau, l):\n",
    "    cat = catalog.copy(deep=True)\n",
    "    #cat['lat_rad'] = np.radians(cat['lat'])\n",
    "    #cat['lon_rad'] = np.radians(cat['lon'])\n",
    "    cat['mu_i'] = 0.0\n",
    "    cat['omega_initial'] = 0.5\n",
    "    for event in cat.head(-1).itertuples():\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        evtime = event.datetime\n",
    "        print(evtime)\n",
    "        #print('Triggered event time:', ttime)\n",
    "        evlatrad = event.lat_rad\n",
    "        evlonrad = event.lon_rad\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((evtime - temp_cat['datetime']).dt.total_seconds())\n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(evlatrad,temp_cat['lat_rad'],evlonrad,temp_cat['lon_rad']))\n",
    "        temp_cat['a_coeffs'] = a_coeff(temp_cat['t_diff_e'], temp_cat['t_diff_s'], tau)\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_initial'],tau,l,temp_cat['a_coeffs'])\n",
    "        print(temp_cat)\n",
    "        cat.loc[event.Index, 'mu_i'] = temp_cat['mu_indiv'].sum()\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returned_cat = nuK(catalog_gdf_d_filter, c=0.001, alpha=2, gamma=2, K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(returned_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = datetime(1970,1,1)\n",
    "#end_time = datetime(2021, 6, 21)\n",
    "#cat_stats = mu(returned_cat, 100.0, 100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cat_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLE estimate of parameter K ##\n",
    "\n",
    "# Calculate parameter F_i for individual events\n",
    "def F_i(alpha, t_diff, c, m):\n",
    "    return np.exp(alpha*m)*(np.log(t_diff) - np.log(c))\n",
    "\n",
    "# MLE estimate of the normalisation parameter K\n",
    "def K_param(catalog, c, alpha): # cat needs to contain mu, lambda for each event\n",
    "    calc_start = datetime.now() # time the function\n",
    "    cat = catalog.copy(deep=True)\n",
    "    cat['c_secs'] = c # in days\n",
    "    cat['t_diffs'] = (ttime - cat['datetime']).dt.total_seconds()\n",
    "    cat['t_quantity'] = cat['t_diffs'] + cat['c']\n",
    "    cat['K_num'] = 1 - (cat['omega_i'])\n",
    "    cat['K_denom'] = F_i(alpha, cat['t_quantity'], c, cat['mag'])\n",
    "    K = cat['K_num'].sum() / cat['K_denom'].sum\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declustering function ##\n",
    "def decluster(cat, cat_start, cat_end, tau, l, c, alpha, gamma):\n",
    "    #################################################################################################\n",
    "    # Function to decluster a catalog and provide probability of events belonging to the background\n",
    "    # cat is a pandas/geopandas dataframe containing cols:\n",
    "    # Index, year, month, day, hour, minute, second, lat,lon, depth_km, mag, datetime\n",
    "    # cat_start, cat_start are the start and end times of the catalog, to be given as datetime objects\n",
    "    # tau, l are temporal and spatial smoothing params, to be given in days and km\n",
    "    # c, alpha, gamma are Omori-Utsu Law and power spectral density constants\n",
    "    ##################################################################################################\n",
    "    assert t_cat_start < t_cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    \n",
    "    # Prepare the catalog for processing by adding lat, lon in rad and adding time differences to the start and end of catalog\n",
    "    cat_preprocessed = prep_catalog(cat_start, cat_end, cat_start)\n",
    "    \n",
    "    # Now calculate nu(xi,yi,ti)/K for all events:\n",
    "    nuK_cat = nuK(cat_preprocessed, c, alpha, gamma, K=1)\n",
    "    \n",
    "    # Calculate background rates at time and place of each event, assuming omega is 0.5\n",
    "    initial_mu_cat = mu(catalog, tau, l)\n",
    "    \n",
    "    # initial_mu_cat should now contain both a nu and a mu for each event\n",
    "    # Now compute omega:\n",
    "    initial_mu_cat['lambda_i'] = initial_mu_cat['mu_i'] + initial_mu_cat['nuK']\n",
    "    initial_mu_cat['omega_i'] = initial_mu_cat['mu_i'] + initial_mu_cat['nuK']\n",
    "    \n",
    "    # Using the omegas, now perform a max likelihood estimate (MLE) for parameter K:\n",
    "    K = K_param(initial_mu_cat, c, alpha)\n",
    "    \n",
    "    # Using the updated K value recalculate triggering rate\n",
    "    final_cat = nuK(initial_mu_cat, c, alpha, gamma, K)\n",
    "    \n",
    "    # now the final catalog should contain the correct omegas, which can be used to estimate a background seismicity rate curve\n",
    "    # Check this visually using a histogram\n",
    "    plt.hist(final_cat[\"omega_i\"],log=True)\n",
    "    return final_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final estimate of background seismicity rate ##\n",
    "def mu_final(x,y,cat_start, cat_end, cat, tau, l):\n",
    "    #########################################################\n",
    "    # Function computes timeseries of the background seismicity rate\n",
    "    # x,y refer to a spatial reference point - should be the centroid of the study area\n",
    "    # Function will build an array of datetime objects with a timestep of 1 day using the cat_start, cat_end times\n",
    "    # catalog should contain omega, a_coeff values for each event - the output of func decluster\n",
    "    #########################################################\n",
    "    assert t_cat_start < t_cat_end # Catalog start time must be earlier than the catalog end time\n",
    "    # Time steps for calculating background rate\n",
    "    times = np.arange(start_time, end_time, timedelta(days=1)).astype(datetime)\n",
    "    mu_t_series = []\n",
    "    # Compute background rate at each time step\n",
    "    for t_step in times:\n",
    "        temp_cat = cat.copy(deep=True)\n",
    "        temp_cat['t_diffs'] = (1./(24.*60.*60.))*((t_step - temp_cat['datetime']).dt.total_seconds())\n",
    "        # Calculate distances between triggered and potential triggers\n",
    "        temp_cat['r_squared'] = np.square(haversine(x, temp_cat['lat_rad'], y, temp_cat['lon_rad']))\n",
    "        temp_cat['mu_indiv'] = mu_calc(temp_cat['r_squared'],temp_cat['t_diffs'],temp_cat['omega_i'],tau,l,temp_cat['a_coeffs'])\n",
    "        mu_t_series.append(temp_cat['mu_indiv'].sum())\n",
    "    return times, mu_t_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declustering implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970-01-01 00:00:00 2021-06-21 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ISC web search params\n",
    "start_time = datetime(1970,1,1)\n",
    "end_time = datetime(2021, 6, 21)\n",
    "min_latitude = -25\n",
    "max_latitude = -11\n",
    "min_longitude = -80\n",
    "max_longitude = -66\n",
    "min_mag = 4.5\n",
    "max_mag = None\n",
    "min_depth = None\n",
    "max_depth = None\n",
    "print(start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic ref point:  0   -1.27409\n",
      "dtype: float64 0   -0.314159\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Savvas Marcou\\.conda\\envs\\gis\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\Savvas Marcou\\.conda\\envs\\gis\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Get spatial reference point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "lat_point_list = [min_latitude, max_latitude, min_latitude]\n",
    "lon_point_list = [min_longitude, max_longitude, min_longitude]\n",
    "\n",
    "search_area = Polygon(zip(lon_point_list, lat_point_list))\n",
    "crs = {'init': 'epsg:4326'}\n",
    "search_area = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])       \n",
    "#print(polygon.geometry)\n",
    "\n",
    "x_cent = np.radians(search_area.centroid.x)\n",
    "y_cent = np.radians(search_area.centroid.y)\n",
    "print('Geographic ref point: ', x_cent, y_cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECLUSTERING ###\n",
    "# Separate catalog into deep and shallow following Jara et al. (2017)\n",
    "catalog_shallow = catalog_gdf.loc[catalog_gdf['depth_km'] < 40.0]\n",
    "catalog_deep = catalog_gdf.loc[catalog_gdf['depth_km'] > 80.0]\n",
    "declustered_shallow_cat = decluster(catalog_shallow, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "declustered_deep_cat = decluster(catalog_deep, start_time, end_time, tau=100, l=100, c=0.001, alpha=2, gamma=2)\n",
    "\n",
    "# And calculate rates:\n",
    "times, deep_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_deep_cat, tau=100, l=100)\n",
    "times, shallow_rate = mu_final(x_cent,y_cent,start_time, end_time, declustered_shallow_cat, tau=100, l=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PLOTTING ####"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis",
   "language": "python",
   "name": "gis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
